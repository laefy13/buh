2023-11-30 08:57:16,120 Training with a single process on 1 device (cuda:0).
2023-11-30 08:57:16,336 Model efficientnet_b0 created, param count:8733680
2023-11-30 08:57:16,336 Data processing configuration for current model + dataset:
2023-11-30 08:57:16,336 	input_size: (3, 259, 259)
2023-11-30 08:57:16,336 	interpolation: bicubic
2023-11-30 08:57:16,336 	mean: (0.485, 0.456, 0.406)
2023-11-30 08:57:16,336 	std: (0.229, 0.224, 0.225)
2023-11-30 08:57:16,336 	crop_pct: 0.875
2023-11-30 08:57:16,336 	crop_mode: center
2023-11-30 08:57:18,773 AMP not enabled. Training in float32.
2023-11-30 08:57:18,815 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 08:57:20,859 FLOPs: 39.779662336 GFLOPs
2023-11-30 08:57:22,052 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.234s,    9.89/s  (3.234s,    9.89/s)  LR: 1.000e-05  Data: 0.887 (0.887)
2023-11-30 08:57:22,317 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.265s,  120.68/s  (1.750s,   18.29/s)  LR: 1.000e-05  Data: 0.008 (0.448)
2023-11-30 08:57:22,577 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.13/s  (1.253s,   25.54/s)  LR: 1.000e-05  Data: 0.003 (0.299)
2023-11-30 08:57:22,839 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.16/s  (1.005s,   31.83/s)  LR: 1.000e-05  Data: 0.005 (0.226)
2023-11-30 08:57:23,102 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.264s,  121.29/s  (0.857s,   37.34/s)  LR: 1.000e-05  Data: 0.006 (0.182)
2023-11-30 08:57:23,363 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  122.94/s  (0.758s,   42.24/s)  LR: 1.000e-05  Data: 0.003 (0.152)
2023-11-30 08:57:23,625 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.15/s  (0.687s,   46.60/s)  LR: 1.000e-05  Data: 0.005 (0.131)
2023-11-30 08:57:23,886 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.49/s  (0.634s,   50.51/s)  LR: 1.000e-05  Data: 0.004 (0.115)
2023-11-30 08:57:24,149 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.263s,  121.84/s  (0.592s,   54.02/s)  LR: 1.000e-05  Data: 0.005 (0.103)
2023-11-30 08:57:24,411 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.14/s  (0.559s,   57.21/s)  LR: 1.000e-05  Data: 0.005 (0.093)
2023-11-30 08:57:24,672 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.29/s  (0.532s,   60.12/s)  LR: 1.000e-05  Data: 0.005 (0.085)
2023-11-30 08:57:24,935 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.263s,  121.87/s  (0.510s,   62.77/s)  LR: 1.000e-05  Data: 0.005 (0.078)
2023-11-30 08:57:25,196 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.35/s  (0.491s,   65.21/s)  LR: 1.000e-05  Data: 0.005 (0.073)
2023-11-30 08:57:25,458 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.26/s  (0.474s,   67.46/s)  LR: 1.000e-05  Data: 0.005 (0.068)
2023-11-30 08:57:25,722 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.264s,  121.23/s  (0.460s,   69.52/s)  LR: 1.000e-05  Data: 0.004 (0.064)
2023-11-30 08:57:25,984 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.04/s  (0.448s,   71.44/s)  LR: 1.000e-05  Data: 0.005 (0.060)
2023-11-30 08:57:26,246 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.14/s  (0.437s,   73.23/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 08:57:26,508 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.01/s  (0.427s,   74.89/s)  LR: 1.000e-05  Data: 0.005 (0.054)
2023-11-30 08:57:26,771 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.263s,  121.81/s  (0.419s,   76.44/s)  LR: 1.000e-05  Data: 0.005 (0.051)
2023-11-30 08:57:27,036 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.265s,  120.85/s  (0.411s,   77.87/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 08:57:27,299 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.263s,  121.68/s  (0.404s,   79.23/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 08:57:27,561 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  121.98/s  (0.397s,   80.51/s)  LR: 1.000e-05  Data: 0.005 (0.045)
2023-11-30 08:57:27,823 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.07/s  (0.392s,   81.72/s)  LR: 1.000e-05  Data: 0.004 (0.043)
2023-11-30 08:57:28,086 Train: 0 [  23/39 ( 61%)]  Loss: 0.938 (1.27)  Time: 0.262s,  122.04/s  (0.386s,   82.86/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 08:57:28,348 Train: 0 [  24/39 ( 63%)]  Loss: 1.32 (1.27)  Time: 0.263s,  121.83/s  (0.381s,   83.94/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 08:57:28,611 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  121.97/s  (0.377s,   84.96/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 08:57:28,873 Train: 0 [  26/39 ( 68%)]  Loss: 0.995 (1.25)  Time: 0.262s,  122.00/s  (0.372s,   85.92/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 08:57:29,136 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.263s,  121.75/s  (0.369s,   86.84/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 08:57:29,398 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.263s,  121.88/s  (0.365s,   87.70/s)  LR: 1.000e-05  Data: 0.005 (0.035)
2023-11-30 08:57:29,661 Train: 0 [  29/39 ( 76%)]  Loss: 0.987 (1.25)  Time: 0.263s,  121.90/s  (0.361s,   88.53/s)  LR: 1.000e-05  Data: 0.005 (0.034)
2023-11-30 08:57:29,923 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.262s,  122.01/s  (0.358s,   89.32/s)  LR: 1.000e-05  Data: 0.005 (0.033)
2023-11-30 08:57:30,184 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.74/s  (0.355s,   90.09/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 08:57:30,445 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.261s,  122.73/s  (0.352s,   90.82/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 08:57:30,706 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.261s,  122.62/s  (0.350s,   91.52/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 08:57:30,966 Train: 0 [  34/39 ( 89%)]  Loss: 0.977 (1.25)  Time: 0.260s,  122.87/s  (0.347s,   92.19/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 08:57:31,226 Train: 0 [  35/39 ( 92%)]  Loss: 1.13 (1.24)  Time: 0.260s,  122.90/s  (0.345s,   92.84/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 08:57:31,487 Train: 0 [  36/39 ( 95%)]  Loss: 1.21 (1.24)  Time: 0.261s,  122.56/s  (0.342s,   93.45/s)  LR: 1.000e-05  Data: 0.004 (0.028)
2023-11-30 08:57:31,748 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.72/s  (0.340s,   94.04/s)  LR: 1.000e-05  Data: 0.004 (0.028)
2023-11-30 08:57:32,005 Train: 0 [  38/39 (100%)]  Loss: 2.56 (1.27)  Time: 0.256s,  124.80/s  (0.338s,   94.64/s)  LR: 1.000e-05  Data: 0.000 (0.027)
2023-11-30 08:57:33,176 Test: [   0/39]  Time: 1.168 (1.168)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:33,241 Test: [   1/39]  Time: 0.064 (0.616)  Loss:   0.181 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:33,304 Test: [   2/39]  Time: 0.064 (0.432)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:33,640 Test: [   3/39]  Time: 0.336 (0.408)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:33,822 Test: [   4/39]  Time: 0.181 (0.363)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:33,885 Test: [   5/39]  Time: 0.064 (0.313)  Loss:   0.188 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:33,949 Test: [   6/39]  Time: 0.064 (0.277)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:34,238 Test: [   7/39]  Time: 0.289 (0.279)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:34,481 Test: [   8/39]  Time: 0.242 (0.275)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:34,545 Test: [   9/39]  Time: 0.064 (0.254)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:34,608 Test: [  10/39]  Time: 0.064 (0.236)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:34,897 Test: [  11/39]  Time: 0.288 (0.241)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,136 Test: [  12/39]  Time: 0.239 (0.241)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,200 Test: [  13/39]  Time: 0.064 (0.228)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,264 Test: [  14/39]  Time: 0.064 (0.217)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,566 Test: [  15/39]  Time: 0.302 (0.222)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,793 Test: [  16/39]  Time: 0.227 (0.223)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,857 Test: [  17/39]  Time: 0.064 (0.214)  Loss:   0.181 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:35,921 Test: [  18/39]  Time: 0.064 (0.206)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:36,185 Test: [  19/39]  Time: 0.264 (0.209)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:36,425 Test: [  20/39]  Time: 0.241 (0.210)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:36,489 Test: [  21/39]  Time: 0.064 (0.204)  Loss:   0.181 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:36,553 Test: [  22/39]  Time: 0.064 (0.198)  Loss:   1.364 ( 0.235)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 08:57:36,774 Test: [  23/39]  Time: 0.222 (0.199)  Loss:   1.776 ( 0.299)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 08:57:37,045 Test: [  24/39]  Time: 0.271 (0.201)  Loss:   1.774 ( 0.358)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 08:57:37,109 Test: [  25/39]  Time: 0.064 (0.196)  Loss:   1.784 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 08:57:37,173 Test: [  26/39]  Time: 0.064 (0.191)  Loss:   1.776 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 08:57:37,403 Test: [  27/39]  Time: 0.230 (0.193)  Loss:   1.782 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 08:57:37,625 Test: [  28/39]  Time: 0.222 (0.194)  Loss:   1.788 ( 0.555)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 08:57:37,689 Test: [  29/39]  Time: 0.064 (0.189)  Loss:   1.788 ( 0.596)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 08:57:37,754 Test: [  30/39]  Time: 0.065 (0.185)  Loss:   1.783 ( 0.634)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 08:57:38,021 Test: [  31/39]  Time: 0.267 (0.188)  Loss:   1.791 ( 0.670)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 08:57:38,232 Test: [  32/39]  Time: 0.211 (0.189)  Loss:   1.773 ( 0.704)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 08:57:38,297 Test: [  33/39]  Time: 0.064 (0.185)  Loss:   1.783 ( 0.735)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 08:57:38,360 Test: [  34/39]  Time: 0.064 (0.181)  Loss:   1.775 ( 0.765)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 08:57:38,427 Test: [  35/39]  Time: 0.067 (0.178)  Loss:   1.788 ( 0.794)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 08:57:38,739 Test: [  36/39]  Time: 0.312 (0.182)  Loss:   1.744 ( 0.819)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 08:57:39,352 Test: [  37/39]  Time: 0.613 (0.193)  Loss:   1.526 ( 0.838)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 08:57:39,413 Test: [  38/39]  Time: 0.061 (0.190)  Loss:   1.583 ( 0.857)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 08:57:39,612 Test: [  39/39]  Time: 0.199 (0.190)  Loss:   1.344 ( 0.858)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 08:57:39,819 Current checkpoints:
 ('./output/train/20231130-085718-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 08:57:39,820 *** Best metric: 57.12 (epoch 0)

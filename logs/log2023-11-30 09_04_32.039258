2023-11-30 09:04:32,088 Training with a single process on 1 device (cuda:0).
2023-11-30 09:04:32,302 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:04:32,302 Data processing configuration for current model + dataset:
2023-11-30 09:04:32,303 	input_size: (3, 259, 259)
2023-11-30 09:04:32,303 	interpolation: bicubic
2023-11-30 09:04:32,303 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:04:32,303 	std: (0.229, 0.224, 0.225)
2023-11-30 09:04:32,303 	crop_pct: 0.875
2023-11-30 09:04:32,303 	crop_mode: center
2023-11-30 09:04:34,691 AMP not enabled. Training in float32.
2023-11-30 09:04:34,733 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:04:36,828 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:04:37,931 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.195s,   10.02/s  (3.195s,   10.02/s)  LR: 1.000e-05  Data: 0.958 (0.958)
2023-11-30 09:04:38,195 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.00/s  (1.730s,   18.50/s)  LR: 1.000e-05  Data: 0.008 (0.483)
2023-11-30 09:04:38,455 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.00/s  (1.240s,   25.81/s)  LR: 1.000e-05  Data: 0.003 (0.323)
2023-11-30 09:04:38,717 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.32/s  (0.995s,   32.15/s)  LR: 1.000e-05  Data: 0.005 (0.243)
2023-11-30 09:04:38,978 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.261s,  122.52/s  (0.849s,   37.71/s)  LR: 1.000e-05  Data: 0.004 (0.195)
2023-11-30 09:04:39,241 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.263s,  121.75/s  (0.751s,   42.61/s)  LR: 1.000e-05  Data: 0.004 (0.164)
2023-11-30 09:04:39,502 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.41/s  (0.681s,   46.99/s)  LR: 1.000e-05  Data: 0.004 (0.141)
2023-11-30 09:04:39,764 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.262s,  122.21/s  (0.629s,   50.91/s)  LR: 1.000e-05  Data: 0.005 (0.124)
2023-11-30 09:04:40,027 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.263s,  121.69/s  (0.588s,   54.43/s)  LR: 1.000e-05  Data: 0.004 (0.111)
2023-11-30 09:04:40,290 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.263s,  121.47/s  (0.556s,   57.60/s)  LR: 1.000e-05  Data: 0.005 (0.100)
2023-11-30 09:04:40,554 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.264s,  121.26/s  (0.529s,   60.49/s)  LR: 1.000e-05  Data: 0.005 (0.091)
2023-11-30 09:04:40,819 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.265s,  120.84/s  (0.507s,   63.12/s)  LR: 1.000e-05  Data: 0.005 (0.084)
2023-11-30 09:04:41,081 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.34/s  (0.488s,   65.56/s)  LR: 1.000e-05  Data: 0.004 (0.078)
2023-11-30 09:04:41,343 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.19/s  (0.472s,   67.80/s)  LR: 1.000e-05  Data: 0.005 (0.073)
2023-11-30 09:04:41,604 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.29/s  (0.458s,   69.88/s)  LR: 1.000e-05  Data: 0.004 (0.068)
2023-11-30 09:04:41,866 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.21/s  (0.446s,   71.80/s)  LR: 1.000e-05  Data: 0.005 (0.064)
2023-11-30 09:04:42,128 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.03/s  (0.435s,   73.58/s)  LR: 1.000e-05  Data: 0.005 (0.061)
2023-11-30 09:04:42,390 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.30/s  (0.425s,   75.25/s)  LR: 1.000e-05  Data: 0.005 (0.058)
2023-11-30 09:04:42,652 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.262s,  122.13/s  (0.417s,   76.80/s)  LR: 1.000e-05  Data: 0.005 (0.055)
2023-11-30 09:04:42,914 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.18/s  (0.409s,   78.25/s)  LR: 1.000e-05  Data: 0.005 (0.052)
2023-11-30 09:04:43,176 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.262s,  122.10/s  (0.402s,   79.61/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 09:04:43,438 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.06/s  (0.396s,   80.89/s)  LR: 1.000e-05  Data: 0.005 (0.048)
2023-11-30 09:04:43,700 Train: 0 [  22/39 ( 58%)]  Loss: 1.85 (1.28)  Time: 0.262s,  122.20/s  (0.390s,   82.10/s)  LR: 1.000e-05  Data: 0.005 (0.046)
2023-11-30 09:04:43,962 Train: 0 [  23/39 ( 61%)]  Loss: 0.942 (1.27)  Time: 0.262s,  122.23/s  (0.384s,   83.24/s)  LR: 1.000e-05  Data: 0.005 (0.044)
2023-11-30 09:04:44,223 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.262s,  122.30/s  (0.380s,   84.32/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:04:44,485 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.34/s  (0.375s,   85.34/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 09:04:44,746 Train: 0 [  26/39 ( 68%)]  Loss: 1.00 (1.25)  Time: 0.261s,  122.41/s  (0.371s,   86.30/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:04:45,008 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  122.23/s  (0.367s,   87.22/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 09:04:45,270 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.11/s  (0.363s,   88.09/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:04:45,532 Train: 0 [  29/39 ( 76%)]  Loss: 0.990 (1.25)  Time: 0.262s,  122.18/s  (0.360s,   88.91/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 09:04:45,794 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.262s,  122.21/s  (0.357s,   89.70/s)  LR: 1.000e-05  Data: 0.005 (0.035)
2023-11-30 09:04:46,062 Train: 0 [  31/39 ( 82%)]  Loss: 1.66 (1.26)  Time: 0.267s,  119.67/s  (0.354s,   90.41/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:04:46,323 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.262s,  122.15/s  (0.351s,   91.13/s)  LR: 1.000e-05  Data: 0.005 (0.033)
2023-11-30 09:04:46,584 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.261s,  122.59/s  (0.349s,   91.82/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:04:46,846 Train: 0 [  34/39 ( 89%)]  Loss: 0.980 (1.25)  Time: 0.261s,  122.47/s  (0.346s,   92.48/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:04:47,108 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.262s,  122.05/s  (0.344s,   93.11/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:04:47,370 Train: 0 [  36/39 ( 95%)]  Loss: 1.27 (1.24)  Time: 0.262s,  122.02/s  (0.341s,   93.71/s)  LR: 1.000e-05  Data: 0.005 (0.030)
2023-11-30 09:04:47,632 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.262s,  122.24/s  (0.339s,   94.29/s)  LR: 1.000e-05  Data: 0.005 (0.030)
2023-11-30 09:04:47,889 Train: 0 [  38/39 (100%)]  Loss: 2.59 (1.27)  Time: 0.257s,  124.52/s  (0.337s,   94.88/s)  LR: 1.000e-05  Data: 0.000 (0.029)
2023-11-30 09:04:48,945 Test: [   0/39]  Time: 1.052 (1.052)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:49,010 Test: [   1/39]  Time: 0.065 (0.559)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:49,075 Test: [   2/39]  Time: 0.064 (0.394)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:49,484 Test: [   3/39]  Time: 0.409 (0.398)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:49,612 Test: [   4/39]  Time: 0.128 (0.344)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:49,676 Test: [   5/39]  Time: 0.064 (0.297)  Loss:   0.190 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:49,740 Test: [   6/39]  Time: 0.064 (0.264)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,081 Test: [   7/39]  Time: 0.341 (0.274)  Loss:   0.184 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,255 Test: [   8/39]  Time: 0.174 (0.262)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,319 Test: [   9/39]  Time: 0.064 (0.243)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,382 Test: [  10/39]  Time: 0.064 (0.226)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,735 Test: [  11/39]  Time: 0.352 (0.237)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,906 Test: [  12/39]  Time: 0.171 (0.232)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:50,969 Test: [  13/39]  Time: 0.064 (0.220)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:51,033 Test: [  14/39]  Time: 0.064 (0.209)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:51,414 Test: [  15/39]  Time: 0.380 (0.220)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:51,567 Test: [  16/39]  Time: 0.154 (0.216)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:51,631 Test: [  17/39]  Time: 0.064 (0.208)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:51,695 Test: [  18/39]  Time: 0.064 (0.200)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:52,043 Test: [  19/39]  Time: 0.349 (0.208)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:52,194 Test: [  20/39]  Time: 0.151 (0.205)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:52,258 Test: [  21/39]  Time: 0.064 (0.198)  Loss:   0.182 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:52,321 Test: [  22/39]  Time: 0.064 (0.193)  Loss:   1.359 ( 0.236)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:04:52,643 Test: [  23/39]  Time: 0.321 (0.198)  Loss:   1.770 ( 0.300)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:04:52,819 Test: [  24/39]  Time: 0.177 (0.197)  Loss:   1.768 ( 0.359)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:04:52,883 Test: [  25/39]  Time: 0.064 (0.192)  Loss:   1.778 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:04:52,947 Test: [  26/39]  Time: 0.064 (0.187)  Loss:   1.770 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:04:53,274 Test: [  27/39]  Time: 0.327 (0.192)  Loss:   1.777 ( 0.510)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:04:53,407 Test: [  28/39]  Time: 0.133 (0.190)  Loss:   1.782 ( 0.554)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:04:53,471 Test: [  29/39]  Time: 0.064 (0.186)  Loss:   1.783 ( 0.595)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:04:53,534 Test: [  30/39]  Time: 0.064 (0.182)  Loss:   1.777 ( 0.633)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:04:53,921 Test: [  31/39]  Time: 0.386 (0.188)  Loss:   1.785 ( 0.669)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:04:54,017 Test: [  32/39]  Time: 0.096 (0.186)  Loss:   1.768 ( 0.703)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:04:54,081 Test: [  33/39]  Time: 0.064 (0.182)  Loss:   1.776 ( 0.734)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:04:54,144 Test: [  34/39]  Time: 0.064 (0.179)  Loss:   1.770 ( 0.764)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:04:54,295 Test: [  35/39]  Time: 0.150 (0.178)  Loss:   1.783 ( 0.792)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:04:54,542 Test: [  36/39]  Time: 0.247 (0.180)  Loss:   1.731 ( 0.818)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:04:55,255 Test: [  37/39]  Time: 0.713 (0.194)  Loss:   1.503 ( 0.836)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 09:04:55,316 Test: [  38/39]  Time: 0.061 (0.190)  Loss:   1.563 ( 0.854)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 09:04:55,514 Test: [  39/39]  Time: 0.198 (0.191)  Loss:   1.328 ( 0.855)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 09:04:55,718 Current checkpoints:
 ('./output/train/20231130-090434-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 09:04:55,719 *** Best metric: 57.12 (epoch 0)

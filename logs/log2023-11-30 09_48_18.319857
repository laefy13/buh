2023-11-30 09:48:18,379 Training with a single process on 1 device (cuda:0).
2023-11-30 09:48:18,591 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:48:18,592 Data processing configuration for current model + dataset:
2023-11-30 09:48:18,592 	input_size: (3, 259, 259)
2023-11-30 09:48:18,592 	interpolation: bicubic
2023-11-30 09:48:18,592 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:48:18,592 	std: (0.229, 0.224, 0.225)
2023-11-30 09:48:18,592 	crop_pct: 0.875
2023-11-30 09:48:18,592 	crop_mode: center
2023-11-30 09:48:21,308 AMP not enabled. Training in float32.
2023-11-30 09:48:21,356 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:48:23,623 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:48:24,774 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.415s,    9.37/s  (3.415s,    9.37/s)  LR: 1.000e-05  Data: 1.020 (1.020)
2023-11-30 09:48:25,037 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.263s,  121.68/s  (1.839s,   17.40/s)  LR: 1.000e-05  Data: 0.006 (0.513)
2023-11-30 09:48:25,297 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  122.93/s  (1.313s,   24.37/s)  LR: 1.000e-05  Data: 0.003 (0.343)
2023-11-30 09:48:25,559 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.27/s  (1.050s,   30.47/s)  LR: 1.000e-05  Data: 0.005 (0.258)
2023-11-30 09:48:25,825 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.266s,  120.41/s  (0.893s,   35.83/s)  LR: 1.000e-05  Data: 0.009 (0.208)
2023-11-30 09:48:26,085 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.02/s  (0.788s,   40.63/s)  LR: 1.000e-05  Data: 0.003 (0.174)
2023-11-30 09:48:26,345 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.260s,  122.90/s  (0.712s,   44.92/s)  LR: 1.000e-05  Data: 0.003 (0.150)
2023-11-30 09:48:26,608 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.263s,  121.89/s  (0.656s,   48.77/s)  LR: 1.000e-05  Data: 0.005 (0.131)
2023-11-30 09:48:26,870 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.09/s  (0.612s,   52.26/s)  LR: 1.000e-05  Data: 0.005 (0.117)
2023-11-30 09:48:27,132 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.00/s  (0.577s,   55.43/s)  LR: 1.000e-05  Data: 0.005 (0.106)
2023-11-30 09:48:27,395 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.263s,  121.72/s  (0.549s,   58.31/s)  LR: 1.000e-05  Data: 0.005 (0.097)
2023-11-30 09:48:27,657 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.262s,  122.11/s  (0.525s,   60.97/s)  LR: 1.000e-05  Data: 0.005 (0.089)
2023-11-30 09:48:27,919 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.10/s  (0.505s,   63.41/s)  LR: 1.000e-05  Data: 0.004 (0.083)
2023-11-30 09:48:28,181 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.15/s  (0.487s,   65.67/s)  LR: 1.000e-05  Data: 0.005 (0.077)
2023-11-30 09:48:28,443 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.14/s  (0.472s,   67.75/s)  LR: 1.000e-05  Data: 0.005 (0.072)
2023-11-30 09:48:28,707 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.264s,  121.20/s  (0.459s,   69.67/s)  LR: 1.000e-05  Data: 0.004 (0.068)
2023-11-30 09:48:28,971 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.264s,  121.18/s  (0.448s,   71.46/s)  LR: 1.000e-05  Data: 0.005 (0.064)
2023-11-30 09:48:29,238 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.266s,  120.08/s  (0.438s,   73.11/s)  LR: 1.000e-05  Data: 0.009 (0.061)
2023-11-30 09:48:29,500 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.262s,  122.01/s  (0.428s,   74.68/s)  LR: 1.000e-05  Data: 0.005 (0.058)
2023-11-30 09:48:29,761 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.33/s  (0.420s,   76.16/s)  LR: 1.000e-05  Data: 0.004 (0.056)
2023-11-30 09:48:30,023 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.262s,  122.12/s  (0.413s,   77.55/s)  LR: 1.000e-05  Data: 0.005 (0.053)
2023-11-30 09:48:30,285 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.18/s  (0.406s,   78.86/s)  LR: 1.000e-05  Data: 0.005 (0.051)
2023-11-30 09:48:30,548 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.03/s  (0.400s,   80.10/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 09:48:30,809 Train: 0 [  23/39 ( 61%)]  Loss: 0.940 (1.27)  Time: 0.261s,  122.46/s  (0.394s,   81.27/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 09:48:31,071 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.05/s  (0.388s,   82.37/s)  LR: 1.000e-05  Data: 0.005 (0.045)
2023-11-30 09:48:31,333 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.08/s  (0.384s,   83.41/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 09:48:31,595 Train: 0 [  26/39 ( 68%)]  Loss: 0.997 (1.25)  Time: 0.262s,  122.07/s  (0.379s,   84.40/s)  LR: 1.000e-05  Data: 0.005 (0.042)
2023-11-30 09:48:31,857 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.261s,  122.39/s  (0.375s,   85.35/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 09:48:32,119 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.11/s  (0.371s,   86.24/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:48:32,381 Train: 0 [  29/39 ( 76%)]  Loss: 0.988 (1.25)  Time: 0.262s,  122.02/s  (0.367s,   87.09/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 09:48:32,643 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.262s,  122.17/s  (0.364s,   87.91/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 09:48:32,904 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.82/s  (0.361s,   88.70/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:48:33,164 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.260s,  122.86/s  (0.358s,   89.45/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:48:33,425 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.260s,  122.87/s  (0.355s,   90.17/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:48:33,685 Train: 0 [  34/39 ( 89%)]  Loss: 0.979 (1.25)  Time: 0.260s,  122.96/s  (0.352s,   90.86/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:48:33,946 Train: 0 [  35/39 ( 92%)]  Loss: 1.12 (1.24)  Time: 0.261s,  122.55/s  (0.350s,   91.52/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:48:34,208 Train: 0 [  36/39 ( 95%)]  Loss: 1.26 (1.24)  Time: 0.262s,  122.23/s  (0.347s,   92.15/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:48:34,469 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.62/s  (0.345s,   92.75/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:48:34,725 Train: 0 [  38/39 (100%)]  Loss: 2.56 (1.27)  Time: 0.256s,  124.92/s  (0.343s,   93.37/s)  LR: 1.000e-05  Data: 0.000 (0.031)
2023-11-30 09:48:35,963 Test: [   0/39]  Time: 1.235 (1.235)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:36,027 Test: [   1/39]  Time: 0.064 (0.650)  Loss:   0.185 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:36,091 Test: [   2/39]  Time: 0.064 (0.454)  Loss:   0.188 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:36,426 Test: [   3/39]  Time: 0.335 (0.424)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:36,593 Test: [   4/39]  Time: 0.167 (0.373)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:36,657 Test: [   5/39]  Time: 0.064 (0.321)  Loss:   0.193 ( 0.189)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:36,720 Test: [   6/39]  Time: 0.064 (0.285)  Loss:   0.188 ( 0.189)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,001 Test: [   7/39]  Time: 0.281 (0.284)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,239 Test: [   8/39]  Time: 0.238 (0.279)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,303 Test: [   9/39]  Time: 0.064 (0.257)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,367 Test: [  10/39]  Time: 0.064 (0.240)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,639 Test: [  11/39]  Time: 0.273 (0.243)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,884 Test: [  12/39]  Time: 0.245 (0.243)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:37,948 Test: [  13/39]  Time: 0.064 (0.230)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:38,012 Test: [  14/39]  Time: 0.064 (0.219)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:38,323 Test: [  15/39]  Time: 0.310 (0.225)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:38,563 Test: [  16/39]  Time: 0.240 (0.226)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:38,626 Test: [  17/39]  Time: 0.064 (0.217)  Loss:   0.185 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:38,690 Test: [  18/39]  Time: 0.064 (0.209)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:38,943 Test: [  19/39]  Time: 0.253 (0.211)  Loss:   0.186 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:39,197 Test: [  20/39]  Time: 0.253 (0.213)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:39,261 Test: [  21/39]  Time: 0.064 (0.206)  Loss:   0.185 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:39,324 Test: [  22/39]  Time: 0.064 (0.200)  Loss:   1.350 ( 0.238)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:48:39,543 Test: [  23/39]  Time: 0.219 (0.201)  Loss:   1.756 ( 0.302)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:48:39,838 Test: [  24/39]  Time: 0.295 (0.204)  Loss:   1.754 ( 0.360)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:48:39,902 Test: [  25/39]  Time: 0.064 (0.199)  Loss:   1.764 ( 0.414)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:48:39,966 Test: [  26/39]  Time: 0.064 (0.194)  Loss:   1.757 ( 0.463)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:48:40,182 Test: [  27/39]  Time: 0.216 (0.195)  Loss:   1.763 ( 0.510)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:48:40,416 Test: [  28/39]  Time: 0.234 (0.196)  Loss:   1.769 ( 0.553)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:48:40,480 Test: [  29/39]  Time: 0.064 (0.192)  Loss:   1.769 ( 0.594)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:48:40,544 Test: [  30/39]  Time: 0.065 (0.188)  Loss:   1.764 ( 0.632)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:48:40,803 Test: [  31/39]  Time: 0.259 (0.190)  Loss:   1.771 ( 0.667)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:48:41,026 Test: [  32/39]  Time: 0.222 (0.191)  Loss:   1.754 ( 0.700)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:48:41,089 Test: [  33/39]  Time: 0.064 (0.187)  Loss:   1.763 ( 0.731)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:48:41,153 Test: [  34/39]  Time: 0.064 (0.184)  Loss:   1.756 ( 0.761)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:48:41,219 Test: [  35/39]  Time: 0.066 (0.180)  Loss:   1.769 ( 0.789)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:48:41,534 Test: [  36/39]  Time: 0.314 (0.184)  Loss:   1.721 ( 0.814)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:48:42,167 Test: [  37/39]  Time: 0.634 (0.196)  Loss:   1.499 ( 0.832)  Acc@1:   6.250 ( 58.717)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.062 (  0.002)soec:   0.000 (  0.605)f1:   0.118 (  0.003)
2023-11-30 09:48:42,228 Test: [  38/39]  Time: 0.061 (0.192)  Loss:   1.558 ( 0.850)  Acc@1:   3.125 ( 57.292)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.005)
2023-11-30 09:48:42,421 Test: [  39/39]  Time: 0.193 (0.192)  Loss:   1.323 ( 0.851)  Acc@1:   0.000 ( 57.200)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.005)
2023-11-30 09:48:42,632 Current checkpoints:
 ('./output/train/20231130-094821-efficientnet_b0-259/checkpoint-0.pth.tar', 57.2)

2023-11-30 09:48:42,633 *** Best metric: 57.2 (epoch 0)

2023-11-30 09:50:04,312 Training with a single process on 1 device (cuda:0).
2023-11-30 09:50:04,530 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:50:04,530 Data processing configuration for current model + dataset:
2023-11-30 09:50:04,530 	input_size: (3, 259, 259)
2023-11-30 09:50:04,530 	interpolation: bicubic
2023-11-30 09:50:04,530 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:50:04,530 	std: (0.229, 0.224, 0.225)
2023-11-30 09:50:04,530 	crop_pct: 0.875
2023-11-30 09:50:04,530 	crop_mode: center
2023-11-30 09:50:06,986 AMP not enabled. Training in float32.
2023-11-30 09:50:07,031 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:50:09,191 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:50:10,314 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.281s,    9.75/s  (3.281s,    9.75/s)  LR: 1.000e-05  Data: 1.011 (1.011)
2023-11-30 09:50:10,579 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.265s,  120.81/s  (1.773s,   18.05/s)  LR: 1.000e-05  Data: 0.008 (0.510)
2023-11-30 09:50:10,840 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.261s,  122.65/s  (1.269s,   25.22/s)  LR: 1.000e-05  Data: 0.003 (0.341)
2023-11-30 09:50:11,102 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.06/s  (1.017s,   31.46/s)  LR: 1.000e-05  Data: 0.004 (0.257)
2023-11-30 09:50:11,367 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.264s,  121.09/s  (0.867s,   36.92/s)  LR: 1.000e-05  Data: 0.007 (0.207)
2023-11-30 09:50:11,627 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.05/s  (0.766s,   41.80/s)  LR: 1.000e-05  Data: 0.003 (0.173)
2023-11-30 09:50:11,886 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.260s,  123.18/s  (0.693s,   46.16/s)  LR: 1.000e-05  Data: 0.003 (0.148)
2023-11-30 09:50:12,149 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.262s,  121.99/s  (0.639s,   50.04/s)  LR: 1.000e-05  Data: 0.005 (0.130)
2023-11-30 09:50:12,410 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.261s,  122.58/s  (0.597s,   53.57/s)  LR: 1.000e-05  Data: 0.004 (0.116)
2023-11-30 09:50:12,671 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.28/s  (0.564s,   56.76/s)  LR: 1.000e-05  Data: 0.005 (0.105)
2023-11-30 09:50:12,934 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  121.94/s  (0.536s,   59.65/s)  LR: 1.000e-05  Data: 0.004 (0.096)
2023-11-30 09:50:13,195 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.43/s  (0.514s,   62.32/s)  LR: 1.000e-05  Data: 0.005 (0.088)
2023-11-30 09:50:13,456 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.79/s  (0.494s,   64.77/s)  LR: 1.000e-05  Data: 0.004 (0.082)
2023-11-30 09:50:13,717 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.49/s  (0.477s,   67.03/s)  LR: 1.000e-05  Data: 0.005 (0.076)
2023-11-30 09:50:13,979 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.28/s  (0.463s,   69.11/s)  LR: 1.000e-05  Data: 0.004 (0.072)
2023-11-30 09:50:14,241 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.17/s  (0.450s,   71.04/s)  LR: 1.000e-05  Data: 0.005 (0.067)
2023-11-30 09:50:14,502 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.49/s  (0.439s,   72.84/s)  LR: 1.000e-05  Data: 0.004 (0.064)
2023-11-30 09:50:14,763 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.52/s  (0.429s,   74.52/s)  LR: 1.000e-05  Data: 0.004 (0.060)
2023-11-30 09:50:15,025 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.262s,  122.36/s  (0.421s,   76.08/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 09:50:15,286 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.39/s  (0.413s,   77.55/s)  LR: 1.000e-05  Data: 0.005 (0.055)
2023-11-30 09:50:15,547 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.261s,  122.61/s  (0.405s,   78.93/s)  LR: 1.000e-05  Data: 0.004 (0.052)
2023-11-30 09:50:15,809 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.25/s  (0.399s,   80.22/s)  LR: 1.000e-05  Data: 0.005 (0.050)
2023-11-30 09:50:16,071 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.27/s  (0.393s,   81.44/s)  LR: 1.000e-05  Data: 0.004 (0.048)
2023-11-30 09:50:16,332 Train: 0 [  23/39 ( 61%)]  Loss: 0.943 (1.27)  Time: 0.261s,  122.41/s  (0.387s,   82.59/s)  LR: 1.000e-05  Data: 0.004 (0.046)
2023-11-30 09:50:16,593 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.261s,  122.68/s  (0.382s,   83.69/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 09:50:16,854 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.261s,  122.42/s  (0.378s,   84.72/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:50:17,116 Train: 0 [  26/39 ( 68%)]  Loss: 1.00 (1.25)  Time: 0.262s,  122.15/s  (0.373s,   85.69/s)  LR: 1.000e-05  Data: 0.005 (0.042)
2023-11-30 09:50:17,378 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  122.29/s  (0.369s,   86.61/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:50:17,639 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.261s,  122.53/s  (0.366s,   87.50/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:50:17,900 Train: 0 [  29/39 ( 76%)]  Loss: 0.990 (1.25)  Time: 0.261s,  122.38/s  (0.362s,   88.34/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 09:50:18,162 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.261s,  122.54/s  (0.359s,   89.14/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:50:18,422 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.260s,  122.89/s  (0.356s,   89.91/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:50:18,681 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.259s,  123.33/s  (0.353s,   90.66/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:50:18,941 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.260s,  123.16/s  (0.350s,   91.37/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:50:19,203 Train: 0 [  34/39 ( 89%)]  Loss: 0.983 (1.25)  Time: 0.262s,  122.31/s  (0.348s,   92.03/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:50:19,464 Train: 0 [  35/39 ( 92%)]  Loss: 1.12 (1.24)  Time: 0.261s,  122.68/s  (0.345s,   92.67/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:50:19,724 Train: 0 [  36/39 ( 95%)]  Loss: 1.29 (1.24)  Time: 0.260s,  123.10/s  (0.343s,   93.30/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:50:19,984 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.84/s  (0.341s,   93.89/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:50:20,240 Train: 0 [  38/39 (100%)]  Loss: 2.57 (1.27)  Time: 0.255s,  125.30/s  (0.339s,   94.50/s)  LR: 1.000e-05  Data: 0.000 (0.030)
2023-11-30 09:50:21,430 Test: [   0/39]  Time: 1.187 (1.187)  Loss:   0.190 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:21,494 Test: [   1/39]  Time: 0.064 (0.626)  Loss:   0.187 ( 0.189)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:21,558 Test: [   2/39]  Time: 0.064 (0.438)  Loss:   0.190 ( 0.189)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:21,972 Test: [   3/39]  Time: 0.414 (0.432)  Loss:   0.191 ( 0.189)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,078 Test: [   4/39]  Time: 0.106 (0.367)  Loss:   0.191 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,141 Test: [   5/39]  Time: 0.064 (0.316)  Loss:   0.194 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,205 Test: [   6/39]  Time: 0.064 (0.280)  Loss:   0.190 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,566 Test: [   7/39]  Time: 0.361 (0.290)  Loss:   0.189 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,732 Test: [   8/39]  Time: 0.166 (0.277)  Loss:   0.189 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,796 Test: [   9/39]  Time: 0.064 (0.255)  Loss:   0.191 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:22,859 Test: [  10/39]  Time: 0.064 (0.238)  Loss:   0.191 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:23,207 Test: [  11/39]  Time: 0.348 (0.247)  Loss:   0.189 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:23,382 Test: [  12/39]  Time: 0.175 (0.241)  Loss:   0.190 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:23,446 Test: [  13/39]  Time: 0.064 (0.229)  Loss:   0.190 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:23,509 Test: [  14/39]  Time: 0.064 (0.218)  Loss:   0.191 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:23,877 Test: [  15/39]  Time: 0.368 (0.227)  Loss:   0.191 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,036 Test: [  16/39]  Time: 0.159 (0.223)  Loss:   0.190 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,100 Test: [  17/39]  Time: 0.064 (0.214)  Loss:   0.187 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,163 Test: [  18/39]  Time: 0.064 (0.206)  Loss:   0.190 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,495 Test: [  19/39]  Time: 0.332 (0.213)  Loss:   0.188 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,667 Test: [  20/39]  Time: 0.171 (0.211)  Loss:   0.188 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,730 Test: [  21/39]  Time: 0.064 (0.204)  Loss:   0.187 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:24,794 Test: [  22/39]  Time: 0.064 (0.198)  Loss:   1.344 ( 0.240)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:50:25,086 Test: [  23/39]  Time: 0.292 (0.202)  Loss:   1.747 ( 0.303)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:50:25,290 Test: [  24/39]  Time: 0.204 (0.202)  Loss:   1.746 ( 0.360)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:50:25,354 Test: [  25/39]  Time: 0.064 (0.197)  Loss:   1.756 ( 0.414)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:50:25,417 Test: [  26/39]  Time: 0.064 (0.192)  Loss:   1.748 ( 0.463)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:50:25,709 Test: [  27/39]  Time: 0.292 (0.195)  Loss:   1.755 ( 0.510)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:50:25,868 Test: [  28/39]  Time: 0.158 (0.194)  Loss:   1.760 ( 0.553)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:50:25,931 Test: [  29/39]  Time: 0.064 (0.190)  Loss:   1.761 ( 0.593)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:50:25,995 Test: [  30/39]  Time: 0.064 (0.186)  Loss:   1.755 ( 0.630)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:50:26,333 Test: [  31/39]  Time: 0.338 (0.190)  Loss:   1.763 ( 0.666)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:50:26,475 Test: [  32/39]  Time: 0.142 (0.189)  Loss:   1.746 ( 0.699)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:50:26,539 Test: [  33/39]  Time: 0.064 (0.185)  Loss:   1.754 ( 0.730)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:50:26,602 Test: [  34/39]  Time: 0.064 (0.182)  Loss:   1.748 ( 0.759)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:50:26,698 Test: [  35/39]  Time: 0.096 (0.179)  Loss:   1.760 ( 0.787)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:50:26,983 Test: [  36/39]  Time: 0.284 (0.182)  Loss:   1.709 ( 0.811)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:50:27,602 Test: [  37/39]  Time: 0.619 (0.194)  Loss:   1.482 ( 0.829)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 09:50:27,663 Test: [  38/39]  Time: 0.061 (0.190)  Loss:   1.542 ( 0.847)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 09:50:27,858 Test: [  39/39]  Time: 0.195 (0.190)  Loss:   1.309 ( 0.848)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 09:50:28,059 Current checkpoints:
 ('./output/train/20231130-095007-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 09:50:28,059 *** Best metric: 57.28 (epoch 0)

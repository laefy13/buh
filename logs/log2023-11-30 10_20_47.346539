2023-11-30 10:20:47,396 Training with a single process on 1 device (cuda:0).
2023-11-30 10:20:47,608 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:20:47,609 Data processing configuration for current model + dataset:
2023-11-30 10:20:47,609 	input_size: (3, 259, 259)
2023-11-30 10:20:47,609 	interpolation: bicubic
2023-11-30 10:20:47,609 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:20:47,609 	std: (0.229, 0.224, 0.225)
2023-11-30 10:20:47,609 	crop_pct: 0.875
2023-11-30 10:20:47,609 	crop_mode: center
2023-11-30 10:20:50,037 AMP not enabled. Training in float32.
2023-11-30 10:20:50,080 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:20:52,210 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:20:53,332 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.250s,    9.85/s  (3.250s,    9.85/s)  LR: 1.000e-05  Data: 0.976 (0.976)
2023-11-30 10:20:53,595 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.263s,  121.79/s  (1.756s,   18.22/s)  LR: 1.000e-05  Data: 0.006 (0.491)
2023-11-30 10:20:53,855 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.261s,  122.72/s  (1.258s,   25.44/s)  LR: 1.000e-05  Data: 0.003 (0.328)
2023-11-30 10:20:54,117 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.40/s  (1.009s,   31.72/s)  LR: 1.000e-05  Data: 0.004 (0.247)
2023-11-30 10:20:54,379 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.262s,  121.96/s  (0.859s,   37.23/s)  LR: 1.000e-05  Data: 0.005 (0.199)
2023-11-30 10:20:54,639 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.05/s  (0.760s,   42.13/s)  LR: 1.000e-05  Data: 0.003 (0.166)
2023-11-30 10:20:54,901 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.15/s  (0.688s,   46.48/s)  LR: 1.000e-05  Data: 0.005 (0.143)
2023-11-30 10:20:55,162 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.56/s  (0.635s,   50.39/s)  LR: 1.000e-05  Data: 0.004 (0.126)
2023-11-30 10:20:55,423 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.261s,  122.57/s  (0.593s,   53.92/s)  LR: 1.000e-05  Data: 0.004 (0.112)
2023-11-30 10:20:55,686 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.11/s  (0.560s,   57.11/s)  LR: 1.000e-05  Data: 0.005 (0.101)
2023-11-30 10:20:55,948 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.11/s  (0.533s,   60.01/s)  LR: 1.000e-05  Data: 0.005 (0.093)
2023-11-30 10:20:56,209 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.57/s  (0.511s,   62.68/s)  LR: 1.000e-05  Data: 0.004 (0.085)
2023-11-30 10:20:56,470 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.69/s  (0.491s,   65.13/s)  LR: 1.000e-05  Data: 0.004 (0.079)
2023-11-30 10:20:56,731 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.28/s  (0.475s,   67.38/s)  LR: 1.000e-05  Data: 0.004 (0.074)
2023-11-30 10:20:56,993 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.21/s  (0.461s,   69.45/s)  LR: 1.000e-05  Data: 0.004 (0.069)
2023-11-30 10:20:57,255 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.23/s  (0.448s,   71.38/s)  LR: 1.000e-05  Data: 0.004 (0.065)
2023-11-30 10:20:57,516 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.52/s  (0.437s,   73.18/s)  LR: 1.000e-05  Data: 0.004 (0.061)
2023-11-30 10:20:57,780 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.264s,  121.39/s  (0.428s,   74.83/s)  LR: 1.000e-05  Data: 0.004 (0.058)
2023-11-30 10:20:58,042 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.262s,  122.17/s  (0.419s,   76.39/s)  LR: 1.000e-05  Data: 0.004 (0.055)
2023-11-30 10:20:58,303 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.29/s  (0.411s,   77.85/s)  LR: 1.000e-05  Data: 0.004 (0.053)
2023-11-30 10:20:58,565 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.261s,  122.44/s  (0.404s,   79.22/s)  LR: 1.000e-05  Data: 0.004 (0.051)
2023-11-30 10:20:58,826 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.28/s  (0.397s,   80.51/s)  LR: 1.000e-05  Data: 0.004 (0.048)
2023-11-30 10:20:59,088 Train: 0 [  22/39 ( 58%)]  Loss: 1.85 (1.28)  Time: 0.262s,  122.27/s  (0.392s,   81.72/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 10:20:59,349 Train: 0 [  23/39 ( 61%)]  Loss: 0.940 (1.27)  Time: 0.261s,  122.39/s  (0.386s,   82.87/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 10:20:59,611 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.261s,  122.44/s  (0.381s,   83.96/s)  LR: 1.000e-05  Data: 0.004 (0.043)
2023-11-30 10:20:59,872 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.35/s  (0.377s,   84.98/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 10:21:00,135 Train: 0 [  26/39 ( 68%)]  Loss: 0.996 (1.25)  Time: 0.262s,  122.01/s  (0.372s,   85.95/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 10:21:00,396 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.261s,  122.58/s  (0.368s,   86.88/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 10:21:00,657 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.261s,  122.64/s  (0.365s,   87.76/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 10:21:00,918 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  122.33/s  (0.361s,   88.59/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 10:21:01,180 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.261s,  122.42/s  (0.358s,   89.39/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 10:21:01,440 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.83/s  (0.355s,   90.16/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 10:21:01,700 Train: 0 [  32/39 ( 84%)]  Loss: 1.04 (1.25)  Time: 0.260s,  123.17/s  (0.352s,   90.89/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 10:21:01,960 Train: 0 [  33/39 ( 87%)]  Loss: 1.37 (1.25)  Time: 0.260s,  123.06/s  (0.349s,   91.60/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 10:21:02,220 Train: 0 [  34/39 ( 89%)]  Loss: 0.975 (1.25)  Time: 0.260s,  123.12/s  (0.347s,   92.27/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:21:02,480 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.260s,  123.14/s  (0.344s,   92.92/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:21:02,739 Train: 0 [  36/39 ( 95%)]  Loss: 1.23 (1.24)  Time: 0.259s,  123.41/s  (0.342s,   93.55/s)  LR: 1.000e-05  Data: 0.003 (0.030)
2023-11-30 10:21:02,999 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.260s,  123.12/s  (0.340s,   94.14/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 10:21:03,254 Train: 0 [  38/39 (100%)]  Loss: 2.60 (1.27)  Time: 0.255s,  125.28/s  (0.338s,   94.74/s)  LR: 1.000e-05  Data: 0.000 (0.029)
2023-11-30 10:21:04,369 Test: [   0/39]  Time: 1.111 (1.111)  Loss:   0.180 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:04,433 Test: [   1/39]  Time: 0.064 (0.588)  Loss:   0.177 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:04,497 Test: [   2/39]  Time: 0.064 (0.413)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:04,863 Test: [   3/39]  Time: 0.366 (0.401)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,002 Test: [   4/39]  Time: 0.140 (0.349)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,066 Test: [   5/39]  Time: 0.064 (0.301)  Loss:   0.184 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,129 Test: [   6/39]  Time: 0.064 (0.267)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,459 Test: [   7/39]  Time: 0.330 (0.275)  Loss:   0.179 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,713 Test: [   8/39]  Time: 0.254 (0.273)  Loss:   0.179 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,777 Test: [   9/39]  Time: 0.064 (0.252)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:05,841 Test: [  10/39]  Time: 0.064 (0.235)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:06,165 Test: [  11/39]  Time: 0.324 (0.242)  Loss:   0.179 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:06,429 Test: [  12/39]  Time: 0.264 (0.244)  Loss:   0.180 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:06,492 Test: [  13/39]  Time: 0.064 (0.231)  Loss:   0.180 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:06,556 Test: [  14/39]  Time: 0.064 (0.220)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:06,877 Test: [  15/39]  Time: 0.320 (0.226)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,095 Test: [  16/39]  Time: 0.218 (0.226)  Loss:   0.180 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,158 Test: [  17/39]  Time: 0.064 (0.217)  Loss:   0.177 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,222 Test: [  18/39]  Time: 0.064 (0.209)  Loss:   0.180 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,509 Test: [  19/39]  Time: 0.287 (0.213)  Loss:   0.178 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,726 Test: [  20/39]  Time: 0.217 (0.213)  Loss:   0.179 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,790 Test: [  21/39]  Time: 0.064 (0.206)  Loss:   0.177 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:07,854 Test: [  22/39]  Time: 0.064 (0.200)  Loss:   1.378 ( 0.232)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:21:08,118 Test: [  23/39]  Time: 0.264 (0.203)  Loss:   1.796 ( 0.297)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:21:08,358 Test: [  24/39]  Time: 0.240 (0.204)  Loss:   1.794 ( 0.357)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:21:08,422 Test: [  25/39]  Time: 0.064 (0.199)  Loss:   1.804 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:21:08,485 Test: [  26/39]  Time: 0.064 (0.194)  Loss:   1.796 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:21:08,746 Test: [  27/39]  Time: 0.261 (0.196)  Loss:   1.803 ( 0.512)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:21:08,943 Test: [  28/39]  Time: 0.196 (0.196)  Loss:   1.809 ( 0.556)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:21:09,006 Test: [  29/39]  Time: 0.064 (0.192)  Loss:   1.809 ( 0.598)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:21:09,070 Test: [  30/39]  Time: 0.064 (0.187)  Loss:   1.804 ( 0.637)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:21:09,376 Test: [  31/39]  Time: 0.306 (0.191)  Loss:   1.812 ( 0.674)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:21:09,554 Test: [  32/39]  Time: 0.178 (0.191)  Loss:   1.794 ( 0.708)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:21:09,618 Test: [  33/39]  Time: 0.064 (0.187)  Loss:   1.803 ( 0.740)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:21:09,681 Test: [  34/39]  Time: 0.064 (0.184)  Loss:   1.796 ( 0.770)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:21:09,746 Test: [  35/39]  Time: 0.065 (0.180)  Loss:   1.809 ( 0.799)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:21:10,074 Test: [  36/39]  Time: 0.328 (0.184)  Loss:   1.761 ( 0.825)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:21:10,689 Test: [  37/39]  Time: 0.615 (0.196)  Loss:   1.537 ( 0.844)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 10:21:10,750 Test: [  38/39]  Time: 0.061 (0.192)  Loss:   1.595 ( 0.863)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 10:21:10,946 Test: [  39/39]  Time: 0.197 (0.192)  Loss:   1.356 ( 0.864)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 10:21:11,155 Current checkpoints:
 ('./output/train/20231130-102050-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 10:21:11,155 *** Best metric: 57.12 (epoch 0)

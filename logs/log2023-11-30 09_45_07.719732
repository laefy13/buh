2023-11-30 09:45:07,768 Training with a single process on 1 device (cuda:0).
2023-11-30 09:45:07,981 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:45:07,981 Data processing configuration for current model + dataset:
2023-11-30 09:45:07,981 	input_size: (3, 259, 259)
2023-11-30 09:45:07,981 	interpolation: bicubic
2023-11-30 09:45:07,981 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:45:07,981 	std: (0.229, 0.224, 0.225)
2023-11-30 09:45:07,981 	crop_pct: 0.875
2023-11-30 09:45:07,981 	crop_mode: center
2023-11-30 09:45:10,587 AMP not enabled. Training in float32.
2023-11-30 09:45:10,629 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:45:12,750 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:45:13,948 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.317s,    9.65/s  (3.317s,    9.65/s)  LR: 1.000e-05  Data: 0.964 (0.964)
2023-11-30 09:45:14,208 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.261s,  122.78/s  (1.789s,   17.89/s)  LR: 1.000e-05  Data: 0.004 (0.484)
2023-11-30 09:45:14,469 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.261s,  122.84/s  (1.279s,   25.01/s)  LR: 1.000e-05  Data: 0.003 (0.323)
2023-11-30 09:45:14,731 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.16/s  (1.025s,   31.22/s)  LR: 1.000e-05  Data: 0.004 (0.244)
2023-11-30 09:45:14,996 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.265s,  120.89/s  (0.873s,   36.66/s)  LR: 1.000e-05  Data: 0.007 (0.196)
2023-11-30 09:45:15,256 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  122.89/s  (0.771s,   41.51/s)  LR: 1.000e-05  Data: 0.003 (0.164)
2023-11-30 09:45:15,517 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.76/s  (0.698s,   45.85/s)  LR: 1.000e-05  Data: 0.003 (0.141)
2023-11-30 09:45:15,778 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.39/s  (0.643s,   49.74/s)  LR: 1.000e-05  Data: 0.005 (0.124)
2023-11-30 09:45:16,040 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  121.94/s  (0.601s,   53.24/s)  LR: 1.000e-05  Data: 0.005 (0.111)
2023-11-30 09:45:16,303 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.263s,  121.87/s  (0.567s,   56.42/s)  LR: 1.000e-05  Data: 0.005 (0.100)
2023-11-30 09:45:16,565 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.19/s  (0.539s,   59.32/s)  LR: 1.000e-05  Data: 0.004 (0.091)
2023-11-30 09:45:16,827 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.262s,  122.07/s  (0.516s,   61.97/s)  LR: 1.000e-05  Data: 0.005 (0.084)
2023-11-30 09:45:17,088 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.42/s  (0.497s,   64.42/s)  LR: 1.000e-05  Data: 0.005 (0.078)
2023-11-30 09:45:17,351 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.05/s  (0.480s,   66.67/s)  LR: 1.000e-05  Data: 0.004 (0.073)
2023-11-30 09:45:17,612 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.34/s  (0.465s,   68.76/s)  LR: 1.000e-05  Data: 0.004 (0.068)
2023-11-30 09:45:17,875 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  121.99/s  (0.453s,   70.68/s)  LR: 1.000e-05  Data: 0.005 (0.064)
2023-11-30 09:45:18,138 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.263s,  121.56/s  (0.442s,   72.47/s)  LR: 1.000e-05  Data: 0.005 (0.061)
2023-11-30 09:45:18,401 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.263s,  121.81/s  (0.432s,   74.14/s)  LR: 1.000e-05  Data: 0.005 (0.058)
2023-11-30 09:45:18,663 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.263s,  121.86/s  (0.423s,   75.70/s)  LR: 1.000e-05  Data: 0.004 (0.055)
2023-11-30 09:45:18,926 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.263s,  121.85/s  (0.415s,   77.16/s)  LR: 1.000e-05  Data: 0.005 (0.052)
2023-11-30 09:45:19,189 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.264s,  121.38/s  (0.408s,   78.52/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 09:45:19,453 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.263s,  121.58/s  (0.401s,   79.80/s)  LR: 1.000e-05  Data: 0.005 (0.048)
2023-11-30 09:45:19,717 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.264s,  121.20/s  (0.395s,   81.01/s)  LR: 1.000e-05  Data: 0.005 (0.046)
2023-11-30 09:45:19,979 Train: 0 [  23/39 ( 61%)]  Loss: 0.942 (1.27)  Time: 0.262s,  122.05/s  (0.389s,   82.16/s)  LR: 1.000e-05  Data: 0.005 (0.044)
2023-11-30 09:45:20,241 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.15/s  (0.384s,   83.25/s)  LR: 1.000e-05  Data: 0.004 (0.043)
2023-11-30 09:45:20,505 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.264s,  121.15/s  (0.380s,   84.26/s)  LR: 1.000e-05  Data: 0.005 (0.041)
2023-11-30 09:45:20,767 Train: 0 [  26/39 ( 68%)]  Loss: 1.00 (1.25)  Time: 0.262s,  122.00/s  (0.375s,   85.24/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:45:21,030 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  121.91/s  (0.371s,   86.16/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:45:21,292 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  121.98/s  (0.368s,   87.05/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:45:21,554 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  121.95/s  (0.364s,   87.88/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 09:45:21,816 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.262s,  122.13/s  (0.361s,   88.69/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:45:22,078 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.53/s  (0.358s,   89.46/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:45:22,339 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.261s,  122.52/s  (0.355s,   90.20/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:45:22,600 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.261s,  122.70/s  (0.352s,   90.90/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:45:22,860 Train: 0 [  34/39 ( 89%)]  Loss: 0.983 (1.25)  Time: 0.261s,  122.77/s  (0.349s,   91.58/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:45:23,121 Train: 0 [  35/39 ( 92%)]  Loss: 1.12 (1.24)  Time: 0.261s,  122.82/s  (0.347s,   92.24/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:45:23,381 Train: 0 [  36/39 ( 95%)]  Loss: 1.30 (1.24)  Time: 0.261s,  122.83/s  (0.345s,   92.86/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:45:23,641 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.260s,  122.97/s  (0.342s,   93.46/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:45:23,897 Train: 0 [  38/39 (100%)]  Loss: 2.56 (1.27)  Time: 0.256s,  125.05/s  (0.340s,   94.07/s)  LR: 1.000e-05  Data: 0.000 (0.029)
2023-11-30 09:45:25,066 Test: [   0/39]  Time: 1.166 (1.166)  Loss:   0.191 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:25,131 Test: [   1/39]  Time: 0.065 (0.615)  Loss:   0.188 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:25,195 Test: [   2/39]  Time: 0.064 (0.431)  Loss:   0.191 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:25,566 Test: [   3/39]  Time: 0.371 (0.416)  Loss:   0.192 ( 0.190)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:25,704 Test: [   4/39]  Time: 0.138 (0.361)  Loss:   0.192 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:25,768 Test: [   5/39]  Time: 0.064 (0.311)  Loss:   0.195 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:25,832 Test: [   6/39]  Time: 0.064 (0.276)  Loss:   0.191 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:26,195 Test: [   7/39]  Time: 0.363 (0.287)  Loss:   0.190 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:26,369 Test: [   8/39]  Time: 0.174 (0.274)  Loss:   0.190 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:26,433 Test: [   9/39]  Time: 0.064 (0.253)  Loss:   0.192 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:26,497 Test: [  10/39]  Time: 0.064 (0.236)  Loss:   0.192 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:26,850 Test: [  11/39]  Time: 0.353 (0.246)  Loss:   0.190 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,031 Test: [  12/39]  Time: 0.182 (0.241)  Loss:   0.191 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,095 Test: [  13/39]  Time: 0.064 (0.228)  Loss:   0.191 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,159 Test: [  14/39]  Time: 0.064 (0.217)  Loss:   0.192 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,538 Test: [  15/39]  Time: 0.379 (0.227)  Loss:   0.192 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,707 Test: [  16/39]  Time: 0.169 (0.224)  Loss:   0.190 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,771 Test: [  17/39]  Time: 0.064 (0.215)  Loss:   0.188 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:27,835 Test: [  18/39]  Time: 0.064 (0.207)  Loss:   0.190 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:28,222 Test: [  19/39]  Time: 0.387 (0.216)  Loss:   0.189 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:28,408 Test: [  20/39]  Time: 0.186 (0.215)  Loss:   0.189 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:28,472 Test: [  21/39]  Time: 0.064 (0.208)  Loss:   0.188 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:28,536 Test: [  22/39]  Time: 0.064 (0.202)  Loss:   1.341 ( 0.241)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:45:28,828 Test: [  23/39]  Time: 0.293 (0.205)  Loss:   1.743 ( 0.303)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:45:29,036 Test: [  24/39]  Time: 0.208 (0.205)  Loss:   1.742 ( 0.361)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:45:29,100 Test: [  25/39]  Time: 0.064 (0.200)  Loss:   1.752 ( 0.414)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:45:29,163 Test: [  26/39]  Time: 0.064 (0.195)  Loss:   1.744 ( 0.463)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:45:29,459 Test: [  27/39]  Time: 0.296 (0.199)  Loss:   1.751 ( 0.509)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:45:29,620 Test: [  28/39]  Time: 0.160 (0.197)  Loss:   1.756 ( 0.552)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:45:29,683 Test: [  29/39]  Time: 0.064 (0.193)  Loss:   1.757 ( 0.593)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:45:29,747 Test: [  30/39]  Time: 0.064 (0.189)  Loss:   1.751 ( 0.630)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:45:30,089 Test: [  31/39]  Time: 0.342 (0.193)  Loss:   1.759 ( 0.665)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:45:30,231 Test: [  32/39]  Time: 0.142 (0.192)  Loss:   1.742 ( 0.698)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:45:30,295 Test: [  33/39]  Time: 0.064 (0.188)  Loss:   1.750 ( 0.729)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:45:30,358 Test: [  34/39]  Time: 0.064 (0.185)  Loss:   1.744 ( 0.758)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:45:30,464 Test: [  35/39]  Time: 0.106 (0.182)  Loss:   1.756 ( 0.785)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:45:30,747 Test: [  36/39]  Time: 0.283 (0.185)  Loss:   1.705 ( 0.810)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:45:31,412 Test: [  37/39]  Time: 0.665 (0.198)  Loss:   1.479 ( 0.828)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 09:45:31,473 Test: [  38/39]  Time: 0.061 (0.194)  Loss:   1.539 ( 0.846)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 09:45:31,669 Test: [  39/39]  Time: 0.196 (0.194)  Loss:   1.304 ( 0.847)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 09:45:31,875 Current checkpoints:
 ('./output/train/20231130-094510-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 09:45:31,875 *** Best metric: 57.28 (epoch 0)

2023-11-30 10:23:31,009 Training with a single process on 1 device (cuda:0).
2023-11-30 10:23:31,226 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:23:31,226 Data processing configuration for current model + dataset:
2023-11-30 10:23:31,226 	input_size: (3, 259, 259)
2023-11-30 10:23:31,226 	interpolation: bicubic
2023-11-30 10:23:31,226 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:23:31,226 	std: (0.229, 0.224, 0.225)
2023-11-30 10:23:31,226 	crop_pct: 0.875
2023-11-30 10:23:31,226 	crop_mode: center
2023-11-30 10:23:33,629 AMP not enabled. Training in float32.
2023-11-30 10:23:33,673 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:23:35,735 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:23:36,939 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.263s,    9.81/s  (3.263s,    9.81/s)  LR: 1.000e-05  Data: 0.908 (0.908)
2023-11-30 10:23:37,202 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.263s,  121.66/s  (1.763s,   18.15/s)  LR: 1.000e-05  Data: 0.005 (0.457)
2023-11-30 10:23:37,462 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.261s,  122.72/s  (1.262s,   25.35/s)  LR: 1.000e-05  Data: 0.003 (0.305)
2023-11-30 10:23:37,749 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.286s,  111.75/s  (1.018s,   31.42/s)  LR: 1.000e-05  Data: 0.028 (0.236)
2023-11-30 10:23:38,009 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.260s,  122.94/s  (0.867s,   36.92/s)  LR: 1.000e-05  Data: 0.003 (0.189)
2023-11-30 10:23:38,270 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.261s,  122.61/s  (0.766s,   41.79/s)  LR: 1.000e-05  Data: 0.003 (0.158)
2023-11-30 10:23:38,530 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.260s,  122.90/s  (0.694s,   46.14/s)  LR: 1.000e-05  Data: 0.003 (0.136)
2023-11-30 10:23:38,793 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.262s,  122.10/s  (0.640s,   50.03/s)  LR: 1.000e-05  Data: 0.005 (0.120)
2023-11-30 10:23:39,053 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.260s,  122.93/s  (0.597s,   53.56/s)  LR: 1.000e-05  Data: 0.003 (0.107)
2023-11-30 10:23:39,316 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.263s,  121.49/s  (0.564s,   56.73/s)  LR: 1.000e-05  Data: 0.004 (0.096)
2023-11-30 10:23:39,579 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.263s,  121.78/s  (0.537s,   59.62/s)  LR: 1.000e-05  Data: 0.005 (0.088)
2023-11-30 10:23:39,841 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.262s,  121.91/s  (0.514s,   62.28/s)  LR: 1.000e-05  Data: 0.005 (0.081)
2023-11-30 10:23:40,102 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.61/s  (0.494s,   64.73/s)  LR: 1.000e-05  Data: 0.004 (0.075)
2023-11-30 10:23:40,364 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.16/s  (0.478s,   66.98/s)  LR: 1.000e-05  Data: 0.004 (0.070)
2023-11-30 10:23:40,627 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.03/s  (0.463s,   69.05/s)  LR: 1.000e-05  Data: 0.004 (0.066)
2023-11-30 10:23:40,889 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.13/s  (0.451s,   70.98/s)  LR: 1.000e-05  Data: 0.004 (0.062)
2023-11-30 10:23:41,150 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.45/s  (0.440s,   72.78/s)  LR: 1.000e-05  Data: 0.004 (0.058)
2023-11-30 10:23:41,412 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.15/s  (0.430s,   74.45/s)  LR: 1.000e-05  Data: 0.004 (0.055)
2023-11-30 10:23:41,674 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.262s,  122.00/s  (0.421s,   76.01/s)  LR: 1.000e-05  Data: 0.004 (0.053)
2023-11-30 10:23:41,936 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.06/s  (0.413s,   77.47/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 10:23:42,198 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.261s,  122.39/s  (0.406s,   78.85/s)  LR: 1.000e-05  Data: 0.004 (0.048)
2023-11-30 10:23:42,460 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.08/s  (0.399s,   80.14/s)  LR: 1.000e-05  Data: 0.004 (0.046)
2023-11-30 10:23:42,723 Train: 0 [  22/39 ( 58%)]  Loss: 1.85 (1.28)  Time: 0.263s,  121.52/s  (0.393s,   81.34/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 10:23:42,986 Train: 0 [  23/39 ( 61%)]  Loss: 0.940 (1.27)  Time: 0.262s,  122.04/s  (0.388s,   82.49/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 10:23:43,247 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.25/s  (0.383s,   83.58/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 10:23:43,510 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.03/s  (0.378s,   84.60/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 10:23:43,772 Train: 0 [  26/39 ( 68%)]  Loss: 0.996 (1.25)  Time: 0.262s,  122.01/s  (0.374s,   85.58/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 10:23:44,034 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.262s,  122.02/s  (0.370s,   86.50/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 10:23:44,298 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.264s,  121.37/s  (0.366s,   87.36/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 10:23:44,560 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  122.08/s  (0.363s,   88.20/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 10:23:44,823 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.263s,  121.80/s  (0.360s,   88.99/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 10:23:45,086 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.263s,  121.49/s  (0.357s,   89.74/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 10:23:45,347 Train: 0 [  32/39 ( 84%)]  Loss: 1.04 (1.25)  Time: 0.261s,  122.41/s  (0.354s,   90.47/s)  LR: 1.000e-05  Data: 0.005 (0.032)
2023-11-30 10:23:45,609 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.261s,  122.37/s  (0.351s,   91.17/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:23:45,870 Train: 0 [  34/39 ( 89%)]  Loss: 0.976 (1.25)  Time: 0.261s,  122.49/s  (0.348s,   91.84/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:23:46,132 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.262s,  122.26/s  (0.346s,   92.48/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 10:23:46,393 Train: 0 [  36/39 ( 95%)]  Loss: 1.23 (1.24)  Time: 0.261s,  122.72/s  (0.344s,   93.10/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 10:23:46,653 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.68/s  (0.342s,   93.70/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 10:23:46,910 Train: 0 [  38/39 (100%)]  Loss: 2.60 (1.27)  Time: 0.256s,  124.76/s  (0.339s,   94.30/s)  LR: 1.000e-05  Data: 0.000 (0.028)
2023-11-30 10:23:48,054 Test: [   0/39]  Time: 1.141 (1.141)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:48,118 Test: [   1/39]  Time: 0.064 (0.603)  Loss:   0.178 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:48,182 Test: [   2/39]  Time: 0.064 (0.423)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:48,563 Test: [   3/39]  Time: 0.381 (0.412)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:48,682 Test: [   4/39]  Time: 0.120 (0.354)  Loss:   0.182 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:48,746 Test: [   5/39]  Time: 0.064 (0.305)  Loss:   0.185 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:48,810 Test: [   6/39]  Time: 0.064 (0.271)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:49,156 Test: [   7/39]  Time: 0.347 (0.280)  Loss:   0.179 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:49,321 Test: [   8/39]  Time: 0.165 (0.268)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:49,385 Test: [   9/39]  Time: 0.064 (0.247)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:49,449 Test: [  10/39]  Time: 0.064 (0.230)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:49,804 Test: [  11/39]  Time: 0.355 (0.241)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:49,973 Test: [  12/39]  Time: 0.170 (0.235)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:50,039 Test: [  13/39]  Time: 0.065 (0.223)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:50,103 Test: [  14/39]  Time: 0.064 (0.213)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:50,483 Test: [  15/39]  Time: 0.381 (0.223)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:50,635 Test: [  16/39]  Time: 0.152 (0.219)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:50,699 Test: [  17/39]  Time: 0.064 (0.210)  Loss:   0.178 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:50,762 Test: [  18/39]  Time: 0.064 (0.203)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:51,106 Test: [  19/39]  Time: 0.343 (0.210)  Loss:   0.179 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:51,271 Test: [  20/39]  Time: 0.165 (0.207)  Loss:   0.179 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:51,334 Test: [  21/39]  Time: 0.064 (0.201)  Loss:   0.178 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:51,398 Test: [  22/39]  Time: 0.064 (0.195)  Loss:   1.376 ( 0.232)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:23:51,716 Test: [  23/39]  Time: 0.318 (0.200)  Loss:   1.793 ( 0.297)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:23:51,908 Test: [  24/39]  Time: 0.192 (0.200)  Loss:   1.791 ( 0.357)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:23:51,972 Test: [  25/39]  Time: 0.064 (0.195)  Loss:   1.802 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:23:52,036 Test: [  26/39]  Time: 0.064 (0.190)  Loss:   1.794 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:23:52,344 Test: [  27/39]  Time: 0.309 (0.194)  Loss:   1.800 ( 0.512)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:23:52,491 Test: [  28/39]  Time: 0.146 (0.192)  Loss:   1.806 ( 0.556)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:23:52,556 Test: [  29/39]  Time: 0.065 (0.188)  Loss:   1.806 ( 0.598)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:23:52,621 Test: [  30/39]  Time: 0.066 (0.184)  Loss:   1.801 ( 0.637)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:23:52,965 Test: [  31/39]  Time: 0.343 (0.189)  Loss:   1.809 ( 0.673)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:23:53,092 Test: [  32/39]  Time: 0.127 (0.187)  Loss:   1.791 ( 0.707)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:23:53,155 Test: [  33/39]  Time: 0.064 (0.184)  Loss:   1.800 ( 0.739)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:23:53,219 Test: [  34/39]  Time: 0.064 (0.180)  Loss:   1.793 ( 0.769)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:23:53,335 Test: [  35/39]  Time: 0.116 (0.178)  Loss:   1.806 ( 0.798)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:23:53,600 Test: [  36/39]  Time: 0.266 (0.181)  Loss:   1.758 ( 0.824)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:23:54,292 Test: [  37/39]  Time: 0.692 (0.194)  Loss:   1.534 ( 0.843)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 10:23:54,353 Test: [  38/39]  Time: 0.061 (0.191)  Loss:   1.592 ( 0.862)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 10:23:54,549 Test: [  39/39]  Time: 0.196 (0.191)  Loss:   1.354 ( 0.863)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 10:23:54,749 Current checkpoints:
 ('./output/train/20231130-102333-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 10:23:54,750 *** Best metric: 57.12 (epoch 0)

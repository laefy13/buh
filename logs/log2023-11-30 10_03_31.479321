2023-11-30 10:03:31,527 Training with a single process on 1 device (cuda:0).
2023-11-30 10:03:31,758 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:03:31,758 Data processing configuration for current model + dataset:
2023-11-30 10:03:31,758 	input_size: (3, 259, 259)
2023-11-30 10:03:31,758 	interpolation: bicubic
2023-11-30 10:03:31,758 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:03:31,758 	std: (0.229, 0.224, 0.225)
2023-11-30 10:03:31,758 	crop_pct: 0.875
2023-11-30 10:03:31,758 	crop_mode: center
2023-11-30 10:03:34,212 AMP not enabled. Training in float32.
2023-11-30 10:03:34,254 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:03:36,375 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:03:37,512 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.256s,    9.83/s  (3.256s,    9.83/s)  LR: 1.000e-05  Data: 0.987 (0.987)
2023-11-30 10:03:37,775 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.41/s  (1.760s,   18.19/s)  LR: 1.000e-05  Data: 0.007 (0.497)
2023-11-30 10:03:38,035 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.20/s  (1.260s,   25.40/s)  LR: 1.000e-05  Data: 0.003 (0.332)
2023-11-30 10:03:38,297 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.43/s  (1.010s,   31.68/s)  LR: 1.000e-05  Data: 0.005 (0.250)
2023-11-30 10:03:38,558 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.262s,  122.34/s  (0.860s,   37.19/s)  LR: 1.000e-05  Data: 0.005 (0.201)
2023-11-30 10:03:38,820 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.262s,  122.02/s  (0.761s,   42.07/s)  LR: 1.000e-05  Data: 0.005 (0.168)
2023-11-30 10:03:39,080 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.260s,  123.13/s  (0.689s,   46.44/s)  LR: 1.000e-05  Data: 0.003 (0.145)
2023-11-30 10:03:39,344 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.264s,  121.43/s  (0.636s,   50.32/s)  LR: 1.000e-05  Data: 0.006 (0.127)
2023-11-30 10:03:39,604 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.260s,  123.05/s  (0.594s,   53.86/s)  LR: 1.000e-05  Data: 0.003 (0.114)
2023-11-30 10:03:39,865 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.36/s  (0.561s,   57.05/s)  LR: 1.000e-05  Data: 0.004 (0.103)
2023-11-30 10:03:40,128 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  121.92/s  (0.534s,   59.95/s)  LR: 1.000e-05  Data: 0.005 (0.094)
2023-11-30 10:03:40,389 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.70/s  (0.511s,   62.62/s)  LR: 1.000e-05  Data: 0.004 (0.086)
2023-11-30 10:03:40,649 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.72/s  (0.492s,   65.07/s)  LR: 1.000e-05  Data: 0.004 (0.080)
2023-11-30 10:03:40,910 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.69/s  (0.475s,   67.33/s)  LR: 1.000e-05  Data: 0.004 (0.075)
2023-11-30 10:03:41,172 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.34/s  (0.461s,   69.41/s)  LR: 1.000e-05  Data: 0.004 (0.070)
2023-11-30 10:03:41,433 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.261s,  122.48/s  (0.449s,   71.34/s)  LR: 1.000e-05  Data: 0.004 (0.066)
2023-11-30 10:03:41,695 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.31/s  (0.438s,   73.13/s)  LR: 1.000e-05  Data: 0.004 (0.062)
2023-11-30 10:03:41,956 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.66/s  (0.428s,   74.81/s)  LR: 1.000e-05  Data: 0.004 (0.059)
2023-11-30 10:03:42,223 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.267s,  119.86/s  (0.419s,   76.32/s)  LR: 1.000e-05  Data: 0.004 (0.056)
2023-11-30 10:03:42,486 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.263s,  121.67/s  (0.411s,   77.77/s)  LR: 1.000e-05  Data: 0.005 (0.053)
2023-11-30 10:03:42,748 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.263s,  121.89/s  (0.404s,   79.14/s)  LR: 1.000e-05  Data: 0.004 (0.051)
2023-11-30 10:03:43,010 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  121.98/s  (0.398s,   80.42/s)  LR: 1.000e-05  Data: 0.004 (0.049)
2023-11-30 10:03:43,272 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.15/s  (0.392s,   81.63/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 10:03:43,535 Train: 0 [  23/39 ( 61%)]  Loss: 0.942 (1.27)  Time: 0.262s,  122.03/s  (0.387s,   82.77/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 10:03:43,797 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.262s,  122.15/s  (0.382s,   83.86/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 10:03:44,059 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  121.98/s  (0.377s,   84.88/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 10:03:44,324 Train: 0 [  26/39 ( 68%)]  Loss: 0.999 (1.25)  Time: 0.265s,  120.56/s  (0.373s,   85.82/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 10:03:44,589 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.265s,  120.98/s  (0.369s,   86.72/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 10:03:44,851 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.25/s  (0.365s,   87.59/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 10:03:45,112 Train: 0 [  29/39 ( 76%)]  Loss: 0.990 (1.25)  Time: 0.262s,  122.34/s  (0.362s,   88.43/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 10:03:45,374 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.262s,  122.36/s  (0.359s,   89.23/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 10:03:45,634 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.260s,  122.85/s  (0.356s,   90.00/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 10:03:45,895 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.261s,  122.82/s  (0.353s,   90.73/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 10:03:46,155 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.260s,  123.00/s  (0.350s,   91.44/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 10:03:46,415 Train: 0 [  34/39 ( 89%)]  Loss: 0.979 (1.25)  Time: 0.260s,  123.08/s  (0.347s,   92.12/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:03:46,675 Train: 0 [  35/39 ( 92%)]  Loss: 1.11 (1.24)  Time: 0.260s,  123.12/s  (0.345s,   92.77/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:03:46,935 Train: 0 [  36/39 ( 95%)]  Loss: 1.27 (1.24)  Time: 0.260s,  123.03/s  (0.343s,   93.39/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:03:47,195 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.82/s  (0.341s,   93.98/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 10:03:47,451 Train: 0 [  38/39 (100%)]  Loss: 2.59 (1.27)  Time: 0.256s,  124.99/s  (0.338s,   94.58/s)  LR: 1.000e-05  Data: 0.000 (0.029)
2023-11-30 10:03:48,552 Test: [   0/39]  Time: 1.097 (1.097)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:48,616 Test: [   1/39]  Time: 0.064 (0.581)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:48,680 Test: [   2/39]  Time: 0.064 (0.408)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,081 Test: [   3/39]  Time: 0.401 (0.406)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,175 Test: [   4/39]  Time: 0.095 (0.344)  Loss:   0.186 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,239 Test: [   5/39]  Time: 0.064 (0.297)  Loss:   0.189 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,303 Test: [   6/39]  Time: 0.064 (0.264)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,675 Test: [   7/39]  Time: 0.373 (0.278)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,833 Test: [   8/39]  Time: 0.157 (0.264)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,897 Test: [   9/39]  Time: 0.064 (0.244)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:49,960 Test: [  10/39]  Time: 0.064 (0.228)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:50,322 Test: [  11/39]  Time: 0.361 (0.239)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:50,478 Test: [  12/39]  Time: 0.156 (0.233)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:50,543 Test: [  13/39]  Time: 0.065 (0.221)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:50,607 Test: [  14/39]  Time: 0.064 (0.210)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:50,991 Test: [  15/39]  Time: 0.384 (0.221)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,126 Test: [  16/39]  Time: 0.136 (0.216)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,190 Test: [  17/39]  Time: 0.064 (0.208)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,255 Test: [  18/39]  Time: 0.065 (0.200)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,610 Test: [  19/39]  Time: 0.355 (0.208)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,753 Test: [  20/39]  Time: 0.142 (0.205)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,816 Test: [  21/39]  Time: 0.064 (0.198)  Loss:   0.181 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:51,882 Test: [  22/39]  Time: 0.066 (0.192)  Loss:   1.362 ( 0.235)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:52,198 Test: [  23/39]  Time: 0.316 (0.198)  Loss:   1.774 ( 0.299)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:03:52,393 Test: [  24/39]  Time: 0.195 (0.198)  Loss:   1.773 ( 0.358)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:03:52,457 Test: [  25/39]  Time: 0.064 (0.192)  Loss:   1.783 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:03:52,521 Test: [  26/39]  Time: 0.064 (0.188)  Loss:   1.775 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:03:52,830 Test: [  27/39]  Time: 0.309 (0.192)  Loss:   1.782 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:03:52,970 Test: [  28/39]  Time: 0.140 (0.190)  Loss:   1.787 ( 0.555)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:03:53,034 Test: [  29/39]  Time: 0.064 (0.186)  Loss:   1.787 ( 0.596)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:03:53,099 Test: [  30/39]  Time: 0.065 (0.182)  Loss:   1.782 ( 0.634)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:03:53,459 Test: [  31/39]  Time: 0.360 (0.188)  Loss:   1.790 ( 0.670)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:03:53,588 Test: [  32/39]  Time: 0.130 (0.186)  Loss:   1.772 ( 0.704)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:03:53,652 Test: [  33/39]  Time: 0.064 (0.182)  Loss:   1.781 ( 0.735)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:03:53,716 Test: [  34/39]  Time: 0.064 (0.179)  Loss:   1.775 ( 0.765)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:03:53,851 Test: [  35/39]  Time: 0.135 (0.178)  Loss:   1.787 ( 0.793)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:03:54,127 Test: [  36/39]  Time: 0.276 (0.180)  Loss:   1.735 ( 0.819)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:03:54,800 Test: [  37/39]  Time: 0.673 (0.193)  Loss:   1.507 ( 0.837)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 10:03:54,861 Test: [  38/39]  Time: 0.061 (0.190)  Loss:   1.567 ( 0.856)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 10:03:55,065 Test: [  39/39]  Time: 0.203 (0.190)  Loss:   1.331 ( 0.856)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 10:03:55,263 Current checkpoints:
 ('./output/train/20231130-100334-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 10:03:55,264 *** Best metric: 57.12 (epoch 0)

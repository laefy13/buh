2023-11-30 09:51:12,230 Training with a single process on 1 device (cuda:0).
2023-11-30 09:51:12,451 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:51:12,451 Data processing configuration for current model + dataset:
2023-11-30 09:51:12,451 	input_size: (3, 259, 259)
2023-11-30 09:51:12,451 	interpolation: bicubic
2023-11-30 09:51:12,451 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:51:12,451 	std: (0.229, 0.224, 0.225)
2023-11-30 09:51:12,451 	crop_pct: 0.875
2023-11-30 09:51:12,451 	crop_mode: center
2023-11-30 09:51:15,027 AMP not enabled. Training in float32.
2023-11-30 09:51:15,075 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:51:17,225 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:51:18,328 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.250s,    9.85/s  (3.250s,    9.85/s)  LR: 1.000e-05  Data: 1.000 (1.000)
2023-11-30 09:51:18,593 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.265s,  120.59/s  (1.758s,   18.21/s)  LR: 1.000e-05  Data: 0.008 (0.504)
2023-11-30 09:51:18,854 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  122.87/s  (1.258s,   25.43/s)  LR: 1.000e-05  Data: 0.003 (0.337)
2023-11-30 09:51:19,116 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.04/s  (1.009s,   31.70/s)  LR: 1.000e-05  Data: 0.004 (0.254)
2023-11-30 09:51:19,381 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.265s,  120.92/s  (0.860s,   37.19/s)  LR: 1.000e-05  Data: 0.007 (0.204)
2023-11-30 09:51:19,641 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  122.88/s  (0.760s,   42.08/s)  LR: 1.000e-05  Data: 0.003 (0.171)
2023-11-30 09:51:19,911 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.270s,  118.47/s  (0.690s,   46.35/s)  LR: 1.000e-05  Data: 0.003 (0.147)
2023-11-30 09:51:20,241 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.330s,   97.05/s  (0.645s,   49.59/s)  LR: 1.000e-05  Data: 0.004 (0.129)
2023-11-30 09:51:20,503 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.11/s  (0.603s,   53.09/s)  LR: 1.000e-05  Data: 0.005 (0.115)
2023-11-30 09:51:20,765 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.23/s  (0.569s,   56.27/s)  LR: 1.000e-05  Data: 0.004 (0.104)
2023-11-30 09:51:21,027 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.22/s  (0.541s,   59.18/s)  LR: 1.000e-05  Data: 0.004 (0.095)
2023-11-30 09:51:21,289 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.262s,  122.06/s  (0.518s,   61.83/s)  LR: 1.000e-05  Data: 0.005 (0.087)
2023-11-30 09:51:21,551 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.04/s  (0.498s,   64.27/s)  LR: 1.000e-05  Data: 0.005 (0.081)
2023-11-30 09:51:21,813 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.07/s  (0.481s,   66.52/s)  LR: 1.000e-05  Data: 0.005 (0.076)
2023-11-30 09:51:22,075 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.07/s  (0.466s,   68.60/s)  LR: 1.000e-05  Data: 0.005 (0.071)
2023-11-30 09:51:22,337 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.08/s  (0.454s,   70.53/s)  LR: 1.000e-05  Data: 0.004 (0.067)
2023-11-30 09:51:22,599 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.14/s  (0.442s,   72.33/s)  LR: 1.000e-05  Data: 0.004 (0.063)
2023-11-30 09:51:22,862 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.06/s  (0.432s,   74.01/s)  LR: 1.000e-05  Data: 0.005 (0.060)
2023-11-30 09:51:23,124 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.262s,  122.01/s  (0.423s,   75.57/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 09:51:23,399 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.275s,  116.38/s  (0.416s,   76.92/s)  LR: 1.000e-05  Data: 0.005 (0.054)
2023-11-30 09:51:23,690 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.291s,  109.98/s  (0.410s,   78.04/s)  LR: 1.000e-05  Data: 0.004 (0.052)
2023-11-30 09:51:23,976 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.287s,  111.62/s  (0.404s,   79.12/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 09:51:24,259 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.283s,  113.09/s  (0.399s,   80.17/s)  LR: 1.000e-05  Data: 0.005 (0.048)
2023-11-30 09:51:24,549 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.289s,  110.56/s  (0.395s,   81.09/s)  LR: 1.000e-05  Data: 0.005 (0.046)
2023-11-30 09:51:24,831 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.283s,  113.20/s  (0.390s,   82.02/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 09:51:25,114 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.283s,  113.10/s  (0.386s,   82.90/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:51:25,400 Train: 0 [  26/39 ( 68%)]  Loss: 0.998 (1.25)  Time: 0.286s,  111.91/s  (0.382s,   83.70/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 09:51:25,679 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.278s,  114.96/s  (0.379s,   84.53/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 09:51:25,958 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.279s,  114.54/s  (0.375s,   85.30/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:51:26,241 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.283s,  113.01/s  (0.372s,   86.00/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 09:51:26,527 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.286s,  111.78/s  (0.369s,   86.64/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 09:51:26,807 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.279s,  114.65/s  (0.367s,   87.31/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:51:27,087 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.280s,  114.20/s  (0.364s,   87.94/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:51:27,373 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.287s,  111.63/s  (0.362s,   88.49/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:51:27,672 Train: 0 [  34/39 ( 89%)]  Loss: 0.978 (1.25)  Time: 0.299s,  107.19/s  (0.360s,   88.93/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:51:27,937 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.265s,  120.98/s  (0.357s,   89.59/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:51:28,289 Train: 0 [  36/39 ( 95%)]  Loss: 1.26 (1.24)  Time: 0.352s,   90.88/s  (0.357s,   89.63/s)  LR: 1.000e-05  Data: 0.064 (0.033)
2023-11-30 09:51:28,550 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.262s,  122.35/s  (0.355s,   90.26/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:51:28,870 Train: 0 [  38/39 (100%)]  Loss: 2.59 (1.27)  Time: 0.320s,   99.93/s  (0.354s,   90.49/s)  LR: 1.000e-05  Data: 0.000 (0.031)
2023-11-30 09:51:30,443 Test: [   0/39]  Time: 1.569 (1.569)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:30,517 Test: [   1/39]  Time: 0.074 (0.821)  Loss:   0.181 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:30,587 Test: [   2/39]  Time: 0.070 (0.571)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:30,933 Test: [   3/39]  Time: 0.347 (0.515)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,083 Test: [   4/39]  Time: 0.150 (0.442)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,147 Test: [   5/39]  Time: 0.064 (0.379)  Loss:   0.188 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,211 Test: [   6/39]  Time: 0.064 (0.334)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,516 Test: [   7/39]  Time: 0.305 (0.330)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,728 Test: [   8/39]  Time: 0.211 (0.317)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,792 Test: [   9/39]  Time: 0.064 (0.292)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:31,856 Test: [  10/39]  Time: 0.064 (0.271)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:32,177 Test: [  11/39]  Time: 0.322 (0.275)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:32,384 Test: [  12/39]  Time: 0.206 (0.270)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:32,448 Test: [  13/39]  Time: 0.064 (0.255)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:32,512 Test: [  14/39]  Time: 0.064 (0.242)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:32,859 Test: [  15/39]  Time: 0.347 (0.249)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,047 Test: [  16/39]  Time: 0.188 (0.245)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,111 Test: [  17/39]  Time: 0.064 (0.235)  Loss:   0.181 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,174 Test: [  18/39]  Time: 0.064 (0.226)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,479 Test: [  19/39]  Time: 0.305 (0.230)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,677 Test: [  20/39]  Time: 0.198 (0.229)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,741 Test: [  21/39]  Time: 0.064 (0.221)  Loss:   0.181 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:33,805 Test: [  22/39]  Time: 0.064 (0.214)  Loss:   1.364 ( 0.235)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:51:34,068 Test: [  23/39]  Time: 0.263 (0.216)  Loss:   1.776 ( 0.299)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:51:34,302 Test: [  24/39]  Time: 0.234 (0.217)  Loss:   1.774 ( 0.358)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:51:34,366 Test: [  25/39]  Time: 0.064 (0.211)  Loss:   1.784 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:51:34,430 Test: [  26/39]  Time: 0.064 (0.206)  Loss:   1.776 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:51:34,717 Test: [  27/39]  Time: 0.287 (0.209)  Loss:   1.783 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:51:34,906 Test: [  28/39]  Time: 0.188 (0.208)  Loss:   1.788 ( 0.555)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:51:34,970 Test: [  29/39]  Time: 0.064 (0.203)  Loss:   1.789 ( 0.596)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:51:35,035 Test: [  30/39]  Time: 0.066 (0.199)  Loss:   1.783 ( 0.634)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:51:35,345 Test: [  31/39]  Time: 0.309 (0.202)  Loss:   1.791 ( 0.670)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:51:35,527 Test: [  32/39]  Time: 0.182 (0.202)  Loss:   1.774 ( 0.704)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:51:35,591 Test: [  33/39]  Time: 0.064 (0.198)  Loss:   1.782 ( 0.736)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:51:35,654 Test: [  34/39]  Time: 0.064 (0.194)  Loss:   1.776 ( 0.765)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:51:35,721 Test: [  35/39]  Time: 0.067 (0.190)  Loss:   1.788 ( 0.794)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:51:36,034 Test: [  36/39]  Time: 0.312 (0.193)  Loss:   1.738 ( 0.819)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:51:36,675 Test: [  37/39]  Time: 0.642 (0.205)  Loss:   1.512 ( 0.837)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 09:51:36,736 Test: [  38/39]  Time: 0.061 (0.202)  Loss:   1.572 ( 0.856)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 09:51:36,914 Test: [  39/39]  Time: 0.177 (0.201)  Loss:   1.335 ( 0.857)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 09:51:37,130 Current checkpoints:
 ('./output/train/20231130-095115-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 09:51:37,130 *** Best metric: 57.12 (epoch 0)

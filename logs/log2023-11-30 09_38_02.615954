2023-11-30 09:38:02,670 Training with a single process on 1 device (cuda:0).
2023-11-30 09:38:02,902 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:38:02,902 Data processing configuration for current model + dataset:
2023-11-30 09:38:02,902 	input_size: (3, 259, 259)
2023-11-30 09:38:02,902 	interpolation: bicubic
2023-11-30 09:38:02,903 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:38:02,903 	std: (0.229, 0.224, 0.225)
2023-11-30 09:38:02,903 	crop_pct: 0.875
2023-11-30 09:38:02,903 	crop_mode: center
2023-11-30 09:38:05,369 AMP not enabled. Training in float32.
2023-11-30 09:38:05,412 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:38:07,513 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:38:08,720 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.305s,    9.68/s  (3.305s,    9.68/s)  LR: 1.000e-05  Data: 0.937 (0.937)
2023-11-30 09:38:08,985 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.12/s  (1.785s,   17.93/s)  LR: 1.000e-05  Data: 0.007 (0.472)
2023-11-30 09:38:09,244 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.18/s  (1.276s,   25.07/s)  LR: 1.000e-05  Data: 0.003 (0.316)
2023-11-30 09:38:09,506 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.47/s  (1.023s,   31.29/s)  LR: 1.000e-05  Data: 0.005 (0.238)
2023-11-30 09:38:09,768 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.263s,  121.77/s  (0.871s,   36.75/s)  LR: 1.000e-05  Data: 0.006 (0.191)
2023-11-30 09:38:10,028 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.19/s  (0.769s,   41.62/s)  LR: 1.000e-05  Data: 0.003 (0.160)
2023-11-30 09:38:10,289 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.48/s  (0.696s,   45.95/s)  LR: 1.000e-05  Data: 0.004 (0.138)
2023-11-30 09:38:10,550 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.68/s  (0.642s,   49.85/s)  LR: 1.000e-05  Data: 0.005 (0.121)
2023-11-30 09:38:10,812 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.21/s  (0.600s,   53.36/s)  LR: 1.000e-05  Data: 0.005 (0.108)
2023-11-30 09:38:11,073 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.261s,  122.45/s  (0.566s,   56.55/s)  LR: 1.000e-05  Data: 0.005 (0.098)
2023-11-30 09:38:11,334 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.68/s  (0.538s,   59.47/s)  LR: 1.000e-05  Data: 0.004 (0.089)
2023-11-30 09:38:11,595 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.70/s  (0.515s,   62.13/s)  LR: 1.000e-05  Data: 0.005 (0.082)
2023-11-30 09:38:11,856 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.63/s  (0.495s,   64.59/s)  LR: 1.000e-05  Data: 0.004 (0.076)
2023-11-30 09:38:12,117 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.57/s  (0.479s,   66.84/s)  LR: 1.000e-05  Data: 0.004 (0.071)
2023-11-30 09:38:12,378 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.261s,  122.56/s  (0.464s,   68.93/s)  LR: 1.000e-05  Data: 0.004 (0.067)
2023-11-30 09:38:12,640 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.35/s  (0.452s,   70.87/s)  LR: 1.000e-05  Data: 0.005 (0.063)
2023-11-30 09:38:12,902 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.00/s  (0.440s,   72.66/s)  LR: 1.000e-05  Data: 0.005 (0.059)
2023-11-30 09:38:13,163 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.60/s  (0.430s,   74.34/s)  LR: 1.000e-05  Data: 0.004 (0.056)
2023-11-30 09:38:13,424 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.261s,  122.67/s  (0.422s,   75.91/s)  LR: 1.000e-05  Data: 0.004 (0.054)
2023-11-30 09:38:13,685 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.40/s  (0.414s,   77.38/s)  LR: 1.000e-05  Data: 0.005 (0.051)
2023-11-30 09:38:13,947 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.262s,  122.31/s  (0.406s,   78.76/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 09:38:14,209 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  121.99/s  (0.400s,   80.05/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 09:38:14,470 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.261s,  122.55/s  (0.394s,   81.28/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 09:38:14,732 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.262s,  122.24/s  (0.388s,   82.43/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:38:14,994 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.25/s  (0.383s,   83.52/s)  LR: 1.000e-05  Data: 0.005 (0.042)
2023-11-30 09:38:15,256 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.32/s  (0.378s,   84.55/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:38:15,517 Train: 0 [  26/39 ( 68%)]  Loss: 0.998 (1.25)  Time: 0.261s,  122.58/s  (0.374s,   85.53/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:38:15,779 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  121.92/s  (0.370s,   86.45/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 09:38:16,041 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.33/s  (0.366s,   87.34/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:38:16,302 Train: 0 [  29/39 ( 76%)]  Loss: 0.988 (1.25)  Time: 0.262s,  122.26/s  (0.363s,   88.18/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 09:38:16,563 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.261s,  122.78/s  (0.360s,   88.98/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:38:16,823 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.260s,  123.12/s  (0.356s,   89.76/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:38:17,083 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.260s,  123.22/s  (0.354s,   90.51/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:38:17,343 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.260s,  123.04/s  (0.351s,   91.22/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:38:17,602 Train: 0 [  34/39 ( 89%)]  Loss: 0.978 (1.25)  Time: 0.260s,  123.24/s  (0.348s,   91.90/s)  LR: 1.000e-05  Data: 0.003 (0.031)
2023-11-30 09:38:17,862 Train: 0 [  35/39 ( 92%)]  Loss: 1.12 (1.24)  Time: 0.260s,  123.22/s  (0.346s,   92.55/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:38:18,123 Train: 0 [  36/39 ( 95%)]  Loss: 1.25 (1.24)  Time: 0.261s,  122.50/s  (0.343s,   93.17/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:38:18,384 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.58/s  (0.341s,   93.76/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 09:38:18,641 Train: 0 [  38/39 (100%)]  Loss: 2.58 (1.27)  Time: 0.257s,  124.74/s  (0.339s,   94.36/s)  LR: 1.000e-05  Data: 0.000 (0.028)
2023-11-30 09:38:19,832 Test: [   0/39]  Time: 1.187 (1.187)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:19,897 Test: [   1/39]  Time: 0.065 (0.626)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:19,961 Test: [   2/39]  Time: 0.064 (0.439)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:20,332 Test: [   3/39]  Time: 0.371 (0.422)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:20,471 Test: [   4/39]  Time: 0.139 (0.365)  Loss:   0.187 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:20,535 Test: [   5/39]  Time: 0.064 (0.315)  Loss:   0.190 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:20,599 Test: [   6/39]  Time: 0.064 (0.279)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:20,914 Test: [   7/39]  Time: 0.316 (0.284)  Loss:   0.184 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,121 Test: [   8/39]  Time: 0.207 (0.275)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,185 Test: [   9/39]  Time: 0.064 (0.254)  Loss:   0.187 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,249 Test: [  10/39]  Time: 0.064 (0.237)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,542 Test: [  11/39]  Time: 0.293 (0.241)  Loss:   0.184 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,765 Test: [  12/39]  Time: 0.223 (0.240)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,831 Test: [  13/39]  Time: 0.066 (0.228)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:21,894 Test: [  14/39]  Time: 0.064 (0.217)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:22,191 Test: [  15/39]  Time: 0.297 (0.222)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:22,421 Test: [  16/39]  Time: 0.229 (0.222)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:22,484 Test: [  17/39]  Time: 0.064 (0.213)  Loss:   0.183 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:22,548 Test: [  18/39]  Time: 0.064 (0.205)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:22,807 Test: [  19/39]  Time: 0.259 (0.208)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:23,051 Test: [  20/39]  Time: 0.244 (0.210)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:23,115 Test: [  21/39]  Time: 0.064 (0.203)  Loss:   0.182 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:23,179 Test: [  22/39]  Time: 0.064 (0.197)  Loss:   1.359 ( 0.236)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:38:23,388 Test: [  23/39]  Time: 0.210 (0.198)  Loss:   1.769 ( 0.300)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:38:23,674 Test: [  24/39]  Time: 0.286 (0.201)  Loss:   1.767 ( 0.359)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:38:23,738 Test: [  25/39]  Time: 0.064 (0.196)  Loss:   1.777 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:38:23,802 Test: [  26/39]  Time: 0.064 (0.191)  Loss:   1.769 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:38:24,011 Test: [  27/39]  Time: 0.209 (0.192)  Loss:   1.776 ( 0.510)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:38:24,253 Test: [  28/39]  Time: 0.242 (0.193)  Loss:   1.781 ( 0.554)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:38:24,320 Test: [  29/39]  Time: 0.067 (0.189)  Loss:   1.782 ( 0.595)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:38:24,383 Test: [  30/39]  Time: 0.064 (0.185)  Loss:   1.776 ( 0.633)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:38:24,630 Test: [  31/39]  Time: 0.246 (0.187)  Loss:   1.784 ( 0.669)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:38:24,859 Test: [  32/39]  Time: 0.229 (0.188)  Loss:   1.766 ( 0.702)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:38:24,923 Test: [  33/39]  Time: 0.064 (0.185)  Loss:   1.776 ( 0.734)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:38:24,986 Test: [  34/39]  Time: 0.064 (0.181)  Loss:   1.769 ( 0.764)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:38:25,053 Test: [  35/39]  Time: 0.067 (0.178)  Loss:   1.781 ( 0.792)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:38:25,369 Test: [  36/39]  Time: 0.316 (0.182)  Loss:   1.732 ( 0.817)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:38:26,048 Test: [  37/39]  Time: 0.680 (0.195)  Loss:   1.508 ( 0.835)  Acc@1:   6.250 ( 58.717)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.062 (  0.002)soec:   0.000 (  0.605)f1:   0.118 (  0.003)
2023-11-30 09:38:26,109 Test: [  38/39]  Time: 0.061 (0.191)  Loss:   1.567 ( 0.854)  Acc@1:   3.125 ( 57.292)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.005)
2023-11-30 09:38:26,307 Test: [  39/39]  Time: 0.198 (0.192)  Loss:   1.331 ( 0.855)  Acc@1:   0.000 ( 57.200)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.005)
2023-11-30 09:38:26,515 Current checkpoints:
 ('./output/train/20231130-093805-efficientnet_b0-259/checkpoint-0.pth.tar', 57.2)

2023-11-30 09:38:26,515 *** Best metric: 57.2 (epoch 0)

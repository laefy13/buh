2023-11-30 09:57:18,256 Training with a single process on 1 device (cuda:0).
2023-11-30 09:57:18,471 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:57:18,471 Data processing configuration for current model + dataset:
2023-11-30 09:57:18,471 	input_size: (3, 259, 259)
2023-11-30 09:57:18,471 	interpolation: bicubic
2023-11-30 09:57:18,471 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:57:18,471 	std: (0.229, 0.224, 0.225)
2023-11-30 09:57:18,471 	crop_pct: 0.875
2023-11-30 09:57:18,471 	crop_mode: center
2023-11-30 09:57:20,869 AMP not enabled. Training in float32.
2023-11-30 09:57:20,911 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:57:22,925 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:57:24,091 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.177s,   10.07/s  (3.177s,   10.07/s)  LR: 1.000e-05  Data: 0.863 (0.863)
2023-11-30 09:57:24,355 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.25/s  (1.720s,   18.60/s)  LR: 1.000e-05  Data: 0.006 (0.435)
2023-11-30 09:57:24,616 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.261s,  122.54/s  (1.234s,   25.93/s)  LR: 1.000e-05  Data: 0.003 (0.291)
2023-11-30 09:57:24,877 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.26/s  (0.991s,   32.29/s)  LR: 1.000e-05  Data: 0.005 (0.219)
2023-11-30 09:57:25,145 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.268s,  119.57/s  (0.846s,   37.81/s)  LR: 1.000e-05  Data: 0.010 (0.177)
2023-11-30 09:57:25,405 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.27/s  (0.748s,   42.75/s)  LR: 1.000e-05  Data: 0.003 (0.148)
2023-11-30 09:57:25,664 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.259s,  123.34/s  (0.679s,   47.16/s)  LR: 1.000e-05  Data: 0.003 (0.127)
2023-11-30 09:57:25,925 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.65/s  (0.626s,   51.09/s)  LR: 1.000e-05  Data: 0.004 (0.112)
2023-11-30 09:57:26,186 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.261s,  122.40/s  (0.586s,   54.62/s)  LR: 1.000e-05  Data: 0.004 (0.100)
2023-11-30 09:57:26,448 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.261s,  122.46/s  (0.553s,   57.83/s)  LR: 1.000e-05  Data: 0.005 (0.091)
2023-11-30 09:57:26,709 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.45/s  (0.527s,   60.74/s)  LR: 1.000e-05  Data: 0.004 (0.083)
2023-11-30 09:57:26,970 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.46/s  (0.505s,   63.40/s)  LR: 1.000e-05  Data: 0.005 (0.076)
2023-11-30 09:57:27,232 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.47/s  (0.486s,   65.85/s)  LR: 1.000e-05  Data: 0.004 (0.071)
2023-11-30 09:57:27,493 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.38/s  (0.470s,   68.09/s)  LR: 1.000e-05  Data: 0.004 (0.066)
2023-11-30 09:57:27,755 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.35/s  (0.456s,   70.17/s)  LR: 1.000e-05  Data: 0.004 (0.062)
2023-11-30 09:57:28,016 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.261s,  122.52/s  (0.444s,   72.09/s)  LR: 1.000e-05  Data: 0.004 (0.058)
2023-11-30 09:57:28,277 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.50/s  (0.433s,   73.88/s)  LR: 1.000e-05  Data: 0.005 (0.055)
2023-11-30 09:57:28,539 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.35/s  (0.424s,   75.54/s)  LR: 1.000e-05  Data: 0.004 (0.052)
2023-11-30 09:57:28,800 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.262s,  122.36/s  (0.415s,   77.10/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 09:57:29,062 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.42/s  (0.407s,   78.55/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 09:57:29,323 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.262s,  122.27/s  (0.400s,   79.91/s)  LR: 1.000e-05  Data: 0.005 (0.045)
2023-11-30 09:57:29,585 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.31/s  (0.394s,   81.19/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 09:57:29,847 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.26/s  (0.388s,   82.39/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 09:57:30,108 Train: 0 [  23/39 ( 61%)]  Loss: 0.940 (1.27)  Time: 0.261s,  122.60/s  (0.383s,   83.54/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 09:57:30,370 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.09/s  (0.378s,   84.60/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 09:57:30,632 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.22/s  (0.374s,   85.62/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 09:57:30,893 Train: 0 [  26/39 ( 68%)]  Loss: 0.997 (1.25)  Time: 0.262s,  122.35/s  (0.370s,   86.58/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 09:57:31,157 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.264s,  121.14/s  (0.366s,   87.47/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:57:31,419 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.261s,  122.39/s  (0.362s,   88.34/s)  LR: 1.000e-05  Data: 0.005 (0.034)
2023-11-30 09:57:31,680 Train: 0 [  29/39 ( 76%)]  Loss: 0.988 (1.25)  Time: 0.261s,  122.53/s  (0.359s,   89.17/s)  LR: 1.000e-05  Data: 0.005 (0.033)
2023-11-30 09:57:31,940 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.261s,  122.78/s  (0.356s,   89.96/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:57:32,201 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.260s,  123.02/s  (0.353s,   90.73/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:57:32,461 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.260s,  123.06/s  (0.350s,   91.45/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:57:32,723 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.262s,  122.15/s  (0.347s,   92.14/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:57:32,985 Train: 0 [  34/39 ( 89%)]  Loss: 0.976 (1.25)  Time: 0.263s,  121.73/s  (0.345s,   92.78/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 09:57:33,246 Train: 0 [  35/39 ( 92%)]  Loss: 1.11 (1.24)  Time: 0.261s,  122.78/s  (0.343s,   93.41/s)  LR: 1.000e-05  Data: 0.004 (0.028)
2023-11-30 09:57:33,507 Train: 0 [  36/39 ( 95%)]  Loss: 1.23 (1.24)  Time: 0.261s,  122.51/s  (0.340s,   94.02/s)  LR: 1.000e-05  Data: 0.004 (0.028)
2023-11-30 09:57:33,768 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.62/s  (0.338s,   94.60/s)  LR: 1.000e-05  Data: 0.004 (0.027)
2023-11-30 09:57:34,024 Train: 0 [  38/39 (100%)]  Loss: 2.59 (1.27)  Time: 0.256s,  125.16/s  (0.336s,   95.19/s)  LR: 1.000e-05  Data: 0.000 (0.026)
2023-11-30 09:57:35,084 Test: [   0/39]  Time: 1.056 (1.056)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:35,148 Test: [   1/39]  Time: 0.064 (0.560)  Loss:   0.180 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:35,212 Test: [   2/39]  Time: 0.064 (0.395)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:35,571 Test: [   3/39]  Time: 0.360 (0.386)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:35,719 Test: [   4/39]  Time: 0.147 (0.338)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:35,782 Test: [   5/39]  Time: 0.064 (0.292)  Loss:   0.187 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:35,846 Test: [   6/39]  Time: 0.064 (0.260)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:36,153 Test: [   7/39]  Time: 0.307 (0.266)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:36,363 Test: [   8/39]  Time: 0.210 (0.260)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:36,427 Test: [   9/39]  Time: 0.063 (0.240)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:36,490 Test: [  10/39]  Time: 0.064 (0.224)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:36,788 Test: [  11/39]  Time: 0.297 (0.230)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,008 Test: [  12/39]  Time: 0.220 (0.229)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,071 Test: [  13/39]  Time: 0.064 (0.217)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,135 Test: [  14/39]  Time: 0.064 (0.207)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,451 Test: [  15/39]  Time: 0.316 (0.214)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,662 Test: [  16/39]  Time: 0.212 (0.214)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,726 Test: [  17/39]  Time: 0.064 (0.205)  Loss:   0.181 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:37,789 Test: [  18/39]  Time: 0.064 (0.198)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:38,062 Test: [  19/39]  Time: 0.273 (0.202)  Loss:   0.181 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:38,291 Test: [  20/39]  Time: 0.229 (0.203)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:38,355 Test: [  21/39]  Time: 0.064 (0.197)  Loss:   0.180 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:38,418 Test: [  22/39]  Time: 0.063 (0.191)  Loss:   1.367 ( 0.234)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:57:38,641 Test: [  23/39]  Time: 0.223 (0.192)  Loss:   1.780 ( 0.299)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:57:38,914 Test: [  24/39]  Time: 0.274 (0.195)  Loss:   1.778 ( 0.358)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:57:38,978 Test: [  25/39]  Time: 0.064 (0.190)  Loss:   1.788 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:57:39,041 Test: [  26/39]  Time: 0.064 (0.186)  Loss:   1.781 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:57:39,259 Test: [  27/39]  Time: 0.218 (0.187)  Loss:   1.787 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:57:39,495 Test: [  28/39]  Time: 0.236 (0.189)  Loss:   1.793 ( 0.555)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:57:39,558 Test: [  29/39]  Time: 0.064 (0.184)  Loss:   1.793 ( 0.596)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:57:39,622 Test: [  30/39]  Time: 0.064 (0.180)  Loss:   1.788 ( 0.635)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:57:39,872 Test: [  31/39]  Time: 0.250 (0.183)  Loss:   1.795 ( 0.671)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:57:40,099 Test: [  32/39]  Time: 0.227 (0.184)  Loss:   1.778 ( 0.705)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:57:40,163 Test: [  33/39]  Time: 0.064 (0.180)  Loss:   1.787 ( 0.736)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:57:40,226 Test: [  34/39]  Time: 0.063 (0.177)  Loss:   1.780 ( 0.766)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:57:40,293 Test: [  35/39]  Time: 0.067 (0.174)  Loss:   1.793 ( 0.795)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:57:40,599 Test: [  36/39]  Time: 0.307 (0.178)  Loss:   1.745 ( 0.820)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:57:41,247 Test: [  37/39]  Time: 0.648 (0.190)  Loss:   1.522 ( 0.839)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 09:57:41,308 Test: [  38/39]  Time: 0.061 (0.187)  Loss:   1.581 ( 0.858)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 09:57:41,501 Test: [  39/39]  Time: 0.193 (0.187)  Loss:   1.342 ( 0.859)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 09:57:41,705 Current checkpoints:
 ('./output/train/20231130-095720-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 09:57:41,706 *** Best metric: 57.12 (epoch 0)

2023-11-30 10:01:27,577 Training with a single process on 1 device (cuda:0).
2023-11-30 10:01:27,792 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:01:27,792 Data processing configuration for current model + dataset:
2023-11-30 10:01:27,792 	input_size: (3, 259, 259)
2023-11-30 10:01:27,792 	interpolation: bicubic
2023-11-30 10:01:27,792 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:01:27,793 	std: (0.229, 0.224, 0.225)
2023-11-30 10:01:27,793 	crop_pct: 0.875
2023-11-30 10:01:27,793 	crop_mode: center
2023-11-30 10:01:30,212 AMP not enabled. Training in float32.
2023-11-30 10:01:30,255 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:01:32,421 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:01:33,536 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.279s,    9.76/s  (3.279s,    9.76/s)  LR: 1.000e-05  Data: 1.020 (1.020)
2023-11-30 10:01:33,797 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.261s,  122.52/s  (1.770s,   18.08/s)  LR: 1.000e-05  Data: 0.003 (0.512)
2023-11-30 10:01:34,057 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.04/s  (1.267s,   25.26/s)  LR: 1.000e-05  Data: 0.003 (0.342)
2023-11-30 10:01:34,322 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.265s,  120.75/s  (1.016s,   31.49/s)  LR: 1.000e-05  Data: 0.007 (0.258)
2023-11-30 10:01:34,582 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.260s,  122.97/s  (0.865s,   36.99/s)  LR: 1.000e-05  Data: 0.003 (0.207)
2023-11-30 10:01:34,844 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.261s,  122.46/s  (0.764s,   41.86/s)  LR: 1.000e-05  Data: 0.003 (0.173)
2023-11-30 10:01:35,105 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.39/s  (0.693s,   46.21/s)  LR: 1.000e-05  Data: 0.004 (0.149)
2023-11-30 10:01:35,368 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.263s,  121.89/s  (0.639s,   50.09/s)  LR: 1.000e-05  Data: 0.006 (0.131)
2023-11-30 10:01:35,628 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.260s,  122.98/s  (0.597s,   53.63/s)  LR: 1.000e-05  Data: 0.003 (0.117)
2023-11-30 10:01:35,891 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.263s,  121.61/s  (0.563s,   56.80/s)  LR: 1.000e-05  Data: 0.004 (0.106)
2023-11-30 10:01:36,153 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.28/s  (0.536s,   59.71/s)  LR: 1.000e-05  Data: 0.004 (0.096)
2023-11-30 10:01:36,414 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.262s,  122.27/s  (0.513s,   62.37/s)  LR: 1.000e-05  Data: 0.004 (0.089)
2023-11-30 10:01:36,677 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.06/s  (0.494s,   64.80/s)  LR: 1.000e-05  Data: 0.005 (0.082)
2023-11-30 10:01:36,939 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.04/s  (0.477s,   67.05/s)  LR: 1.000e-05  Data: 0.005 (0.077)
2023-11-30 10:01:37,200 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.28/s  (0.463s,   69.13/s)  LR: 1.000e-05  Data: 0.004 (0.072)
2023-11-30 10:01:37,463 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.263s,  121.78/s  (0.450s,   71.05/s)  LR: 1.000e-05  Data: 0.005 (0.068)
2023-11-30 10:01:37,726 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.263s,  121.74/s  (0.439s,   72.84/s)  LR: 1.000e-05  Data: 0.004 (0.064)
2023-11-30 10:01:37,989 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.263s,  121.52/s  (0.430s,   74.49/s)  LR: 1.000e-05  Data: 0.005 (0.061)
2023-11-30 10:01:38,252 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.263s,  121.67/s  (0.421s,   76.05/s)  LR: 1.000e-05  Data: 0.004 (0.058)
2023-11-30 10:01:38,515 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.263s,  121.75/s  (0.413s,   77.50/s)  LR: 1.000e-05  Data: 0.005 (0.055)
2023-11-30 10:01:38,777 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.262s,  122.09/s  (0.406s,   78.87/s)  LR: 1.000e-05  Data: 0.004 (0.053)
2023-11-30 10:01:39,039 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.12/s  (0.399s,   80.16/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 10:01:39,301 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.24/s  (0.393s,   81.38/s)  LR: 1.000e-05  Data: 0.004 (0.048)
2023-11-30 10:01:39,563 Train: 0 [  23/39 ( 61%)]  Loss: 0.940 (1.27)  Time: 0.262s,  122.22/s  (0.388s,   82.53/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 10:01:39,825 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.26/s  (0.383s,   83.62/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 10:01:40,087 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.08/s  (0.378s,   84.64/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 10:01:40,348 Train: 0 [  26/39 ( 68%)]  Loss: 0.997 (1.25)  Time: 0.262s,  122.34/s  (0.374s,   85.62/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 10:01:40,610 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.262s,  122.22/s  (0.370s,   86.54/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 10:01:40,872 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.262s,  122.14/s  (0.366s,   87.42/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 10:01:41,134 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  122.02/s  (0.363s,   88.26/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 10:01:41,396 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.261s,  122.47/s  (0.359s,   89.06/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 10:01:41,656 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.75/s  (0.356s,   89.83/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 10:01:41,917 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.260s,  122.93/s  (0.353s,   90.57/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 10:01:42,177 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.260s,  122.93/s  (0.351s,   91.28/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 10:01:42,437 Train: 0 [  34/39 ( 89%)]  Loss: 0.978 (1.25)  Time: 0.260s,  123.17/s  (0.348s,   91.96/s)  LR: 1.000e-05  Data: 0.003 (0.033)
2023-11-30 10:01:42,697 Train: 0 [  35/39 ( 92%)]  Loss: 1.12 (1.24)  Time: 0.260s,  122.88/s  (0.346s,   92.60/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:01:42,957 Train: 0 [  36/39 ( 95%)]  Loss: 1.25 (1.24)  Time: 0.260s,  123.00/s  (0.343s,   93.23/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:01:43,219 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.57/s  (0.341s,   93.82/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:01:43,476 Train: 0 [  38/39 (100%)]  Loss: 2.57 (1.27)  Time: 0.258s,  124.09/s  (0.339s,   94.41/s)  LR: 1.000e-05  Data: 0.000 (0.030)
2023-11-30 10:01:44,656 Test: [   0/39]  Time: 1.176 (1.176)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:44,720 Test: [   1/39]  Time: 0.064 (0.620)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:44,784 Test: [   2/39]  Time: 0.064 (0.435)  Loss:   0.185 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,177 Test: [   3/39]  Time: 0.393 (0.424)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,287 Test: [   4/39]  Time: 0.110 (0.361)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,351 Test: [   5/39]  Time: 0.064 (0.312)  Loss:   0.190 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,415 Test: [   6/39]  Time: 0.064 (0.276)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,767 Test: [   7/39]  Time: 0.352 (0.286)  Loss:   0.184 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,936 Test: [   8/39]  Time: 0.169 (0.273)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:45,999 Test: [   9/39]  Time: 0.064 (0.252)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:46,063 Test: [  10/39]  Time: 0.064 (0.235)  Loss:   0.186 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:46,399 Test: [  11/39]  Time: 0.337 (0.243)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:46,581 Test: [  12/39]  Time: 0.182 (0.239)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:46,645 Test: [  13/39]  Time: 0.064 (0.226)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:46,709 Test: [  14/39]  Time: 0.064 (0.215)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,077 Test: [  15/39]  Time: 0.368 (0.225)  Loss:   0.186 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,247 Test: [  16/39]  Time: 0.170 (0.222)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,311 Test: [  17/39]  Time: 0.064 (0.213)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,375 Test: [  18/39]  Time: 0.064 (0.205)  Loss:   0.185 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,715 Test: [  19/39]  Time: 0.340 (0.212)  Loss:   0.183 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,905 Test: [  20/39]  Time: 0.190 (0.211)  Loss:   0.184 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:47,969 Test: [  21/39]  Time: 0.064 (0.204)  Loss:   0.182 ( 0.185)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:48,032 Test: [  22/39]  Time: 0.064 (0.198)  Loss:   1.360 ( 0.236)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:01:48,314 Test: [  23/39]  Time: 0.282 (0.201)  Loss:   1.770 ( 0.300)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:01:48,535 Test: [  24/39]  Time: 0.221 (0.202)  Loss:   1.768 ( 0.359)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:01:48,599 Test: [  25/39]  Time: 0.064 (0.197)  Loss:   1.778 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:01:48,662 Test: [  26/39]  Time: 0.064 (0.192)  Loss:   1.770 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:01:48,937 Test: [  27/39]  Time: 0.275 (0.195)  Loss:   1.777 ( 0.510)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:01:49,112 Test: [  28/39]  Time: 0.175 (0.194)  Loss:   1.783 ( 0.554)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:01:49,175 Test: [  29/39]  Time: 0.064 (0.190)  Loss:   1.783 ( 0.595)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:01:49,239 Test: [  30/39]  Time: 0.064 (0.186)  Loss:   1.778 ( 0.633)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:01:49,555 Test: [  31/39]  Time: 0.315 (0.190)  Loss:   1.785 ( 0.669)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:01:49,722 Test: [  32/39]  Time: 0.168 (0.189)  Loss:   1.768 ( 0.703)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:01:49,786 Test: [  33/39]  Time: 0.064 (0.185)  Loss:   1.777 ( 0.734)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:01:49,849 Test: [  34/39]  Time: 0.064 (0.182)  Loss:   1.770 ( 0.764)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:01:49,918 Test: [  35/39]  Time: 0.069 (0.179)  Loss:   1.783 ( 0.792)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:01:50,227 Test: [  36/39]  Time: 0.309 (0.182)  Loss:   1.735 ( 0.818)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:01:50,784 Test: [  37/39]  Time: 0.557 (0.192)  Loss:   1.512 ( 0.836)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 10:01:50,845 Test: [  38/39]  Time: 0.061 (0.189)  Loss:   1.570 ( 0.855)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 10:01:51,048 Test: [  39/39]  Time: 0.204 (0.189)  Loss:   1.334 ( 0.856)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 10:01:51,252 Current checkpoints:
 ('./output/train/20231130-100130-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 10:01:51,252 *** Best metric: 57.12 (epoch 0)

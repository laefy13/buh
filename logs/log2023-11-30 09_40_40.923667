2023-11-30 09:40:40,977 Training with a single process on 1 device (cuda:0).
2023-11-30 09:40:41,195 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:40:41,195 Data processing configuration for current model + dataset:
2023-11-30 09:40:41,195 	input_size: (3, 259, 259)
2023-11-30 09:40:41,195 	interpolation: bicubic
2023-11-30 09:40:41,195 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:40:41,195 	std: (0.229, 0.224, 0.225)
2023-11-30 09:40:41,195 	crop_pct: 0.875
2023-11-30 09:40:41,195 	crop_mode: center
2023-11-30 09:40:43,663 AMP not enabled. Training in float32.
2023-11-30 09:40:43,705 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:40:45,818 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:40:46,951 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.244s,    9.87/s  (3.244s,    9.87/s)  LR: 1.000e-05  Data: 0.950 (0.950)
2023-11-30 09:40:47,215 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.26/s  (1.754s,   18.25/s)  LR: 1.000e-05  Data: 0.007 (0.479)
2023-11-30 09:40:47,475 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.16/s  (1.256s,   25.48/s)  LR: 1.000e-05  Data: 0.003 (0.320)
2023-11-30 09:40:47,736 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.58/s  (1.007s,   31.77/s)  LR: 1.000e-05  Data: 0.004 (0.241)
2023-11-30 09:40:47,999 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.263s,  121.76/s  (0.858s,   37.28/s)  LR: 1.000e-05  Data: 0.006 (0.194)
2023-11-30 09:40:48,259 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.21/s  (0.759s,   42.19/s)  LR: 1.000e-05  Data: 0.003 (0.162)
2023-11-30 09:40:48,520 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.31/s  (0.688s,   46.54/s)  LR: 1.000e-05  Data: 0.004 (0.140)
2023-11-30 09:40:48,780 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.260s,  123.00/s  (0.634s,   50.46/s)  LR: 1.000e-05  Data: 0.004 (0.123)
2023-11-30 09:40:49,042 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.33/s  (0.593s,   53.99/s)  LR: 1.000e-05  Data: 0.004 (0.109)
2023-11-30 09:40:49,303 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.261s,  122.63/s  (0.560s,   57.19/s)  LR: 1.000e-05  Data: 0.004 (0.099)
2023-11-30 09:40:49,564 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.49/s  (0.532s,   60.10/s)  LR: 1.000e-05  Data: 0.004 (0.090)
2023-11-30 09:40:49,825 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.82/s  (0.510s,   62.77/s)  LR: 1.000e-05  Data: 0.004 (0.083)
2023-11-30 09:40:50,086 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.36/s  (0.491s,   65.22/s)  LR: 1.000e-05  Data: 0.004 (0.077)
2023-11-30 09:40:50,348 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.42/s  (0.474s,   67.47/s)  LR: 1.000e-05  Data: 0.004 (0.072)
2023-11-30 09:40:50,609 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.32/s  (0.460s,   69.55/s)  LR: 1.000e-05  Data: 0.005 (0.067)
2023-11-30 09:40:50,871 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.261s,  122.47/s  (0.448s,   71.48/s)  LR: 1.000e-05  Data: 0.004 (0.063)
2023-11-30 09:40:51,132 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.42/s  (0.437s,   73.27/s)  LR: 1.000e-05  Data: 0.004 (0.060)
2023-11-30 09:40:51,393 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.72/s  (0.427s,   74.95/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 09:40:51,654 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.261s,  122.45/s  (0.418s,   76.51/s)  LR: 1.000e-05  Data: 0.005 (0.054)
2023-11-30 09:40:51,915 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.46/s  (0.410s,   77.97/s)  LR: 1.000e-05  Data: 0.004 (0.052)
2023-11-30 09:40:52,177 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.261s,  122.49/s  (0.403s,   79.35/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 09:40:52,439 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.19/s  (0.397s,   80.63/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 09:40:52,700 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.261s,  122.48/s  (0.391s,   81.85/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 09:40:52,961 Train: 0 [  23/39 ( 61%)]  Loss: 0.942 (1.27)  Time: 0.261s,  122.67/s  (0.386s,   83.00/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 09:40:53,222 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.262s,  122.31/s  (0.381s,   84.08/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 09:40:53,483 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.261s,  122.74/s  (0.376s,   85.11/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 09:40:53,744 Train: 0 [  26/39 ( 68%)]  Loss: 0.999 (1.25)  Time: 0.261s,  122.42/s  (0.372s,   86.08/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:40:54,006 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.261s,  122.56/s  (0.368s,   87.01/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 09:40:54,267 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.28/s  (0.364s,   87.88/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:40:54,528 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.261s,  122.72/s  (0.361s,   88.72/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:40:54,789 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.261s,  122.76/s  (0.357s,   89.52/s)  LR: 1.000e-05  Data: 0.005 (0.035)
2023-11-30 09:40:55,048 Train: 0 [  31/39 ( 82%)]  Loss: 1.66 (1.26)  Time: 0.260s,  123.27/s  (0.354s,   90.29/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:40:55,308 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.260s,  123.15/s  (0.352s,   91.03/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:40:55,567 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.259s,  123.38/s  (0.349s,   91.74/s)  LR: 1.000e-05  Data: 0.003 (0.032)
2023-11-30 09:40:55,827 Train: 0 [  34/39 ( 89%)]  Loss: 0.978 (1.25)  Time: 0.260s,  123.21/s  (0.346s,   92.41/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:40:56,087 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.259s,  123.34/s  (0.344s,   93.06/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:40:56,346 Train: 0 [  36/39 ( 95%)]  Loss: 1.26 (1.24)  Time: 0.260s,  123.23/s  (0.342s,   93.68/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:40:56,607 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.77/s  (0.339s,   94.27/s)  LR: 1.000e-05  Data: 0.003 (0.029)
2023-11-30 09:40:56,863 Train: 0 [  38/39 (100%)]  Loss: 2.61 (1.27)  Time: 0.256s,  124.83/s  (0.337s,   94.86/s)  LR: 1.000e-05  Data: 0.000 (0.028)
2023-11-30 09:40:57,948 Test: [   0/39]  Time: 1.081 (1.081)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:58,013 Test: [   1/39]  Time: 0.064 (0.573)  Loss:   0.181 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:58,076 Test: [   2/39]  Time: 0.064 (0.403)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:58,452 Test: [   3/39]  Time: 0.375 (0.396)  Loss:   0.184 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:58,569 Test: [   4/39]  Time: 0.117 (0.340)  Loss:   0.185 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:58,633 Test: [   5/39]  Time: 0.064 (0.294)  Loss:   0.188 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:58,697 Test: [   6/39]  Time: 0.064 (0.261)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,037 Test: [   7/39]  Time: 0.341 (0.271)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,222 Test: [   8/39]  Time: 0.185 (0.262)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,286 Test: [   9/39]  Time: 0.064 (0.242)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,350 Test: [  10/39]  Time: 0.064 (0.226)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,679 Test: [  11/39]  Time: 0.329 (0.234)  Loss:   0.182 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,876 Test: [  12/39]  Time: 0.196 (0.231)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:40:59,939 Test: [  13/39]  Time: 0.064 (0.219)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:00,003 Test: [  14/39]  Time: 0.064 (0.209)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:00,346 Test: [  15/39]  Time: 0.343 (0.217)  Loss:   0.184 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:00,560 Test: [  16/39]  Time: 0.214 (0.217)  Loss:   0.183 ( 0.184)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:00,624 Test: [  17/39]  Time: 0.064 (0.209)  Loss:   0.181 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:00,687 Test: [  18/39]  Time: 0.064 (0.201)  Loss:   0.183 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:00,968 Test: [  19/39]  Time: 0.280 (0.205)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:01,197 Test: [  20/39]  Time: 0.229 (0.206)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:01,260 Test: [  21/39]  Time: 0.064 (0.200)  Loss:   0.180 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:01,324 Test: [  22/39]  Time: 0.064 (0.194)  Loss:   1.366 ( 0.235)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:41:01,550 Test: [  23/39]  Time: 0.226 (0.195)  Loss:   1.779 ( 0.299)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:41:01,830 Test: [  24/39]  Time: 0.280 (0.199)  Loss:   1.777 ( 0.358)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:41:01,894 Test: [  25/39]  Time: 0.064 (0.193)  Loss:   1.788 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:41:01,957 Test: [  26/39]  Time: 0.064 (0.189)  Loss:   1.780 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:41:02,178 Test: [  27/39]  Time: 0.221 (0.190)  Loss:   1.786 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:41:02,432 Test: [  28/39]  Time: 0.254 (0.192)  Loss:   1.792 ( 0.555)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:41:02,496 Test: [  29/39]  Time: 0.064 (0.188)  Loss:   1.792 ( 0.596)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:41:02,560 Test: [  30/39]  Time: 0.064 (0.184)  Loss:   1.787 ( 0.635)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:41:02,890 Test: [  31/39]  Time: 0.330 (0.188)  Loss:   1.795 ( 0.671)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:41:03,134 Test: [  32/39]  Time: 0.244 (0.190)  Loss:   1.777 ( 0.704)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:41:03,197 Test: [  33/39]  Time: 0.064 (0.186)  Loss:   1.786 ( 0.736)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:41:03,261 Test: [  34/39]  Time: 0.064 (0.183)  Loss:   1.779 ( 0.766)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:41:03,333 Test: [  35/39]  Time: 0.072 (0.180)  Loss:   1.792 ( 0.795)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:41:03,714 Test: [  36/39]  Time: 0.381 (0.185)  Loss:   1.742 ( 0.820)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:41:04,347 Test: [  37/39]  Time: 0.633 (0.197)  Loss:   1.514 ( 0.838)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 09:41:04,408 Test: [  38/39]  Time: 0.061 (0.193)  Loss:   1.573 ( 0.857)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 09:41:04,632 Test: [  39/39]  Time: 0.223 (0.194)  Loss:   1.337 ( 0.858)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 09:41:04,893 Current checkpoints:
 ('./output/train/20231130-094043-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 09:41:04,893 *** Best metric: 57.12 (epoch 0)

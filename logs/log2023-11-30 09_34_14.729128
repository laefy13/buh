2023-11-30 09:34:14,780 Training with a single process on 1 device (cuda:0).
2023-11-30 09:34:14,993 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:34:14,993 Data processing configuration for current model + dataset:
2023-11-30 09:34:14,993 	input_size: (3, 259, 259)
2023-11-30 09:34:14,993 	interpolation: bicubic
2023-11-30 09:34:14,993 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:34:14,993 	std: (0.229, 0.224, 0.225)
2023-11-30 09:34:14,993 	crop_pct: 0.875
2023-11-30 09:34:14,994 	crop_mode: center
2023-11-30 09:34:17,532 AMP not enabled. Training in float32.
2023-11-30 09:34:17,584 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:34:19,632 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:34:20,819 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.231s,    9.90/s  (3.231s,    9.90/s)  LR: 1.000e-05  Data: 0.895 (0.895)
2023-11-30 09:34:21,083 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.00/s  (1.748s,   18.31/s)  LR: 1.000e-05  Data: 0.008 (0.452)
2023-11-30 09:34:21,343 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.23/s  (1.252s,   25.56/s)  LR: 1.000e-05  Data: 0.003 (0.302)
2023-11-30 09:34:21,604 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.57/s  (1.004s,   31.87/s)  LR: 1.000e-05  Data: 0.004 (0.227)
2023-11-30 09:34:21,869 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.265s,  120.75/s  (0.856s,   37.37/s)  LR: 1.000e-05  Data: 0.008 (0.183)
2023-11-30 09:34:22,128 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.259s,  123.41/s  (0.757s,   42.28/s)  LR: 1.000e-05  Data: 0.003 (0.153)
2023-11-30 09:34:22,388 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.259s,  123.34/s  (0.686s,   46.66/s)  LR: 1.000e-05  Data: 0.003 (0.132)
2023-11-30 09:34:22,648 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.260s,  122.84/s  (0.633s,   50.59/s)  LR: 1.000e-05  Data: 0.004 (0.116)
2023-11-30 09:34:22,910 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.261s,  122.49/s  (0.591s,   54.11/s)  LR: 1.000e-05  Data: 0.004 (0.103)
2023-11-30 09:34:23,171 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.37/s  (0.558s,   57.31/s)  LR: 1.000e-05  Data: 0.005 (0.094)
2023-11-30 09:34:23,432 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.50/s  (0.531s,   60.22/s)  LR: 1.000e-05  Data: 0.004 (0.085)
2023-11-30 09:34:23,696 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.264s,  121.41/s  (0.509s,   62.86/s)  LR: 1.000e-05  Data: 0.004 (0.079)
2023-11-30 09:34:23,958 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.25/s  (0.490s,   65.31/s)  LR: 1.000e-05  Data: 0.005 (0.073)
2023-11-30 09:34:24,220 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.21/s  (0.474s,   67.55/s)  LR: 1.000e-05  Data: 0.004 (0.068)
2023-11-30 09:34:24,482 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.263s,  121.88/s  (0.460s,   69.62/s)  LR: 1.000e-05  Data: 0.005 (0.064)
2023-11-30 09:34:24,743 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.261s,  122.62/s  (0.447s,   71.55/s)  LR: 1.000e-05  Data: 0.004 (0.060)
2023-11-30 09:34:25,004 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.57/s  (0.436s,   73.35/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 09:34:25,265 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.54/s  (0.427s,   75.02/s)  LR: 1.000e-05  Data: 0.005 (0.054)
2023-11-30 09:34:25,526 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.260s,  122.85/s  (0.418s,   76.59/s)  LR: 1.000e-05  Data: 0.004 (0.051)
2023-11-30 09:34:25,786 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.78/s  (0.410s,   78.06/s)  LR: 1.000e-05  Data: 0.004 (0.049)
2023-11-30 09:34:26,047 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.261s,  122.58/s  (0.403s,   79.43/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 09:34:26,309 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.261s,  122.56/s  (0.396s,   80.73/s)  LR: 1.000e-05  Data: 0.005 (0.045)
2023-11-30 09:34:26,570 Train: 0 [  22/39 ( 58%)]  Loss: 1.85 (1.28)  Time: 0.261s,  122.56/s  (0.391s,   81.94/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:34:26,830 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.261s,  122.77/s  (0.385s,   83.09/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 09:34:27,091 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.261s,  122.51/s  (0.380s,   84.18/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:34:27,352 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.261s,  122.62/s  (0.376s,   85.20/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:34:27,614 Train: 0 [  26/39 ( 68%)]  Loss: 0.997 (1.25)  Time: 0.261s,  122.53/s  (0.371s,   86.18/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:34:27,874 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.261s,  122.74/s  (0.367s,   87.10/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:34:28,136 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.261s,  122.50/s  (0.364s,   87.98/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:34:28,396 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.261s,  122.64/s  (0.360s,   88.82/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:34:28,657 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.260s,  122.94/s  (0.357s,   89.62/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:34:28,916 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.259s,  123.56/s  (0.354s,   90.39/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:34:29,176 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.261s,  122.77/s  (0.351s,   91.12/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:34:29,438 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.262s,  122.28/s  (0.349s,   91.81/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:34:29,699 Train: 0 [  34/39 ( 89%)]  Loss: 0.977 (1.25)  Time: 0.261s,  122.70/s  (0.346s,   92.48/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:34:29,959 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.260s,  123.17/s  (0.344s,   93.12/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 09:34:30,219 Train: 0 [  36/39 ( 95%)]  Loss: 1.26 (1.24)  Time: 0.260s,  122.92/s  (0.341s,   93.73/s)  LR: 1.000e-05  Data: 0.004 (0.028)
2023-11-30 09:34:30,479 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.260s,  123.29/s  (0.339s,   94.33/s)  LR: 1.000e-05  Data: 0.004 (0.028)
2023-11-30 09:34:30,735 Train: 0 [  38/39 (100%)]  Loss: 2.60 (1.27)  Time: 0.256s,  124.87/s  (0.337s,   94.93/s)  LR: 1.000e-05  Data: 0.000 (0.027)
2023-11-30 09:34:31,831 Test: [   0/39]  Time: 1.092 (1.092)  Loss:   0.182 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:31,897 Test: [   1/39]  Time: 0.066 (0.579)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:31,960 Test: [   2/39]  Time: 0.064 (0.407)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:32,388 Test: [   3/39]  Time: 0.428 (0.412)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:32,471 Test: [   4/39]  Time: 0.083 (0.347)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:32,535 Test: [   5/39]  Time: 0.064 (0.299)  Loss:   0.186 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:32,599 Test: [   6/39]  Time: 0.064 (0.266)  Loss:   0.182 ( 0.183)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:32,973 Test: [   7/39]  Time: 0.374 (0.279)  Loss:   0.181 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,121 Test: [   8/39]  Time: 0.148 (0.265)  Loss:   0.181 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,185 Test: [   9/39]  Time: 0.064 (0.245)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,248 Test: [  10/39]  Time: 0.064 (0.228)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,610 Test: [  11/39]  Time: 0.362 (0.239)  Loss:   0.181 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,769 Test: [  12/39]  Time: 0.159 (0.233)  Loss:   0.182 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,834 Test: [  13/39]  Time: 0.065 (0.221)  Loss:   0.182 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:33,897 Test: [  14/39]  Time: 0.064 (0.211)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:34,294 Test: [  15/39]  Time: 0.397 (0.222)  Loss:   0.183 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:34,429 Test: [  16/39]  Time: 0.135 (0.217)  Loss:   0.182 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:34,493 Test: [  17/39]  Time: 0.064 (0.209)  Loss:   0.180 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:34,556 Test: [  18/39]  Time: 0.064 (0.201)  Loss:   0.182 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:34,920 Test: [  19/39]  Time: 0.364 (0.209)  Loss:   0.180 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:35,058 Test: [  20/39]  Time: 0.138 (0.206)  Loss:   0.181 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:35,122 Test: [  21/39]  Time: 0.064 (0.199)  Loss:   0.179 ( 0.182)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:35,185 Test: [  22/39]  Time: 0.064 (0.193)  Loss:   1.370 ( 0.234)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:34:35,518 Test: [  23/39]  Time: 0.332 (0.199)  Loss:   1.785 ( 0.298)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:34:35,694 Test: [  24/39]  Time: 0.176 (0.198)  Loss:   1.783 ( 0.358)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:34:35,758 Test: [  25/39]  Time: 0.064 (0.193)  Loss:   1.794 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:34:35,821 Test: [  26/39]  Time: 0.064 (0.188)  Loss:   1.786 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:34:36,140 Test: [  27/39]  Time: 0.319 (0.193)  Loss:   1.792 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:34:36,275 Test: [  28/39]  Time: 0.135 (0.191)  Loss:   1.798 ( 0.556)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:34:36,341 Test: [  29/39]  Time: 0.065 (0.187)  Loss:   1.798 ( 0.597)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:34:36,404 Test: [  30/39]  Time: 0.064 (0.183)  Loss:   1.793 ( 0.636)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:34:36,754 Test: [  31/39]  Time: 0.349 (0.188)  Loss:   1.800 ( 0.672)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:34:36,878 Test: [  32/39]  Time: 0.124 (0.186)  Loss:   1.783 ( 0.706)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:34:36,942 Test: [  33/39]  Time: 0.064 (0.182)  Loss:   1.792 ( 0.738)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:34:37,005 Test: [  34/39]  Time: 0.063 (0.179)  Loss:   1.785 ( 0.767)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:34:37,117 Test: [  35/39]  Time: 0.112 (0.177)  Loss:   1.798 ( 0.796)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:34:37,386 Test: [  36/39]  Time: 0.269 (0.180)  Loss:   1.748 ( 0.822)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:34:38,068 Test: [  37/39]  Time: 0.682 (0.193)  Loss:   1.521 ( 0.840)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 09:34:38,129 Test: [  38/39]  Time: 0.061 (0.189)  Loss:   1.580 ( 0.859)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 09:34:38,325 Test: [  39/39]  Time: 0.196 (0.190)  Loss:   1.343 ( 0.860)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 09:34:38,528 Current checkpoints:
 ('./output/train/20231130-093417-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 09:34:38,529 *** Best metric: 57.12 (epoch 0)

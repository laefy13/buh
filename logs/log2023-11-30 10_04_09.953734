2023-11-30 10:04:10,007 Training with a single process on 1 device (cuda:0).
2023-11-30 10:04:10,220 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:04:10,220 Data processing configuration for current model + dataset:
2023-11-30 10:04:10,220 	input_size: (3, 259, 259)
2023-11-30 10:04:10,220 	interpolation: bicubic
2023-11-30 10:04:10,221 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:04:10,221 	std: (0.229, 0.224, 0.225)
2023-11-30 10:04:10,221 	crop_pct: 0.875
2023-11-30 10:04:10,221 	crop_mode: center
2023-11-30 10:04:12,621 AMP not enabled. Training in float32.
2023-11-30 10:04:12,663 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:04:14,791 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:04:15,916 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.251s,    9.84/s  (3.251s,    9.84/s)  LR: 1.000e-05  Data: 0.950 (0.950)
2023-11-30 10:04:16,181 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.265s,  120.68/s  (1.758s,   18.20/s)  LR: 1.000e-05  Data: 0.008 (0.479)
2023-11-30 10:04:16,441 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.259s,  123.42/s  (1.258s,   25.43/s)  LR: 1.000e-05  Data: 0.003 (0.320)
2023-11-30 10:04:16,702 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.62/s  (1.009s,   31.71/s)  LR: 1.000e-05  Data: 0.004 (0.241)
2023-11-30 10:04:16,964 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.263s,  121.81/s  (0.860s,   37.22/s)  LR: 1.000e-05  Data: 0.006 (0.194)
2023-11-30 10:04:17,224 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.259s,  123.35/s  (0.760s,   42.12/s)  LR: 1.000e-05  Data: 0.003 (0.162)
2023-11-30 10:04:17,485 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.59/s  (0.688s,   46.48/s)  LR: 1.000e-05  Data: 0.004 (0.140)
2023-11-30 10:04:17,746 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.55/s  (0.635s,   50.39/s)  LR: 1.000e-05  Data: 0.005 (0.123)
2023-11-30 10:04:18,007 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.261s,  122.52/s  (0.594s,   53.91/s)  LR: 1.000e-05  Data: 0.005 (0.110)
2023-11-30 10:04:18,268 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.261s,  122.53/s  (0.560s,   57.11/s)  LR: 1.000e-05  Data: 0.004 (0.099)
2023-11-30 10:04:18,530 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.45/s  (0.533s,   60.02/s)  LR: 1.000e-05  Data: 0.005 (0.091)
2023-11-30 10:04:18,791 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.54/s  (0.510s,   62.69/s)  LR: 1.000e-05  Data: 0.005 (0.083)
2023-11-30 10:04:19,052 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.59/s  (0.491s,   65.14/s)  LR: 1.000e-05  Data: 0.005 (0.077)
2023-11-30 10:04:19,313 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.63/s  (0.475s,   67.39/s)  LR: 1.000e-05  Data: 0.004 (0.072)
2023-11-30 10:04:19,574 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.261s,  122.60/s  (0.461s,   69.48/s)  LR: 1.000e-05  Data: 0.004 (0.068)
2023-11-30 10:04:19,835 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.261s,  122.60/s  (0.448s,   71.41/s)  LR: 1.000e-05  Data: 0.005 (0.064)
2023-11-30 10:04:20,096 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.261s,  122.53/s  (0.437s,   73.21/s)  LR: 1.000e-05  Data: 0.005 (0.060)
2023-11-30 10:04:20,357 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.76/s  (0.427s,   74.89/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 10:04:20,618 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.261s,  122.55/s  (0.419s,   76.46/s)  LR: 1.000e-05  Data: 0.004 (0.054)
2023-11-30 10:04:20,879 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.58/s  (0.411s,   77.92/s)  LR: 1.000e-05  Data: 0.004 (0.052)
2023-11-30 10:04:21,140 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.262s,  122.33/s  (0.404s,   79.29/s)  LR: 1.000e-05  Data: 0.005 (0.050)
2023-11-30 10:04:21,402 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.261s,  122.38/s  (0.397s,   80.58/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 10:04:21,663 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.261s,  122.53/s  (0.391s,   81.80/s)  LR: 1.000e-05  Data: 0.004 (0.046)
2023-11-30 10:04:21,924 Train: 0 [  23/39 ( 61%)]  Loss: 0.943 (1.27)  Time: 0.261s,  122.52/s  (0.386s,   82.95/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 10:04:22,186 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.262s,  122.15/s  (0.381s,   84.03/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 10:04:22,448 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.261s,  122.37/s  (0.376s,   85.05/s)  LR: 1.000e-05  Data: 0.005 (0.041)
2023-11-30 10:04:22,709 Train: 0 [  26/39 ( 68%)]  Loss: 1.00 (1.25)  Time: 0.262s,  122.23/s  (0.372s,   86.02/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 10:04:22,972 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  121.97/s  (0.368s,   86.94/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 10:04:23,234 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.04/s  (0.364s,   87.81/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 10:04:23,496 Train: 0 [  29/39 ( 76%)]  Loss: 0.990 (1.25)  Time: 0.262s,  122.28/s  (0.361s,   88.64/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 10:04:23,757 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.262s,  122.32/s  (0.358s,   89.43/s)  LR: 1.000e-05  Data: 0.005 (0.035)
2023-11-30 10:04:24,019 Train: 0 [  31/39 ( 82%)]  Loss: 1.66 (1.26)  Time: 0.262s,  122.14/s  (0.355s,   90.19/s)  LR: 1.000e-05  Data: 0.005 (0.034)
2023-11-30 10:04:24,281 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.262s,  122.32/s  (0.352s,   90.91/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 10:04:24,544 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.263s,  121.47/s  (0.349s,   91.59/s)  LR: 1.000e-05  Data: 0.005 (0.032)
2023-11-30 10:04:24,808 Train: 0 [  34/39 ( 89%)]  Loss: 0.983 (1.25)  Time: 0.264s,  121.14/s  (0.347s,   92.23/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:04:25,070 Train: 0 [  35/39 ( 92%)]  Loss: 1.11 (1.24)  Time: 0.262s,  122.19/s  (0.345s,   92.87/s)  LR: 1.000e-05  Data: 0.005 (0.031)
2023-11-30 10:04:25,332 Train: 0 [  36/39 ( 95%)]  Loss: 1.29 (1.24)  Time: 0.262s,  122.18/s  (0.342s,   93.47/s)  LR: 1.000e-05  Data: 0.005 (0.030)
2023-11-30 10:04:25,593 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.48/s  (0.340s,   94.06/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 10:04:25,849 Train: 0 [  38/39 (100%)]  Loss: 2.58 (1.27)  Time: 0.256s,  125.14/s  (0.338s,   94.66/s)  LR: 1.000e-05  Data: 0.000 (0.029)
2023-11-30 10:04:26,881 Test: [   0/39]  Time: 1.028 (1.028)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:26,945 Test: [   1/39]  Time: 0.064 (0.546)  Loss:   0.185 ( 0.186)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:27,009 Test: [   2/39]  Time: 0.064 (0.385)  Loss:   0.187 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:27,386 Test: [   3/39]  Time: 0.377 (0.383)  Loss:   0.188 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:27,520 Test: [   4/39]  Time: 0.134 (0.333)  Loss:   0.189 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:27,584 Test: [   5/39]  Time: 0.064 (0.289)  Loss:   0.192 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:27,648 Test: [   6/39]  Time: 0.064 (0.256)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:27,978 Test: [   7/39]  Time: 0.330 (0.266)  Loss:   0.186 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,177 Test: [   8/39]  Time: 0.199 (0.258)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,240 Test: [   9/39]  Time: 0.064 (0.239)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,304 Test: [  10/39]  Time: 0.064 (0.223)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,620 Test: [  11/39]  Time: 0.315 (0.231)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,827 Test: [  12/39]  Time: 0.208 (0.229)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,891 Test: [  13/39]  Time: 0.064 (0.217)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:28,955 Test: [  14/39]  Time: 0.064 (0.207)  Loss:   0.189 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:29,286 Test: [  15/39]  Time: 0.331 (0.215)  Loss:   0.188 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:29,495 Test: [  16/39]  Time: 0.209 (0.214)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:29,559 Test: [  17/39]  Time: 0.064 (0.206)  Loss:   0.185 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:29,623 Test: [  18/39]  Time: 0.064 (0.198)  Loss:   0.187 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:29,891 Test: [  19/39]  Time: 0.269 (0.202)  Loss:   0.186 ( 0.188)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:30,112 Test: [  20/39]  Time: 0.221 (0.203)  Loss:   0.186 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:30,176 Test: [  21/39]  Time: 0.064 (0.197)  Loss:   0.184 ( 0.187)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:30,240 Test: [  22/39]  Time: 0.064 (0.191)  Loss:   1.351 ( 0.238)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:04:30,487 Test: [  23/39]  Time: 0.247 (0.193)  Loss:   1.759 ( 0.301)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:04:30,749 Test: [  24/39]  Time: 0.262 (0.196)  Loss:   1.757 ( 0.360)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:04:30,814 Test: [  25/39]  Time: 0.064 (0.191)  Loss:   1.767 ( 0.414)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:04:30,877 Test: [  26/39]  Time: 0.064 (0.186)  Loss:   1.759 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:04:31,124 Test: [  27/39]  Time: 0.247 (0.188)  Loss:   1.766 ( 0.510)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:04:31,336 Test: [  28/39]  Time: 0.212 (0.189)  Loss:   1.771 ( 0.554)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:04:31,400 Test: [  29/39]  Time: 0.064 (0.185)  Loss:   1.772 ( 0.594)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:04:31,466 Test: [  30/39]  Time: 0.066 (0.181)  Loss:   1.766 ( 0.632)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:04:31,755 Test: [  31/39]  Time: 0.289 (0.184)  Loss:   1.774 ( 0.668)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:04:31,959 Test: [  32/39]  Time: 0.205 (0.185)  Loss:   1.757 ( 0.701)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:04:32,023 Test: [  33/39]  Time: 0.064 (0.181)  Loss:   1.765 ( 0.732)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:04:32,087 Test: [  34/39]  Time: 0.064 (0.178)  Loss:   1.759 ( 0.761)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:04:32,153 Test: [  35/39]  Time: 0.066 (0.175)  Loss:   1.772 ( 0.789)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:04:32,472 Test: [  36/39]  Time: 0.318 (0.179)  Loss:   1.719 ( 0.814)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:04:33,134 Test: [  37/39]  Time: 0.662 (0.192)  Loss:   1.489 ( 0.832)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 10:04:33,195 Test: [  38/39]  Time: 0.061 (0.188)  Loss:   1.549 ( 0.851)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 10:04:33,386 Test: [  39/39]  Time: 0.191 (0.188)  Loss:   1.314 ( 0.851)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 10:04:33,600 Current checkpoints:
 ('./output/train/20231130-100412-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 10:04:33,600 *** Best metric: 57.28 (epoch 0)

2023-11-30 09:28:33,325 Training with a single process on 1 device (cuda:0).
2023-11-30 09:28:33,542 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:28:33,542 Data processing configuration for current model + dataset:
2023-11-30 09:28:33,542 	input_size: (3, 259, 259)
2023-11-30 09:28:33,542 	interpolation: bicubic
2023-11-30 09:28:33,542 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:28:33,542 	std: (0.229, 0.224, 0.225)
2023-11-30 09:28:33,542 	crop_pct: 0.875
2023-11-30 09:28:33,542 	crop_mode: center
2023-11-30 09:28:36,028 AMP not enabled. Training in float32.
2023-11-30 09:28:36,066 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:28:38,221 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:28:39,419 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.350s,    9.55/s  (3.350s,    9.55/s)  LR: 1.000e-05  Data: 0.999 (0.999)
2023-11-30 09:28:39,685 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.266s,  120.50/s  (1.808s,   17.70/s)  LR: 1.000e-05  Data: 0.008 (0.504)
2023-11-30 09:28:39,945 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.06/s  (1.292s,   24.77/s)  LR: 1.000e-05  Data: 0.003 (0.337)
2023-11-30 09:28:40,208 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.263s,  121.76/s  (1.035s,   30.93/s)  LR: 1.000e-05  Data: 0.005 (0.254)
2023-11-30 09:28:40,471 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.264s,  121.36/s  (0.881s,   36.34/s)  LR: 1.000e-05  Data: 0.007 (0.204)
2023-11-30 09:28:40,732 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  122.95/s  (0.777s,   41.18/s)  LR: 1.000e-05  Data: 0.003 (0.171)
2023-11-30 09:28:40,993 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.41/s  (0.703s,   45.49/s)  LR: 1.000e-05  Data: 0.005 (0.147)
2023-11-30 09:28:41,254 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.61/s  (0.648s,   49.37/s)  LR: 1.000e-05  Data: 0.005 (0.129)
2023-11-30 09:28:41,516 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.29/s  (0.605s,   52.87/s)  LR: 1.000e-05  Data: 0.004 (0.115)
2023-11-30 09:28:41,778 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.08/s  (0.571s,   56.05/s)  LR: 1.000e-05  Data: 0.005 (0.104)
2023-11-30 09:28:42,042 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.264s,  121.07/s  (0.543s,   58.93/s)  LR: 1.000e-05  Data: 0.004 (0.095)
2023-11-30 09:28:42,303 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.76/s  (0.519s,   61.60/s)  LR: 1.000e-05  Data: 0.005 (0.088)
2023-11-30 09:28:42,564 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.70/s  (0.500s,   64.05/s)  LR: 1.000e-05  Data: 0.005 (0.081)
2023-11-30 09:28:42,825 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.38/s  (0.483s,   66.31/s)  LR: 1.000e-05  Data: 0.005 (0.076)
2023-11-30 09:28:43,086 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.261s,  122.47/s  (0.468s,   68.40/s)  LR: 1.000e-05  Data: 0.004 (0.071)
2023-11-30 09:28:43,348 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.35/s  (0.455s,   70.34/s)  LR: 1.000e-05  Data: 0.005 (0.067)
2023-11-30 09:28:43,610 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.34/s  (0.444s,   72.14/s)  LR: 1.000e-05  Data: 0.005 (0.063)
2023-11-30 09:28:43,871 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.40/s  (0.433s,   73.83/s)  LR: 1.000e-05  Data: 0.005 (0.060)
2023-11-30 09:28:44,133 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.262s,  122.33/s  (0.424s,   75.40/s)  LR: 1.000e-05  Data: 0.005 (0.057)
2023-11-30 09:28:44,394 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.261s,  122.49/s  (0.416s,   76.88/s)  LR: 1.000e-05  Data: 0.005 (0.054)
2023-11-30 09:28:44,655 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.261s,  122.38/s  (0.409s,   78.26/s)  LR: 1.000e-05  Data: 0.005 (0.052)
2023-11-30 09:28:44,917 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.261s,  122.48/s  (0.402s,   79.57/s)  LR: 1.000e-05  Data: 0.005 (0.050)
2023-11-30 09:28:45,178 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.261s,  122.49/s  (0.396s,   80.80/s)  LR: 1.000e-05  Data: 0.005 (0.048)
2023-11-30 09:28:45,439 Train: 0 [  23/39 ( 61%)]  Loss: 0.939 (1.27)  Time: 0.262s,  122.29/s  (0.390s,   81.96/s)  LR: 1.000e-05  Data: 0.005 (0.046)
2023-11-30 09:28:45,702 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.263s,  121.86/s  (0.385s,   83.05/s)  LR: 1.000e-05  Data: 0.005 (0.044)
2023-11-30 09:28:45,964 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.32/s  (0.381s,   84.08/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:28:46,226 Train: 0 [  26/39 ( 68%)]  Loss: 0.995 (1.25)  Time: 0.262s,  122.18/s  (0.376s,   85.07/s)  LR: 1.000e-05  Data: 0.005 (0.042)
2023-11-30 09:28:46,487 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.262s,  122.29/s  (0.372s,   86.00/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:28:46,749 Train: 0 [  28/39 ( 74%)]  Loss: 1.87 (1.26)  Time: 0.262s,  122.35/s  (0.368s,   86.89/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 09:28:47,010 Train: 0 [  29/39 ( 76%)]  Loss: 0.987 (1.25)  Time: 0.261s,  122.43/s  (0.365s,   87.74/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 09:28:47,271 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.261s,  122.75/s  (0.361s,   88.56/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 09:28:47,531 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.260s,  123.04/s  (0.358s,   89.34/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:28:47,792 Train: 0 [  32/39 ( 84%)]  Loss: 1.04 (1.25)  Time: 0.261s,  122.69/s  (0.355s,   90.08/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:28:48,052 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.260s,  122.94/s  (0.352s,   90.79/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:28:48,312 Train: 0 [  34/39 ( 89%)]  Loss: 0.974 (1.25)  Time: 0.260s,  123.18/s  (0.350s,   91.48/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:28:48,571 Train: 0 [  35/39 ( 92%)]  Loss: 1.10 (1.24)  Time: 0.260s,  123.21/s  (0.347s,   92.14/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:28:48,831 Train: 0 [  36/39 ( 95%)]  Loss: 1.21 (1.24)  Time: 0.259s,  123.34/s  (0.345s,   92.78/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:28:49,090 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.23)  Time: 0.260s,  123.29/s  (0.343s,   93.38/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:28:49,346 Train: 0 [  38/39 (100%)]  Loss: 2.61 (1.27)  Time: 0.256s,  125.17/s  (0.340s,   94.00/s)  LR: 1.000e-05  Data: 0.000 (0.030)
2023-11-30 09:28:50,541 Test: [   0/39]  Time: 1.191 (1.191)  Loss:   0.179 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:50,605 Test: [   1/39]  Time: 0.064 (0.628)  Loss:   0.176 ( 0.178)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:50,668 Test: [   2/39]  Time: 0.064 (0.440)  Loss:   0.179 ( 0.178)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,020 Test: [   3/39]  Time: 0.352 (0.418)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,155 Test: [   4/39]  Time: 0.135 (0.361)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,219 Test: [   5/39]  Time: 0.064 (0.312)  Loss:   0.183 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,282 Test: [   6/39]  Time: 0.064 (0.276)  Loss:   0.179 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,614 Test: [   7/39]  Time: 0.331 (0.283)  Loss:   0.178 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,808 Test: [   8/39]  Time: 0.194 (0.273)  Loss:   0.178 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,872 Test: [   9/39]  Time: 0.064 (0.252)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:51,935 Test: [  10/39]  Time: 0.064 (0.235)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:52,267 Test: [  11/39]  Time: 0.332 (0.243)  Loss:   0.178 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:52,460 Test: [  12/39]  Time: 0.193 (0.239)  Loss:   0.179 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:52,524 Test: [  13/39]  Time: 0.064 (0.227)  Loss:   0.179 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:52,588 Test: [  14/39]  Time: 0.064 (0.216)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:52,943 Test: [  15/39]  Time: 0.355 (0.225)  Loss:   0.180 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,119 Test: [  16/39]  Time: 0.176 (0.222)  Loss:   0.179 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,183 Test: [  17/39]  Time: 0.064 (0.213)  Loss:   0.176 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,247 Test: [  18/39]  Time: 0.064 (0.205)  Loss:   0.179 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,565 Test: [  19/39]  Time: 0.318 (0.211)  Loss:   0.177 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,751 Test: [  20/39]  Time: 0.186 (0.210)  Loss:   0.178 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,815 Test: [  21/39]  Time: 0.064 (0.203)  Loss:   0.176 ( 0.179)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:53,879 Test: [  22/39]  Time: 0.064 (0.197)  Loss:   1.382 ( 0.231)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:28:54,162 Test: [  23/39]  Time: 0.283 (0.201)  Loss:   1.801 ( 0.296)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:28:54,383 Test: [  24/39]  Time: 0.222 (0.201)  Loss:   1.799 ( 0.357)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:28:54,447 Test: [  25/39]  Time: 0.064 (0.196)  Loss:   1.810 ( 0.412)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:28:54,511 Test: [  26/39]  Time: 0.064 (0.191)  Loss:   1.802 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:28:54,789 Test: [  27/39]  Time: 0.278 (0.194)  Loss:   1.808 ( 0.512)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:28:54,960 Test: [  28/39]  Time: 0.171 (0.193)  Loss:   1.814 ( 0.557)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:28:55,024 Test: [  29/39]  Time: 0.064 (0.189)  Loss:   1.814 ( 0.599)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:28:55,088 Test: [  30/39]  Time: 0.064 (0.185)  Loss:   1.809 ( 0.638)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:28:55,409 Test: [  31/39]  Time: 0.322 (0.189)  Loss:   1.817 ( 0.675)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:28:55,567 Test: [  32/39]  Time: 0.158 (0.188)  Loss:   1.799 ( 0.709)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:28:55,631 Test: [  33/39]  Time: 0.064 (0.185)  Loss:   1.808 ( 0.741)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:28:55,694 Test: [  34/39]  Time: 0.064 (0.181)  Loss:   1.801 ( 0.771)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:28:55,772 Test: [  35/39]  Time: 0.077 (0.178)  Loss:   1.814 ( 0.800)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:28:56,079 Test: [  36/39]  Time: 0.307 (0.182)  Loss:   1.767 ( 0.826)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:28:56,751 Test: [  37/39]  Time: 0.672 (0.195)  Loss:   1.543 ( 0.845)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 09:28:56,812 Test: [  38/39]  Time: 0.061 (0.191)  Loss:   1.601 ( 0.865)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 09:28:57,016 Test: [  39/39]  Time: 0.204 (0.192)  Loss:   1.361 ( 0.865)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 09:28:57,226 Current checkpoints:
 ('./output/train/20231130-092836-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 09:28:57,226 *** Best metric: 57.12 (epoch 0)

2023-11-30 09:05:19,157 Training with a single process on 1 device (cuda:0).
2023-11-30 09:05:19,375 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:05:19,375 Data processing configuration for current model + dataset:
2023-11-30 09:05:19,375 	input_size: (3, 259, 259)
2023-11-30 09:05:19,375 	interpolation: bicubic
2023-11-30 09:05:19,375 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:05:19,375 	std: (0.229, 0.224, 0.225)
2023-11-30 09:05:19,375 	crop_pct: 0.875
2023-11-30 09:05:19,375 	crop_mode: center
2023-11-30 09:05:21,819 AMP not enabled. Training in float32.
2023-11-30 09:05:21,862 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:05:24,037 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:05:25,230 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.366s,    9.51/s  (3.366s,    9.51/s)  LR: 1.000e-05  Data: 1.033 (1.033)
2023-11-30 09:05:25,491 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.261s,  122.82/s  (1.813s,   17.65/s)  LR: 1.000e-05  Data: 0.003 (0.518)
2023-11-30 09:05:25,751 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  122.94/s  (1.296s,   24.70/s)  LR: 1.000e-05  Data: 0.003 (0.346)
2023-11-30 09:05:26,018 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.267s,  120.07/s  (1.038s,   30.82/s)  LR: 1.000e-05  Data: 0.008 (0.262)
2023-11-30 09:05:26,279 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.261s,  122.40/s  (0.883s,   36.24/s)  LR: 1.000e-05  Data: 0.004 (0.210)
2023-11-30 09:05:26,539 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.14/s  (0.779s,   41.07/s)  LR: 1.000e-05  Data: 0.003 (0.176)
2023-11-30 09:05:26,801 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.04/s  (0.705s,   45.37/s)  LR: 1.000e-05  Data: 0.003 (0.151)
2023-11-30 09:05:27,063 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.262s,  122.23/s  (0.650s,   49.24/s)  LR: 1.000e-05  Data: 0.004 (0.133)
2023-11-30 09:05:27,326 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.263s,  121.66/s  (0.607s,   52.73/s)  LR: 1.000e-05  Data: 0.005 (0.118)
2023-11-30 09:05:27,589 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  121.91/s  (0.572s,   55.90/s)  LR: 1.000e-05  Data: 0.005 (0.107)
2023-11-30 09:05:27,853 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.264s,  121.20/s  (0.544s,   58.78/s)  LR: 1.000e-05  Data: 0.004 (0.098)
2023-11-30 09:05:28,118 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.265s,  120.78/s  (0.521s,   61.41/s)  LR: 1.000e-05  Data: 0.005 (0.090)
2023-11-30 09:05:28,383 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.265s,  120.62/s  (0.501s,   63.82/s)  LR: 1.000e-05  Data: 0.005 (0.083)
2023-11-30 09:05:28,645 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.03/s  (0.484s,   66.07/s)  LR: 1.000e-05  Data: 0.005 (0.078)
2023-11-30 09:05:28,907 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.19/s  (0.470s,   68.16/s)  LR: 1.000e-05  Data: 0.005 (0.073)
2023-11-30 09:05:29,169 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.19/s  (0.457s,   70.09/s)  LR: 1.000e-05  Data: 0.005 (0.069)
2023-11-30 09:05:29,431 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.26/s  (0.445s,   71.90/s)  LR: 1.000e-05  Data: 0.005 (0.065)
2023-11-30 09:05:29,693 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.14/s  (0.435s,   73.58/s)  LR: 1.000e-05  Data: 0.005 (0.062)
2023-11-30 09:05:29,955 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.263s,  121.79/s  (0.426s,   75.14/s)  LR: 1.000e-05  Data: 0.005 (0.059)
2023-11-30 09:05:30,217 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.07/s  (0.418s,   76.62/s)  LR: 1.000e-05  Data: 0.005 (0.056)
2023-11-30 09:05:30,480 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.262s,  122.07/s  (0.410s,   78.00/s)  LR: 1.000e-05  Data: 0.005 (0.053)
2023-11-30 09:05:30,742 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  121.91/s  (0.404s,   79.30/s)  LR: 1.000e-05  Data: 0.005 (0.051)
2023-11-30 09:05:31,004 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.04/s  (0.397s,   80.52/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 09:05:31,268 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.263s,  121.57/s  (0.392s,   81.67/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 09:05:31,531 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.264s,  121.31/s  (0.387s,   82.75/s)  LR: 1.000e-05  Data: 0.006 (0.046)
2023-11-30 09:05:31,793 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.10/s  (0.382s,   83.79/s)  LR: 1.000e-05  Data: 0.005 (0.044)
2023-11-30 09:05:32,056 Train: 0 [  26/39 ( 68%)]  Loss: 0.999 (1.25)  Time: 0.262s,  122.05/s  (0.377s,   84.78/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 09:05:32,321 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.265s,  120.63/s  (0.373s,   85.69/s)  LR: 1.000e-05  Data: 0.005 (0.041)
2023-11-30 09:05:32,583 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.03/s  (0.370s,   86.58/s)  LR: 1.000e-05  Data: 0.005 (0.040)
2023-11-30 09:05:32,845 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  122.19/s  (0.366s,   87.43/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:05:33,107 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.262s,  122.30/s  (0.363s,   88.24/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 09:05:33,368 Train: 0 [  31/39 ( 82%)]  Loss: 1.68 (1.26)  Time: 0.261s,  122.40/s  (0.359s,   89.01/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:05:33,628 Train: 0 [  32/39 ( 84%)]  Loss: 1.06 (1.25)  Time: 0.260s,  122.97/s  (0.356s,   89.76/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:05:33,890 Train: 0 [  33/39 ( 87%)]  Loss: 1.34 (1.25)  Time: 0.262s,  122.24/s  (0.354s,   90.47/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:05:34,151 Train: 0 [  34/39 ( 89%)]  Loss: 0.995 (1.25)  Time: 0.261s,  122.54/s  (0.351s,   91.15/s)  LR: 1.000e-05  Data: 0.005 (0.034)
2023-11-30 09:05:34,412 Train: 0 [  35/39 ( 92%)]  Loss: 1.18 (1.25)  Time: 0.261s,  122.68/s  (0.349s,   91.81/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:05:34,672 Train: 0 [  36/39 ( 95%)]  Loss: 1.33 (1.25)  Time: 0.260s,  123.10/s  (0.346s,   92.44/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:05:34,931 Train: 0 [  37/39 ( 97%)]  Loss: 1.02 (1.24)  Time: 0.259s,  123.39/s  (0.344s,   93.06/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:05:35,187 Train: 0 [  38/39 (100%)]  Loss: 2.46 (1.27)  Time: 0.256s,  124.93/s  (0.342s,   93.67/s)  LR: 1.000e-05  Data: 0.000 (0.031)
2023-11-30 09:05:36,301 Test: [   0/39]  Time: 1.110 (1.110)  Loss:   0.210 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:36,365 Test: [   1/39]  Time: 0.064 (0.587)  Loss:   0.206 ( 0.208)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:36,429 Test: [   2/39]  Time: 0.064 (0.413)  Loss:   0.210 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:36,807 Test: [   3/39]  Time: 0.378 (0.404)  Loss:   0.211 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:36,925 Test: [   4/39]  Time: 0.118 (0.347)  Loss:   0.211 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:36,989 Test: [   5/39]  Time: 0.064 (0.300)  Loss:   0.214 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:37,053 Test: [   6/39]  Time: 0.064 (0.266)  Loss:   0.209 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:37,394 Test: [   7/39]  Time: 0.341 (0.275)  Loss:   0.208 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:37,575 Test: [   8/39]  Time: 0.181 (0.265)  Loss:   0.209 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:37,639 Test: [   9/39]  Time: 0.064 (0.245)  Loss:   0.211 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:37,702 Test: [  10/39]  Time: 0.064 (0.228)  Loss:   0.211 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,034 Test: [  11/39]  Time: 0.332 (0.237)  Loss:   0.208 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,217 Test: [  12/39]  Time: 0.182 (0.233)  Loss:   0.210 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,282 Test: [  13/39]  Time: 0.065 (0.221)  Loss:   0.209 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,345 Test: [  14/39]  Time: 0.064 (0.210)  Loss:   0.210 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,703 Test: [  15/39]  Time: 0.358 (0.219)  Loss:   0.210 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,873 Test: [  16/39]  Time: 0.170 (0.217)  Loss:   0.209 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:38,937 Test: [  17/39]  Time: 0.064 (0.208)  Loss:   0.207 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:39,000 Test: [  18/39]  Time: 0.064 (0.200)  Loss:   0.209 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:39,316 Test: [  19/39]  Time: 0.316 (0.206)  Loss:   0.208 ( 0.210)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:39,501 Test: [  20/39]  Time: 0.185 (0.205)  Loss:   0.208 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:39,565 Test: [  21/39]  Time: 0.064 (0.199)  Loss:   0.206 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:39,629 Test: [  22/39]  Time: 0.064 (0.193)  Loss:   1.283 ( 0.256)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:05:39,908 Test: [  23/39]  Time: 0.279 (0.197)  Loss:   1.659 ( 0.314)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:05:40,126 Test: [  24/39]  Time: 0.218 (0.197)  Loss:   1.657 ( 0.368)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:05:40,190 Test: [  25/39]  Time: 0.064 (0.192)  Loss:   1.666 ( 0.418)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:05:40,253 Test: [  26/39]  Time: 0.064 (0.187)  Loss:   1.659 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:05:40,529 Test: [  27/39]  Time: 0.275 (0.191)  Loss:   1.666 ( 0.507)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:05:40,710 Test: [  28/39]  Time: 0.181 (0.190)  Loss:   1.671 ( 0.547)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:05:40,774 Test: [  29/39]  Time: 0.064 (0.186)  Loss:   1.671 ( 0.585)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:05:40,837 Test: [  30/39]  Time: 0.064 (0.182)  Loss:   1.666 ( 0.619)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:05:41,150 Test: [  31/39]  Time: 0.313 (0.186)  Loss:   1.673 ( 0.652)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:05:41,312 Test: [  32/39]  Time: 0.162 (0.185)  Loss:   1.656 ( 0.683)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:05:41,376 Test: [  33/39]  Time: 0.064 (0.182)  Loss:   1.665 ( 0.712)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:05:41,440 Test: [  34/39]  Time: 0.064 (0.179)  Loss:   1.659 ( 0.739)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:05:41,510 Test: [  35/39]  Time: 0.070 (0.176)  Loss:   1.671 ( 0.765)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:05:41,816 Test: [  36/39]  Time: 0.306 (0.179)  Loss:   1.625 ( 0.788)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:05:42,489 Test: [  37/39]  Time: 0.673 (0.192)  Loss:   1.417 ( 0.804)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 09:05:42,550 Test: [  38/39]  Time: 0.061 (0.189)  Loss:   1.472 ( 0.822)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 09:05:42,745 Test: [  39/39]  Time: 0.196 (0.189)  Loss:   1.246 ( 0.822)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 09:05:42,959 Current checkpoints:
 ('./output/train/20231130-090521-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 09:05:42,960 *** Best metric: 57.28 (epoch 0)

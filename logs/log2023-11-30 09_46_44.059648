2023-11-30 09:46:44,113 Training with a single process on 1 device (cuda:0).
2023-11-30 09:46:44,327 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:46:44,327 Data processing configuration for current model + dataset:
2023-11-30 09:46:44,327 	input_size: (3, 259, 259)
2023-11-30 09:46:44,327 	interpolation: bicubic
2023-11-30 09:46:44,327 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:46:44,327 	std: (0.229, 0.224, 0.225)
2023-11-30 09:46:44,327 	crop_pct: 0.875
2023-11-30 09:46:44,327 	crop_mode: center
2023-11-30 09:46:46,747 AMP not enabled. Training in float32.
2023-11-30 09:46:46,788 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:46:48,927 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:46:50,131 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.340s,    9.58/s  (3.340s,    9.58/s)  LR: 1.000e-05  Data: 0.990 (0.990)
2023-11-30 09:46:50,393 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.262s,  121.91/s  (1.801s,   17.77/s)  LR: 1.000e-05  Data: 0.006 (0.498)
2023-11-30 09:46:50,653 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.16/s  (1.287s,   24.86/s)  LR: 1.000e-05  Data: 0.003 (0.333)
2023-11-30 09:46:50,914 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.35/s  (1.031s,   31.04/s)  LR: 1.000e-05  Data: 0.004 (0.251)
2023-11-30 09:46:51,178 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.263s,  121.64/s  (0.877s,   36.47/s)  LR: 1.000e-05  Data: 0.005 (0.202)
2023-11-30 09:46:51,438 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.261s,  122.74/s  (0.775s,   41.31/s)  LR: 1.000e-05  Data: 0.003 (0.168)
2023-11-30 09:46:51,700 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.24/s  (0.701s,   45.63/s)  LR: 1.000e-05  Data: 0.004 (0.145)
2023-11-30 09:46:51,961 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.64/s  (0.646s,   49.51/s)  LR: 1.000e-05  Data: 0.005 (0.127)
2023-11-30 09:46:52,223 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.21/s  (0.604s,   53.02/s)  LR: 1.000e-05  Data: 0.004 (0.114)
2023-11-30 09:46:52,484 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.33/s  (0.569s,   56.20/s)  LR: 1.000e-05  Data: 0.004 (0.103)
2023-11-30 09:46:52,747 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  121.93/s  (0.541s,   59.10/s)  LR: 1.000e-05  Data: 0.004 (0.094)
2023-11-30 09:46:53,010 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.263s,  121.82/s  (0.518s,   61.75/s)  LR: 1.000e-05  Data: 0.005 (0.087)
2023-11-30 09:46:53,271 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.38/s  (0.498s,   64.19/s)  LR: 1.000e-05  Data: 0.004 (0.080)
2023-11-30 09:46:53,536 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.265s,  120.84/s  (0.482s,   66.42/s)  LR: 1.000e-05  Data: 0.007 (0.075)
2023-11-30 09:46:53,800 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.264s,  121.26/s  (0.467s,   68.48/s)  LR: 1.000e-05  Data: 0.004 (0.070)
2023-11-30 09:46:54,064 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.264s,  121.13/s  (0.455s,   70.40/s)  LR: 1.000e-05  Data: 0.005 (0.066)
2023-11-30 09:46:54,326 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.26/s  (0.443s,   72.20/s)  LR: 1.000e-05  Data: 0.005 (0.062)
2023-11-30 09:46:54,587 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.29/s  (0.433s,   73.88/s)  LR: 1.000e-05  Data: 0.004 (0.059)
2023-11-30 09:46:54,849 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.262s,  122.27/s  (0.424s,   75.45/s)  LR: 1.000e-05  Data: 0.005 (0.056)
2023-11-30 09:46:55,111 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.20/s  (0.416s,   76.92/s)  LR: 1.000e-05  Data: 0.004 (0.054)
2023-11-30 09:46:55,372 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.261s,  122.37/s  (0.409s,   78.31/s)  LR: 1.000e-05  Data: 0.004 (0.051)
2023-11-30 09:46:55,634 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.37/s  (0.402s,   79.61/s)  LR: 1.000e-05  Data: 0.004 (0.049)
2023-11-30 09:46:55,896 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.02/s  (0.396s,   80.83/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 09:46:56,158 Train: 0 [  23/39 ( 61%)]  Loss: 0.943 (1.27)  Time: 0.262s,  122.23/s  (0.390s,   81.99/s)  LR: 1.000e-05  Data: 0.005 (0.046)
2023-11-30 09:46:56,419 Train: 0 [  24/39 ( 63%)]  Loss: 1.30 (1.27)  Time: 0.261s,  122.38/s  (0.385s,   83.09/s)  LR: 1.000e-05  Data: 0.004 (0.044)
2023-11-30 09:46:56,681 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.30/s  (0.380s,   84.12/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 09:46:56,943 Train: 0 [  26/39 ( 68%)]  Loss: 1.00 (1.25)  Time: 0.262s,  122.17/s  (0.376s,   85.10/s)  LR: 1.000e-05  Data: 0.004 (0.041)
2023-11-30 09:46:57,205 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  122.17/s  (0.372s,   86.04/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 09:46:57,467 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.10/s  (0.368s,   86.92/s)  LR: 1.000e-05  Data: 0.005 (0.038)
2023-11-30 09:46:57,729 Train: 0 [  29/39 ( 76%)]  Loss: 0.991 (1.25)  Time: 0.262s,  122.22/s  (0.365s,   87.77/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 09:46:57,990 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.262s,  122.30/s  (0.361s,   88.57/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:46:58,251 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.76/s  (0.358s,   89.35/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:46:58,511 Train: 0 [  32/39 ( 84%)]  Loss: 1.06 (1.25)  Time: 0.260s,  123.05/s  (0.355s,   90.10/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:46:58,772 Train: 0 [  33/39 ( 87%)]  Loss: 1.35 (1.25)  Time: 0.261s,  122.76/s  (0.352s,   90.81/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:46:59,033 Train: 0 [  34/39 ( 89%)]  Loss: 0.994 (1.25)  Time: 0.261s,  122.72/s  (0.350s,   91.49/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:46:59,293 Train: 0 [  35/39 ( 92%)]  Loss: 1.16 (1.24)  Time: 0.260s,  122.88/s  (0.347s,   92.14/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:46:59,554 Train: 0 [  36/39 ( 95%)]  Loss: 1.34 (1.25)  Time: 0.261s,  122.70/s  (0.345s,   92.77/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:46:59,814 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.260s,  122.94/s  (0.343s,   93.37/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:47:00,072 Train: 0 [  38/39 (100%)]  Loss: 2.49 (1.27)  Time: 0.258s,  124.21/s  (0.341s,   93.97/s)  LR: 1.000e-05  Data: 0.000 (0.030)
2023-11-30 09:47:01,227 Test: [   0/39]  Time: 1.151 (1.151)  Loss:   0.203 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:01,291 Test: [   1/39]  Time: 0.064 (0.608)  Loss:   0.200 ( 0.202)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:01,355 Test: [   2/39]  Time: 0.064 (0.426)  Loss:   0.203 ( 0.202)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:01,748 Test: [   3/39]  Time: 0.394 (0.418)  Loss:   0.204 ( 0.202)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:01,870 Test: [   4/39]  Time: 0.121 (0.359)  Loss:   0.204 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:01,934 Test: [   5/39]  Time: 0.064 (0.310)  Loss:   0.208 ( 0.204)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:01,997 Test: [   6/39]  Time: 0.064 (0.275)  Loss:   0.203 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:02,353 Test: [   7/39]  Time: 0.355 (0.285)  Loss:   0.202 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:02,534 Test: [   8/39]  Time: 0.181 (0.273)  Loss:   0.202 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:02,598 Test: [   9/39]  Time: 0.064 (0.252)  Loss:   0.204 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:02,661 Test: [  10/39]  Time: 0.064 (0.235)  Loss:   0.204 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,009 Test: [  11/39]  Time: 0.348 (0.244)  Loss:   0.202 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,194 Test: [  12/39]  Time: 0.184 (0.240)  Loss:   0.203 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,257 Test: [  13/39]  Time: 0.064 (0.227)  Loss:   0.203 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,321 Test: [  14/39]  Time: 0.064 (0.216)  Loss:   0.204 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,677 Test: [  15/39]  Time: 0.356 (0.225)  Loss:   0.204 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,856 Test: [  16/39]  Time: 0.180 (0.222)  Loss:   0.203 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,920 Test: [  17/39]  Time: 0.064 (0.214)  Loss:   0.200 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:03,984 Test: [  18/39]  Time: 0.064 (0.206)  Loss:   0.203 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:04,300 Test: [  19/39]  Time: 0.317 (0.211)  Loss:   0.201 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:04,500 Test: [  20/39]  Time: 0.199 (0.211)  Loss:   0.201 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:04,563 Test: [  21/39]  Time: 0.064 (0.204)  Loss:   0.199 ( 0.203)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:04,627 Test: [  22/39]  Time: 0.064 (0.198)  Loss:   1.303 ( 0.250)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:47:04,902 Test: [  23/39]  Time: 0.274 (0.201)  Loss:   1.688 ( 0.310)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:47:05,131 Test: [  24/39]  Time: 0.230 (0.202)  Loss:   1.686 ( 0.365)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:47:05,195 Test: [  25/39]  Time: 0.064 (0.197)  Loss:   1.696 ( 0.417)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:47:05,259 Test: [  26/39]  Time: 0.064 (0.192)  Loss:   1.689 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:47:05,525 Test: [  27/39]  Time: 0.266 (0.195)  Loss:   1.695 ( 0.508)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:47:05,715 Test: [  28/39]  Time: 0.190 (0.194)  Loss:   1.701 ( 0.549)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:47:05,778 Test: [  29/39]  Time: 0.064 (0.190)  Loss:   1.701 ( 0.587)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:47:05,844 Test: [  30/39]  Time: 0.065 (0.186)  Loss:   1.695 ( 0.623)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:47:06,145 Test: [  31/39]  Time: 0.301 (0.190)  Loss:   1.703 ( 0.657)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:47:06,325 Test: [  32/39]  Time: 0.180 (0.189)  Loss:   1.686 ( 0.688)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:47:06,388 Test: [  33/39]  Time: 0.064 (0.186)  Loss:   1.695 ( 0.717)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:47:06,452 Test: [  34/39]  Time: 0.064 (0.182)  Loss:   1.688 ( 0.745)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:47:06,518 Test: [  35/39]  Time: 0.067 (0.179)  Loss:   1.700 ( 0.772)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:47:06,837 Test: [  36/39]  Time: 0.319 (0.183)  Loss:   1.650 ( 0.795)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:47:07,471 Test: [  37/39]  Time: 0.634 (0.195)  Loss:   1.433 ( 0.812)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 09:47:07,532 Test: [  38/39]  Time: 0.061 (0.191)  Loss:   1.490 ( 0.830)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 09:47:07,727 Test: [  39/39]  Time: 0.195 (0.191)  Loss:   1.265 ( 0.830)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 09:47:07,933 Current checkpoints:
 ('./output/train/20231130-094646-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 09:47:07,933 *** Best metric: 57.28 (epoch 0)

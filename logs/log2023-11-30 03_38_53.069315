2023-11-30 03:38:53,093 Training with a single process on 1 device (cuda:0).
2023-11-30 03:38:53,228 Model efficientnet_b0 created, param count:6520930
2023-11-30 03:38:53,228 Data processing configuration for current model + dataset:
2023-11-30 03:38:53,228 	input_size: (3, 312, 312)
2023-11-30 03:38:53,228 	interpolation: bicubic
2023-11-30 03:38:53,228 	mean: (0.485, 0.456, 0.406)
2023-11-30 03:38:53,228 	std: (0.229, 0.224, 0.225)
2023-11-30 03:38:53,228 	crop_pct: 0.875
2023-11-30 03:38:53,228 	crop_mode: center
2023-11-30 03:38:55,579 AMP not enabled. Training in float32.
2023-11-30 03:38:55,606 Scheduled epochs: 50. LR stepped per epoch.
2023-11-30 03:38:57,681 FLOPs: 37.355884288 GFLOPs
2023-11-30 03:38:58,615 Train: 0 [   0/22 (  0%)]  Loss: 0.790 (0.790)  Time: 3.003s,   10.66/s  (3.003s,   10.66/s)  LR: 1.000e-05  Data: 0.966 (0.966)
2023-11-30 03:38:58,896 Train: 0 [   1/22 (  5%)]  Loss: 0.669 (0.730)  Time: 0.280s,  114.24/s  (1.642s,   19.49/s)  LR: 1.000e-05  Data: 0.006 (0.486)
2023-11-30 03:38:59,177 Train: 0 [   2/22 ( 10%)]  Loss: 1.46 (0.972)  Time: 0.282s,  113.57/s  (1.188s,   26.93/s)  LR: 1.000e-05  Data: 0.006 (0.326)
2023-11-30 03:38:59,462 Train: 0 [   3/22 ( 14%)]  Loss: 0.370 (0.822)  Time: 0.284s,  112.56/s  (0.962s,   33.25/s)  LR: 1.000e-05  Data: 0.010 (0.247)
2023-11-30 03:38:59,741 Train: 0 [   4/22 ( 19%)]  Loss: 0.545 (0.766)  Time: 0.279s,  114.53/s  (0.826s,   38.75/s)  LR: 1.000e-05  Data: 0.005 (0.199)
2023-11-30 03:39:00,019 Train: 0 [   5/22 ( 24%)]  Loss: 0.257 (0.681)  Time: 0.278s,  114.96/s  (0.734s,   43.57/s)  LR: 1.000e-05  Data: 0.004 (0.166)
2023-11-30 03:39:00,297 Train: 0 [   6/22 ( 29%)]  Loss: 0.803 (0.699)  Time: 0.278s,  115.11/s  (0.669s,   47.81/s)  LR: 1.000e-05  Data: 0.004 (0.143)
2023-11-30 03:39:00,577 Train: 0 [   7/22 ( 33%)]  Loss: 0.521 (0.677)  Time: 0.280s,  114.48/s  (0.621s,   51.57/s)  LR: 1.000e-05  Data: 0.005 (0.126)
2023-11-30 03:39:00,857 Train: 0 [   8/22 ( 38%)]  Loss: 0.490 (0.656)  Time: 0.280s,  114.31/s  (0.583s,   54.92/s)  LR: 1.000e-05  Data: 0.005 (0.112)
2023-11-30 03:39:01,136 Train: 0 [   9/22 ( 43%)]  Loss: 0.597 (0.650)  Time: 0.279s,  114.64/s  (0.552s,   57.93/s)  LR: 1.000e-05  Data: 0.005 (0.102)
2023-11-30 03:39:01,415 Train: 0 [  10/22 ( 48%)]  Loss: 0.775 (0.661)  Time: 0.279s,  114.52/s  (0.528s,   60.66/s)  LR: 1.000e-05  Data: 0.005 (0.093)
2023-11-30 03:39:01,695 Train: 0 [  11/22 ( 52%)]  Loss: 0.626 (0.658)  Time: 0.280s,  114.42/s  (0.507s,   63.13/s)  LR: 1.000e-05  Data: 0.006 (0.086)
2023-11-30 03:39:01,975 Train: 0 [  12/22 ( 57%)]  Loss: 0.512 (0.647)  Time: 0.280s,  114.33/s  (0.489s,   65.38/s)  LR: 1.000e-05  Data: 0.005 (0.079)
2023-11-30 03:39:02,254 Train: 0 [  13/22 ( 62%)]  Loss: 0.430 (0.632)  Time: 0.279s,  114.63/s  (0.474s,   67.45/s)  LR: 1.000e-05  Data: 0.005 (0.074)
2023-11-30 03:39:02,533 Train: 0 [  14/22 ( 67%)]  Loss: 0.384 (0.615)  Time: 0.278s,  114.91/s  (0.461s,   69.36/s)  LR: 1.000e-05  Data: 0.005 (0.070)
2023-11-30 03:39:02,810 Train: 0 [  15/22 ( 71%)]  Loss: 0.311 (0.596)  Time: 0.278s,  115.16/s  (0.450s,   71.13/s)  LR: 1.000e-05  Data: 0.005 (0.065)
2023-11-30 03:39:03,089 Train: 0 [  16/22 ( 76%)]  Loss: 1.10 (0.626)  Time: 0.278s,  115.07/s  (0.440s,   72.76/s)  LR: 1.000e-05  Data: 0.005 (0.062)
2023-11-30 03:39:03,367 Train: 0 [  17/22 ( 81%)]  Loss: 0.388 (0.612)  Time: 0.279s,  114.75/s  (0.431s,   74.27/s)  LR: 1.000e-05  Data: 0.005 (0.059)
2023-11-30 03:39:03,646 Train: 0 [  18/22 ( 86%)]  Loss: 0.629 (0.613)  Time: 0.278s,  114.95/s  (0.423s,   75.68/s)  LR: 1.000e-05  Data: 0.005 (0.056)
2023-11-30 03:39:03,924 Train: 0 [  19/22 ( 90%)]  Loss: 0.606 (0.613)  Time: 0.279s,  114.89/s  (0.416s,   77.00/s)  LR: 1.000e-05  Data: 0.005 (0.053)
2023-11-30 03:39:04,203 Train: 0 [  20/22 ( 95%)]  Loss: 0.294 (0.598)  Time: 0.278s,  115.02/s  (0.409s,   78.23/s)  LR: 1.000e-05  Data: 0.005 (0.051)
2023-11-30 03:39:04,476 Train: 0 [  21/22 (100%)]  Loss: 0.387 (0.588)  Time: 0.273s,  117.13/s  (0.403s,   79.43/s)  LR: 1.000e-05  Data: 0.000 (0.049)
2023-11-30 03:39:05,662 Test: [   0/21]  Time: 1.183 (1.183)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:05,730 Test: [   1/21]  Time: 0.068 (0.625)  Loss:   0.172 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:05,797 Test: [   2/21]  Time: 0.067 (0.439)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:06,268 Test: [   3/21]  Time: 0.470 (0.447)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:06,340 Test: [   4/21]  Time: 0.072 (0.372)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:06,407 Test: [   5/21]  Time: 0.067 (0.321)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:06,474 Test: [   6/21]  Time: 0.067 (0.285)  Loss:   0.170 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:06,902 Test: [   7/21]  Time: 0.428 (0.303)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:06,994 Test: [   8/21]  Time: 0.092 (0.279)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:07,061 Test: [   9/21]  Time: 0.067 (0.258)  Loss:   0.172 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:07,128 Test: [  10/21]  Time: 0.067 (0.241)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:07,584 Test: [  11/21]  Time: 0.456 (0.259)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:07,681 Test: [  12/21]  Time: 0.097 (0.246)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:07,750 Test: [  13/21]  Time: 0.069 (0.234)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:07,819 Test: [  14/21]  Time: 0.069 (0.223)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:08,255 Test: [  15/21]  Time: 0.436 (0.236)  Loss:   0.170 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:08,381 Test: [  16/21]  Time: 0.126 (0.230)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:08,448 Test: [  17/21]  Time: 0.067 (0.220)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:08,517 Test: [  18/21]  Time: 0.069 (0.212)  Loss:   0.170 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:08,857 Test: [  19/21]  Time: 0.340 (0.219)  Loss:   0.171 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:08,987 Test: [  20/21]  Time: 0.130 (0.215)  Loss:   0.170 ( 0.171)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:09,050 Test: [  21/21]  Time: 0.063 (0.208)  Loss:   0.329 ( 0.178)  Acc@1:  90.625 ( 99.574)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 03:39:09,214 Current checkpoints:
 ('./output/train/20231130-033855-efficientnet_b0-312/checkpoint-0.pth.tar', 99.57386363636364)


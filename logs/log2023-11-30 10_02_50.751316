2023-11-30 10:02:50,802 Training with a single process on 1 device (cuda:0).
2023-11-30 10:02:51,022 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:02:51,022 Data processing configuration for current model + dataset:
2023-11-30 10:02:51,023 	input_size: (3, 259, 259)
2023-11-30 10:02:51,023 	interpolation: bicubic
2023-11-30 10:02:51,023 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:02:51,023 	std: (0.229, 0.224, 0.225)
2023-11-30 10:02:51,023 	crop_pct: 0.875
2023-11-30 10:02:51,023 	crop_mode: center
2023-11-30 10:02:53,466 AMP not enabled. Training in float32.
2023-11-30 10:02:53,512 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:02:55,629 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:02:56,729 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.215s,    9.95/s  (3.215s,    9.95/s)  LR: 1.000e-05  Data: 0.943 (0.943)
2023-11-30 10:02:56,994 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.265s,  120.96/s  (1.740s,   18.40/s)  LR: 1.000e-05  Data: 0.007 (0.475)
2023-11-30 10:02:57,254 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.261s,  122.78/s  (1.247s,   25.67/s)  LR: 1.000e-05  Data: 0.003 (0.317)
2023-11-30 10:02:57,518 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.263s,  121.49/s  (1.001s,   31.97/s)  LR: 1.000e-05  Data: 0.005 (0.239)
2023-11-30 10:02:57,781 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.264s,  121.33/s  (0.853s,   37.50/s)  LR: 1.000e-05  Data: 0.006 (0.193)
2023-11-30 10:02:58,042 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  122.96/s  (0.755s,   42.41/s)  LR: 1.000e-05  Data: 0.003 (0.161)
2023-11-30 10:02:58,303 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.261s,  122.54/s  (0.684s,   46.78/s)  LR: 1.000e-05  Data: 0.004 (0.138)
2023-11-30 10:02:58,564 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.42/s  (0.631s,   50.70/s)  LR: 1.000e-05  Data: 0.004 (0.122)
2023-11-30 10:02:58,826 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.14/s  (0.590s,   54.22/s)  LR: 1.000e-05  Data: 0.004 (0.109)
2023-11-30 10:02:59,088 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.261s,  122.39/s  (0.557s,   57.42/s)  LR: 1.000e-05  Data: 0.004 (0.098)
2023-11-30 10:02:59,349 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.54/s  (0.530s,   60.33/s)  LR: 1.000e-05  Data: 0.004 (0.090)
2023-11-30 10:02:59,610 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.46/s  (0.508s,   63.00/s)  LR: 1.000e-05  Data: 0.004 (0.083)
2023-11-30 10:02:59,872 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.24/s  (0.489s,   65.43/s)  LR: 1.000e-05  Data: 0.004 (0.077)
2023-11-30 10:03:00,134 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.262s,  122.11/s  (0.473s,   67.68/s)  LR: 1.000e-05  Data: 0.004 (0.071)
2023-11-30 10:03:00,395 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.261s,  122.46/s  (0.459s,   69.76/s)  LR: 1.000e-05  Data: 0.004 (0.067)
2023-11-30 10:03:00,657 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.22/s  (0.446s,   71.68/s)  LR: 1.000e-05  Data: 0.004 (0.063)
2023-11-30 10:03:00,919 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.14/s  (0.436s,   73.47/s)  LR: 1.000e-05  Data: 0.005 (0.060)
2023-11-30 10:03:01,181 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.22/s  (0.426s,   75.13/s)  LR: 1.000e-05  Data: 0.004 (0.056)
2023-11-30 10:03:01,442 Train: 0 [  18/39 ( 47%)]  Loss: 0.948 (1.26)  Time: 0.262s,  122.35/s  (0.417s,   76.69/s)  LR: 1.000e-05  Data: 0.004 (0.054)
2023-11-30 10:03:01,704 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.11/s  (0.410s,   78.14/s)  LR: 1.000e-05  Data: 0.004 (0.051)
2023-11-30 10:03:01,966 Train: 0 [  20/39 ( 53%)]  Loss: 0.966 (1.26)  Time: 0.262s,  122.13/s  (0.402s,   79.51/s)  LR: 1.000e-05  Data: 0.004 (0.049)
2023-11-30 10:03:02,228 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.261s,  122.44/s  (0.396s,   80.79/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 10:03:02,489 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.31/s  (0.390s,   82.01/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 10:03:02,752 Train: 0 [  23/39 ( 61%)]  Loss: 0.942 (1.27)  Time: 0.262s,  121.99/s  (0.385s,   83.14/s)  LR: 1.000e-05  Data: 0.005 (0.043)
2023-11-30 10:03:03,014 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.03/s  (0.380s,   84.21/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 10:03:03,275 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.261s,  122.46/s  (0.375s,   85.24/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 10:03:03,536 Train: 0 [  26/39 ( 68%)]  Loss: 1.00 (1.25)  Time: 0.261s,  122.70/s  (0.371s,   86.21/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 10:03:03,798 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  122.12/s  (0.367s,   87.13/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 10:03:04,060 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.16/s  (0.364s,   88.00/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 10:03:04,322 Train: 0 [  29/39 ( 76%)]  Loss: 0.991 (1.25)  Time: 0.262s,  122.21/s  (0.360s,   88.83/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 10:03:04,583 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.261s,  122.68/s  (0.357s,   89.62/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 10:03:04,843 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.260s,  122.89/s  (0.354s,   90.39/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 10:03:05,104 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.261s,  122.78/s  (0.351s,   91.12/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 10:03:05,364 Train: 0 [  33/39 ( 87%)]  Loss: 1.35 (1.25)  Time: 0.260s,  122.91/s  (0.349s,   91.82/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:03:05,624 Train: 0 [  34/39 ( 89%)]  Loss: 0.988 (1.25)  Time: 0.260s,  123.25/s  (0.346s,   92.49/s)  LR: 1.000e-05  Data: 0.003 (0.031)
2023-11-30 10:03:05,884 Train: 0 [  35/39 ( 92%)]  Loss: 1.14 (1.24)  Time: 0.261s,  122.82/s  (0.344s,   93.13/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 10:03:06,145 Train: 0 [  36/39 ( 95%)]  Loss: 1.31 (1.25)  Time: 0.260s,  122.88/s  (0.341s,   93.74/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 10:03:06,405 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.260s,  123.15/s  (0.339s,   94.34/s)  LR: 1.000e-05  Data: 0.004 (0.029)
2023-11-30 10:03:06,661 Train: 0 [  38/39 (100%)]  Loss: 2.53 (1.27)  Time: 0.257s,  124.56/s  (0.337s,   94.93/s)  LR: 1.000e-05  Data: 0.000 (0.028)
2023-11-30 10:03:07,751 Test: [   0/39]  Time: 1.086 (1.086)  Loss:   0.197 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:07,815 Test: [   1/39]  Time: 0.064 (0.575)  Loss:   0.194 ( 0.196)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:07,879 Test: [   2/39]  Time: 0.064 (0.405)  Loss:   0.197 ( 0.196)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:08,246 Test: [   3/39]  Time: 0.367 (0.395)  Loss:   0.198 ( 0.196)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:08,377 Test: [   4/39]  Time: 0.131 (0.342)  Loss:   0.198 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:08,441 Test: [   5/39]  Time: 0.063 (0.296)  Loss:   0.202 ( 0.198)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:08,504 Test: [   6/39]  Time: 0.064 (0.263)  Loss:   0.197 ( 0.198)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:08,835 Test: [   7/39]  Time: 0.331 (0.271)  Loss:   0.196 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,028 Test: [   8/39]  Time: 0.193 (0.263)  Loss:   0.196 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,092 Test: [   9/39]  Time: 0.064 (0.243)  Loss:   0.198 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,155 Test: [  10/39]  Time: 0.064 (0.226)  Loss:   0.198 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,477 Test: [  11/39]  Time: 0.321 (0.234)  Loss:   0.196 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,674 Test: [  12/39]  Time: 0.197 (0.231)  Loss:   0.197 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,737 Test: [  13/39]  Time: 0.064 (0.219)  Loss:   0.197 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:09,801 Test: [  14/39]  Time: 0.064 (0.209)  Loss:   0.198 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:10,144 Test: [  15/39]  Time: 0.343 (0.217)  Loss:   0.198 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:10,328 Test: [  16/39]  Time: 0.184 (0.215)  Loss:   0.197 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:10,391 Test: [  17/39]  Time: 0.064 (0.207)  Loss:   0.194 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:10,455 Test: [  18/39]  Time: 0.064 (0.199)  Loss:   0.197 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:10,765 Test: [  19/39]  Time: 0.310 (0.205)  Loss:   0.195 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:10,954 Test: [  20/39]  Time: 0.189 (0.204)  Loss:   0.195 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:11,018 Test: [  21/39]  Time: 0.064 (0.198)  Loss:   0.194 ( 0.197)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:11,082 Test: [  22/39]  Time: 0.064 (0.192)  Loss:   1.321 ( 0.246)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:03:11,375 Test: [  23/39]  Time: 0.293 (0.196)  Loss:   1.714 ( 0.307)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:03:11,599 Test: [  24/39]  Time: 0.224 (0.197)  Loss:   1.713 ( 0.363)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:03:11,663 Test: [  25/39]  Time: 0.064 (0.192)  Loss:   1.723 ( 0.415)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:03:11,726 Test: [  26/39]  Time: 0.064 (0.187)  Loss:   1.715 ( 0.463)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:03:12,013 Test: [  27/39]  Time: 0.287 (0.191)  Loss:   1.722 ( 0.508)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:03:12,214 Test: [  28/39]  Time: 0.201 (0.191)  Loss:   1.727 ( 0.550)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:03:12,278 Test: [  29/39]  Time: 0.064 (0.187)  Loss:   1.728 ( 0.590)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:03:12,342 Test: [  30/39]  Time: 0.064 (0.183)  Loss:   1.722 ( 0.626)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:03:12,710 Test: [  31/39]  Time: 0.368 (0.189)  Loss:   1.730 ( 0.661)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:03:12,878 Test: [  32/39]  Time: 0.168 (0.188)  Loss:   1.712 ( 0.693)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:03:12,942 Test: [  33/39]  Time: 0.064 (0.185)  Loss:   1.721 ( 0.723)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:03:13,005 Test: [  34/39]  Time: 0.064 (0.181)  Loss:   1.715 ( 0.751)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:03:13,086 Test: [  35/39]  Time: 0.081 (0.178)  Loss:   1.727 ( 0.778)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:03:13,398 Test: [  36/39]  Time: 0.312 (0.182)  Loss:   1.678 ( 0.803)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:03:14,001 Test: [  37/39]  Time: 0.602 (0.193)  Loss:   1.458 ( 0.820)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 10:03:14,062 Test: [  38/39]  Time: 0.061 (0.190)  Loss:   1.516 ( 0.838)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 10:03:14,262 Test: [  39/39]  Time: 0.201 (0.190)  Loss:   1.286 ( 0.838)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 10:03:14,469 Current checkpoints:
 ('./output/train/20231130-100253-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 10:03:14,470 *** Best metric: 57.28 (epoch 0)

2023-11-30 10:18:34,464 Training with a single process on 1 device (cuda:0).
2023-11-30 10:18:34,680 Model efficientnet_b0 created, param count:8733680
2023-11-30 10:18:34,681 Data processing configuration for current model + dataset:
2023-11-30 10:18:34,681 	input_size: (3, 259, 259)
2023-11-30 10:18:34,681 	interpolation: bicubic
2023-11-30 10:18:34,681 	mean: (0.485, 0.456, 0.406)
2023-11-30 10:18:34,681 	std: (0.229, 0.224, 0.225)
2023-11-30 10:18:34,681 	crop_pct: 0.875
2023-11-30 10:18:34,681 	crop_mode: center
2023-11-30 10:18:37,132 AMP not enabled. Training in float32.
2023-11-30 10:18:37,174 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 10:18:39,339 FLOPs: 39.779662336 GFLOPs
2023-11-30 10:18:40,446 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.270s,    9.79/s  (3.270s,    9.79/s)  LR: 1.000e-05  Data: 1.012 (1.012)
2023-11-30 10:18:40,710 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.264s,  121.43/s  (1.767s,   18.11/s)  LR: 1.000e-05  Data: 0.007 (0.509)
2023-11-30 10:18:40,969 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.12/s  (1.264s,   25.31/s)  LR: 1.000e-05  Data: 0.003 (0.340)
2023-11-30 10:18:41,231 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.261s,  122.38/s  (1.014s,   31.57/s)  LR: 1.000e-05  Data: 0.004 (0.256)
2023-11-30 10:18:41,494 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.263s,  121.84/s  (0.863s,   37.06/s)  LR: 1.000e-05  Data: 0.005 (0.206)
2023-11-30 10:18:41,754 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  122.86/s  (0.763s,   41.94/s)  LR: 1.000e-05  Data: 0.003 (0.172)
2023-11-30 10:18:42,016 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.03/s  (0.691s,   46.28/s)  LR: 1.000e-05  Data: 0.004 (0.148)
2023-11-30 10:18:42,277 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.61/s  (0.638s,   50.19/s)  LR: 1.000e-05  Data: 0.004 (0.130)
2023-11-30 10:18:42,539 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.29/s  (0.596s,   53.71/s)  LR: 1.000e-05  Data: 0.004 (0.116)
2023-11-30 10:18:42,800 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.261s,  122.43/s  (0.562s,   56.90/s)  LR: 1.000e-05  Data: 0.004 (0.105)
2023-11-30 10:18:43,061 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.261s,  122.50/s  (0.535s,   59.81/s)  LR: 1.000e-05  Data: 0.004 (0.096)
2023-11-30 10:18:43,323 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.261s,  122.43/s  (0.512s,   62.47/s)  LR: 1.000e-05  Data: 0.004 (0.088)
2023-11-30 10:18:43,584 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.261s,  122.63/s  (0.493s,   64.92/s)  LR: 1.000e-05  Data: 0.004 (0.082)
2023-11-30 10:18:43,845 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.51/s  (0.476s,   67.18/s)  LR: 1.000e-05  Data: 0.004 (0.076)
2023-11-30 10:18:44,107 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.22/s  (0.462s,   69.26/s)  LR: 1.000e-05  Data: 0.004 (0.071)
2023-11-30 10:18:44,369 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.23/s  (0.450s,   71.19/s)  LR: 1.000e-05  Data: 0.004 (0.067)
2023-11-30 10:18:44,630 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.30/s  (0.438s,   72.98/s)  LR: 1.000e-05  Data: 0.004 (0.064)
2023-11-30 10:18:44,892 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.261s,  122.44/s  (0.429s,   74.66/s)  LR: 1.000e-05  Data: 0.004 (0.060)
2023-11-30 10:18:45,153 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.261s,  122.47/s  (0.420s,   76.22/s)  LR: 1.000e-05  Data: 0.004 (0.057)
2023-11-30 10:18:45,415 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.15/s  (0.412s,   77.68/s)  LR: 1.000e-05  Data: 0.004 (0.055)
2023-11-30 10:18:45,676 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.261s,  122.38/s  (0.405s,   79.06/s)  LR: 1.000e-05  Data: 0.004 (0.052)
2023-11-30 10:18:45,937 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.261s,  122.62/s  (0.398s,   80.36/s)  LR: 1.000e-05  Data: 0.004 (0.050)
2023-11-30 10:18:46,198 Train: 0 [  22/39 ( 58%)]  Loss: 1.85 (1.28)  Time: 0.261s,  122.59/s  (0.392s,   81.58/s)  LR: 1.000e-05  Data: 0.004 (0.048)
2023-11-30 10:18:46,460 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.262s,  122.10/s  (0.387s,   82.72/s)  LR: 1.000e-05  Data: 0.004 (0.046)
2023-11-30 10:18:46,722 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.12/s  (0.382s,   83.80/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 10:18:46,984 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.261s,  122.38/s  (0.377s,   84.83/s)  LR: 1.000e-05  Data: 0.004 (0.043)
2023-11-30 10:18:47,246 Train: 0 [  26/39 ( 68%)]  Loss: 0.998 (1.25)  Time: 0.262s,  122.29/s  (0.373s,   85.81/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 10:18:47,507 Train: 0 [  27/39 ( 71%)]  Loss: 0.884 (1.24)  Time: 0.262s,  122.36/s  (0.369s,   86.73/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 10:18:47,769 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.262s,  122.24/s  (0.365s,   87.61/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 10:18:48,030 Train: 0 [  29/39 ( 76%)]  Loss: 0.988 (1.25)  Time: 0.261s,  122.38/s  (0.362s,   88.45/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 10:18:48,291 Train: 0 [  30/39 ( 79%)]  Loss: 1.06 (1.24)  Time: 0.260s,  122.85/s  (0.359s,   89.25/s)  LR: 1.000e-05  Data: 0.004 (0.037)
2023-11-30 10:18:48,552 Train: 0 [  31/39 ( 82%)]  Loss: 1.66 (1.26)  Time: 0.261s,  122.55/s  (0.355s,   90.02/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 10:18:48,812 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.260s,  122.92/s  (0.353s,   90.75/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 10:18:49,072 Train: 0 [  33/39 ( 87%)]  Loss: 1.36 (1.25)  Time: 0.259s,  123.39/s  (0.350s,   91.46/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 10:18:49,332 Train: 0 [  34/39 ( 89%)]  Loss: 0.977 (1.25)  Time: 0.261s,  122.72/s  (0.347s,   92.14/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 10:18:49,593 Train: 0 [  35/39 ( 92%)]  Loss: 1.09 (1.24)  Time: 0.260s,  122.99/s  (0.345s,   92.78/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 10:18:49,853 Train: 0 [  36/39 ( 95%)]  Loss: 1.25 (1.24)  Time: 0.260s,  123.07/s  (0.343s,   93.40/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:18:50,112 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.260s,  123.27/s  (0.340s,   94.00/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 10:18:50,368 Train: 0 [  38/39 (100%)]  Loss: 2.62 (1.27)  Time: 0.255s,  125.27/s  (0.338s,   94.61/s)  LR: 1.000e-05  Data: 0.000 (0.030)
2023-11-30 10:18:51,434 Test: [   0/39]  Time: 1.063 (1.063)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:51,501 Test: [   1/39]  Time: 0.066 (0.565)  Loss:   0.178 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:51,564 Test: [   2/39]  Time: 0.064 (0.398)  Loss:   0.181 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:51,932 Test: [   3/39]  Time: 0.368 (0.390)  Loss:   0.182 ( 0.180)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,066 Test: [   4/39]  Time: 0.134 (0.339)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,130 Test: [   5/39]  Time: 0.064 (0.293)  Loss:   0.185 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,193 Test: [   6/39]  Time: 0.064 (0.260)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,522 Test: [   7/39]  Time: 0.329 (0.269)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,709 Test: [   8/39]  Time: 0.187 (0.260)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,773 Test: [   9/39]  Time: 0.064 (0.240)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:52,836 Test: [  10/39]  Time: 0.064 (0.224)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:53,161 Test: [  11/39]  Time: 0.325 (0.233)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:53,343 Test: [  12/39]  Time: 0.182 (0.229)  Loss:   0.181 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:53,409 Test: [  13/39]  Time: 0.065 (0.217)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:53,472 Test: [  14/39]  Time: 0.064 (0.207)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:53,827 Test: [  15/39]  Time: 0.354 (0.216)  Loss:   0.182 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,005 Test: [  16/39]  Time: 0.178 (0.214)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,069 Test: [  17/39]  Time: 0.064 (0.205)  Loss:   0.178 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,132 Test: [  18/39]  Time: 0.064 (0.198)  Loss:   0.180 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,450 Test: [  19/39]  Time: 0.317 (0.204)  Loss:   0.179 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,633 Test: [  20/39]  Time: 0.183 (0.203)  Loss:   0.179 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,697 Test: [  21/39]  Time: 0.064 (0.197)  Loss:   0.178 ( 0.181)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:54,760 Test: [  22/39]  Time: 0.064 (0.191)  Loss:   1.375 ( 0.232)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 10:18:55,038 Test: [  23/39]  Time: 0.277 (0.194)  Loss:   1.792 ( 0.297)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 10:18:55,258 Test: [  24/39]  Time: 0.220 (0.195)  Loss:   1.790 ( 0.357)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 10:18:55,321 Test: [  25/39]  Time: 0.064 (0.190)  Loss:   1.801 ( 0.413)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 10:18:55,385 Test: [  26/39]  Time: 0.064 (0.186)  Loss:   1.793 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 10:18:55,665 Test: [  27/39]  Time: 0.280 (0.189)  Loss:   1.799 ( 0.511)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 10:18:55,833 Test: [  28/39]  Time: 0.169 (0.188)  Loss:   1.805 ( 0.556)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 10:18:55,899 Test: [  29/39]  Time: 0.066 (0.184)  Loss:   1.805 ( 0.598)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 10:18:55,963 Test: [  30/39]  Time: 0.064 (0.180)  Loss:   1.800 ( 0.637)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 10:18:56,284 Test: [  31/39]  Time: 0.321 (0.185)  Loss:   1.808 ( 0.673)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 10:18:56,437 Test: [  32/39]  Time: 0.153 (0.184)  Loss:   1.790 ( 0.707)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 10:18:56,502 Test: [  33/39]  Time: 0.065 (0.180)  Loss:   1.799 ( 0.739)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 10:18:56,566 Test: [  34/39]  Time: 0.064 (0.177)  Loss:   1.792 ( 0.769)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 10:18:56,652 Test: [  35/39]  Time: 0.086 (0.174)  Loss:   1.805 ( 0.798)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 10:18:56,945 Test: [  36/39]  Time: 0.293 (0.178)  Loss:   1.755 ( 0.824)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 10:18:57,662 Test: [  37/39]  Time: 0.717 (0.192)  Loss:   1.526 ( 0.842)  Acc@1:   3.125 ( 58.635)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.031 (  0.001)soec:   0.000 (  0.605)f1:   0.061 (  0.002)
2023-11-30 10:18:57,724 Test: [  38/39]  Time: 0.061 (0.189)  Loss:   1.586 ( 0.861)  Acc@1:   3.125 ( 57.212)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.003)
2023-11-30 10:18:57,912 Test: [  39/39]  Time: 0.188 (0.189)  Loss:   1.348 ( 0.862)  Acc@1:   0.000 ( 57.120)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.003)
2023-11-30 10:18:58,119 Current checkpoints:
 ('./output/train/20231130-101837-efficientnet_b0-259/checkpoint-0.pth.tar', 57.12)

2023-11-30 10:18:58,120 *** Best metric: 57.12 (epoch 0)

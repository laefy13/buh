2023-11-30 09:53:33,416 Training with a single process on 1 device (cuda:0).
2023-11-30 09:53:33,632 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:53:33,632 Data processing configuration for current model + dataset:
2023-11-30 09:53:33,632 	input_size: (3, 259, 259)
2023-11-30 09:53:33,632 	interpolation: bicubic
2023-11-30 09:53:33,632 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:53:33,632 	std: (0.229, 0.224, 0.225)
2023-11-30 09:53:33,632 	crop_pct: 0.875
2023-11-30 09:53:33,632 	crop_mode: center
2023-11-30 09:53:36,056 AMP not enabled. Training in float32.
2023-11-30 09:53:36,098 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:53:38,209 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:53:39,308 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.208s,    9.98/s  (3.208s,    9.98/s)  LR: 1.000e-05  Data: 0.975 (0.975)
2023-11-30 09:53:39,571 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.263s,  121.61/s  (1.735s,   18.44/s)  LR: 1.000e-05  Data: 0.006 (0.491)
2023-11-30 09:53:39,834 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.263s,  121.84/s  (1.244s,   25.71/s)  LR: 1.000e-05  Data: 0.005 (0.329)
2023-11-30 09:53:40,096 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.25/s  (0.999s,   32.04/s)  LR: 1.000e-05  Data: 0.004 (0.247)
2023-11-30 09:53:40,357 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.262s,  122.29/s  (0.851s,   37.59/s)  LR: 1.000e-05  Data: 0.004 (0.199)
2023-11-30 09:53:40,619 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.262s,  122.28/s  (0.753s,   42.49/s)  LR: 1.000e-05  Data: 0.004 (0.166)
2023-11-30 09:53:40,881 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.262s,  122.30/s  (0.683s,   46.86/s)  LR: 1.000e-05  Data: 0.004 (0.143)
2023-11-30 09:53:41,141 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.260s,  122.92/s  (0.630s,   50.79/s)  LR: 1.000e-05  Data: 0.004 (0.126)
2023-11-30 09:53:41,403 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.20/s  (0.589s,   54.32/s)  LR: 1.000e-05  Data: 0.004 (0.112)
2023-11-30 09:53:41,665 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.10/s  (0.556s,   57.51/s)  LR: 1.000e-05  Data: 0.005 (0.102)
2023-11-30 09:53:41,927 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.16/s  (0.530s,   60.41/s)  LR: 1.000e-05  Data: 0.005 (0.093)
2023-11-30 09:53:42,190 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.263s,  121.65/s  (0.507s,   63.06/s)  LR: 1.000e-05  Data: 0.004 (0.085)
2023-11-30 09:53:42,452 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.18/s  (0.489s,   65.50/s)  LR: 1.000e-05  Data: 0.004 (0.079)
2023-11-30 09:53:42,715 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.263s,  121.63/s  (0.472s,   67.73/s)  LR: 1.000e-05  Data: 0.004 (0.074)
2023-11-30 09:53:42,977 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.18/s  (0.458s,   69.80/s)  LR: 1.000e-05  Data: 0.004 (0.069)
2023-11-30 09:53:43,240 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.263s,  121.52/s  (0.446s,   71.71/s)  LR: 1.000e-05  Data: 0.004 (0.065)
2023-11-30 09:53:43,503 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  121.99/s  (0.435s,   73.49/s)  LR: 1.000e-05  Data: 0.004 (0.062)
2023-11-30 09:53:43,767 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.264s,  121.20/s  (0.426s,   75.14/s)  LR: 1.000e-05  Data: 0.005 (0.058)
2023-11-30 09:53:44,032 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.266s,  120.45/s  (0.417s,   76.65/s)  LR: 1.000e-05  Data: 0.008 (0.056)
2023-11-30 09:53:44,294 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.30/s  (0.410s,   78.11/s)  LR: 1.000e-05  Data: 0.004 (0.053)
2023-11-30 09:53:44,555 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.261s,  122.51/s  (0.403s,   79.48/s)  LR: 1.000e-05  Data: 0.004 (0.051)
2023-11-30 09:53:44,817 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.14/s  (0.396s,   80.77/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 09:53:45,079 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.15/s  (0.390s,   81.97/s)  LR: 1.000e-05  Data: 0.005 (0.047)
2023-11-30 09:53:45,341 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.262s,  122.12/s  (0.385s,   83.11/s)  LR: 1.000e-05  Data: 0.004 (0.045)
2023-11-30 09:53:45,603 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.08/s  (0.380s,   84.19/s)  LR: 1.000e-05  Data: 0.004 (0.043)
2023-11-30 09:53:45,865 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.21/s  (0.376s,   85.21/s)  LR: 1.000e-05  Data: 0.005 (0.042)
2023-11-30 09:53:46,128 Train: 0 [  26/39 ( 68%)]  Loss: 0.998 (1.25)  Time: 0.262s,  121.91/s  (0.371s,   86.17/s)  LR: 1.000e-05  Data: 0.005 (0.041)
2023-11-30 09:53:46,390 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  121.99/s  (0.367s,   87.08/s)  LR: 1.000e-05  Data: 0.005 (0.039)
2023-11-30 09:53:46,651 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.261s,  122.42/s  (0.364s,   87.96/s)  LR: 1.000e-05  Data: 0.004 (0.038)
2023-11-30 09:53:46,913 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  122.16/s  (0.360s,   88.79/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:53:47,175 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.261s,  122.41/s  (0.357s,   89.58/s)  LR: 1.000e-05  Data: 0.004 (0.036)
2023-11-30 09:53:47,436 Train: 0 [  31/39 ( 82%)]  Loss: 1.67 (1.26)  Time: 0.261s,  122.65/s  (0.354s,   90.34/s)  LR: 1.000e-05  Data: 0.005 (0.035)
2023-11-30 09:53:47,696 Train: 0 [  32/39 ( 84%)]  Loss: 1.05 (1.25)  Time: 0.261s,  122.69/s  (0.351s,   91.07/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:53:47,957 Train: 0 [  33/39 ( 87%)]  Loss: 1.35 (1.25)  Time: 0.261s,  122.72/s  (0.349s,   91.76/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:53:48,218 Train: 0 [  34/39 ( 89%)]  Loss: 0.981 (1.25)  Time: 0.261s,  122.59/s  (0.346s,   92.43/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:53:48,479 Train: 0 [  35/39 ( 92%)]  Loss: 1.14 (1.24)  Time: 0.261s,  122.53/s  (0.344s,   93.06/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:53:48,740 Train: 0 [  36/39 ( 95%)]  Loss: 1.27 (1.24)  Time: 0.261s,  122.68/s  (0.342s,   93.67/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:53:49,001 Train: 0 [  37/39 ( 97%)]  Loss: 1.01 (1.24)  Time: 0.261s,  122.75/s  (0.339s,   94.26/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:53:49,257 Train: 0 [  38/39 (100%)]  Loss: 2.54 (1.27)  Time: 0.256s,  124.84/s  (0.337s,   94.86/s)  LR: 1.000e-05  Data: 0.000 (0.029)
2023-11-30 09:53:50,447 Test: [   0/39]  Time: 1.186 (1.186)  Loss:   0.192 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:50,511 Test: [   1/39]  Time: 0.064 (0.625)  Loss:   0.189 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:50,574 Test: [   2/39]  Time: 0.064 (0.438)  Loss:   0.192 ( 0.191)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:50,943 Test: [   3/39]  Time: 0.369 (0.421)  Loss:   0.193 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,082 Test: [   4/39]  Time: 0.139 (0.364)  Loss:   0.193 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,146 Test: [   5/39]  Time: 0.064 (0.314)  Loss:   0.197 ( 0.193)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,209 Test: [   6/39]  Time: 0.064 (0.278)  Loss:   0.192 ( 0.193)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,529 Test: [   7/39]  Time: 0.319 (0.283)  Loss:   0.191 ( 0.193)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,728 Test: [   8/39]  Time: 0.199 (0.274)  Loss:   0.191 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,792 Test: [   9/39]  Time: 0.064 (0.253)  Loss:   0.193 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:51,855 Test: [  10/39]  Time: 0.064 (0.236)  Loss:   0.193 ( 0.193)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:52,176 Test: [  11/39]  Time: 0.321 (0.243)  Loss:   0.191 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:52,371 Test: [  12/39]  Time: 0.195 (0.239)  Loss:   0.192 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:52,434 Test: [  13/39]  Time: 0.064 (0.227)  Loss:   0.192 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:52,498 Test: [  14/39]  Time: 0.064 (0.216)  Loss:   0.193 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:52,843 Test: [  15/39]  Time: 0.346 (0.224)  Loss:   0.193 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,026 Test: [  16/39]  Time: 0.182 (0.221)  Loss:   0.192 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,089 Test: [  17/39]  Time: 0.064 (0.213)  Loss:   0.189 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,153 Test: [  18/39]  Time: 0.064 (0.205)  Loss:   0.192 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,461 Test: [  19/39]  Time: 0.308 (0.210)  Loss:   0.190 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,636 Test: [  20/39]  Time: 0.176 (0.208)  Loss:   0.191 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,700 Test: [  21/39]  Time: 0.064 (0.202)  Loss:   0.189 ( 0.192)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:53,764 Test: [  22/39]  Time: 0.064 (0.196)  Loss:   1.337 ( 0.242)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:54,055 Test: [  23/39]  Time: 0.291 (0.200)  Loss:   1.736 ( 0.304)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:53:54,267 Test: [  24/39]  Time: 0.213 (0.200)  Loss:   1.735 ( 0.361)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:53:54,331 Test: [  25/39]  Time: 0.064 (0.195)  Loss:   1.745 ( 0.414)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:53:54,395 Test: [  26/39]  Time: 0.064 (0.190)  Loss:   1.737 ( 0.463)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:53:54,682 Test: [  27/39]  Time: 0.287 (0.194)  Loss:   1.743 ( 0.509)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:53:54,830 Test: [  28/39]  Time: 0.148 (0.192)  Loss:   1.749 ( 0.552)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:53:54,895 Test: [  29/39]  Time: 0.065 (0.188)  Loss:   1.749 ( 0.592)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:53:54,959 Test: [  30/39]  Time: 0.064 (0.184)  Loss:   1.744 ( 0.629)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:53:55,301 Test: [  31/39]  Time: 0.342 (0.189)  Loss:   1.752 ( 0.664)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:53:55,428 Test: [  32/39]  Time: 0.127 (0.187)  Loss:   1.734 ( 0.696)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:53:55,492 Test: [  33/39]  Time: 0.064 (0.183)  Loss:   1.743 ( 0.727)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:53:55,556 Test: [  34/39]  Time: 0.064 (0.180)  Loss:   1.736 ( 0.756)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:53:55,667 Test: [  35/39]  Time: 0.111 (0.178)  Loss:   1.749 ( 0.784)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:53:55,952 Test: [  36/39]  Time: 0.286 (0.181)  Loss:   1.702 ( 0.809)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:53:56,634 Test: [  37/39]  Time: 0.682 (0.194)  Loss:   1.482 ( 0.826)  Acc@1:   6.250 ( 58.717)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.062 (  0.002)soec:   0.000 (  0.605)f1:   0.118 (  0.003)
2023-11-30 09:53:56,695 Test: [  38/39]  Time: 0.061 (0.191)  Loss:   1.541 ( 0.845)  Acc@1:   3.125 ( 57.292)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.002)soec:   0.000 (  0.590)f1:   0.061 (  0.005)
2023-11-30 09:53:56,887 Test: [  39/39]  Time: 0.192 (0.191)  Loss:   1.307 ( 0.845)  Acc@1:   0.000 ( 57.200)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.002)soec:   0.000 (  0.589)f1:   0.000 (  0.005)
2023-11-30 09:53:57,093 Current checkpoints:
 ('./output/train/20231130-095336-efficientnet_b0-259/checkpoint-0.pth.tar', 57.2)

2023-11-30 09:53:57,093 *** Best metric: 57.2 (epoch 0)

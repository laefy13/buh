2023-11-30 09:52:41,236 Training with a single process on 1 device (cuda:0).
2023-11-30 09:52:41,452 Model efficientnet_b0 created, param count:8733680
2023-11-30 09:52:41,452 Data processing configuration for current model + dataset:
2023-11-30 09:52:41,452 	input_size: (3, 259, 259)
2023-11-30 09:52:41,452 	interpolation: bicubic
2023-11-30 09:52:41,453 	mean: (0.485, 0.456, 0.406)
2023-11-30 09:52:41,453 	std: (0.229, 0.224, 0.225)
2023-11-30 09:52:41,453 	crop_pct: 0.875
2023-11-30 09:52:41,453 	crop_mode: center
2023-11-30 09:52:43,928 AMP not enabled. Training in float32.
2023-11-30 09:52:43,971 Scheduled epochs: 1. LR stepped per epoch.
2023-11-30 09:52:46,113 FLOPs: 39.779662336 GFLOPs
2023-11-30 09:52:47,209 Train: 0 [   0/39 (  0%)]  Loss: 1.38 (1.38)  Time: 3.236s,    9.89/s  (3.236s,    9.89/s)  LR: 1.000e-05  Data: 0.987 (0.987)
2023-11-30 09:52:47,477 Train: 0 [   1/39 (  3%)]  Loss: 0.876 (1.13)  Time: 0.268s,  119.60/s  (1.752s,   18.27/s)  LR: 1.000e-05  Data: 0.010 (0.499)
2023-11-30 09:52:47,736 Train: 0 [   2/39 (  5%)]  Loss: 0.998 (1.09)  Time: 0.260s,  123.16/s  (1.254s,   25.51/s)  LR: 1.000e-05  Data: 0.003 (0.333)
2023-11-30 09:52:47,998 Train: 0 [   3/39 (  8%)]  Loss: 0.736 (0.998)  Time: 0.262s,  122.30/s  (1.006s,   31.80/s)  LR: 1.000e-05  Data: 0.004 (0.251)
2023-11-30 09:52:48,264 Train: 0 [   4/39 ( 11%)]  Loss: 0.971 (0.993)  Time: 0.266s,  120.47/s  (0.858s,   37.29/s)  LR: 1.000e-05  Data: 0.009 (0.203)
2023-11-30 09:52:48,524 Train: 0 [   5/39 ( 13%)]  Loss: 1.26 (1.04)  Time: 0.260s,  123.19/s  (0.758s,   42.20/s)  LR: 1.000e-05  Data: 0.003 (0.169)
2023-11-30 09:52:48,783 Train: 0 [   6/39 ( 16%)]  Loss: 1.23 (1.06)  Time: 0.260s,  123.27/s  (0.687s,   46.57/s)  LR: 1.000e-05  Data: 0.003 (0.146)
2023-11-30 09:52:49,044 Train: 0 [   7/39 ( 18%)]  Loss: 0.919 (1.05)  Time: 0.261s,  122.56/s  (0.634s,   50.49/s)  LR: 1.000e-05  Data: 0.005 (0.128)
2023-11-30 09:52:49,306 Train: 0 [   8/39 ( 21%)]  Loss: 1.93 (1.14)  Time: 0.262s,  122.14/s  (0.593s,   54.01/s)  LR: 1.000e-05  Data: 0.004 (0.114)
2023-11-30 09:52:49,568 Train: 0 [   9/39 ( 24%)]  Loss: 1.46 (1.18)  Time: 0.262s,  122.13/s  (0.559s,   57.20/s)  LR: 1.000e-05  Data: 0.005 (0.103)
2023-11-30 09:52:49,830 Train: 0 [  10/39 ( 26%)]  Loss: 1.78 (1.23)  Time: 0.262s,  122.12/s  (0.532s,   60.10/s)  LR: 1.000e-05  Data: 0.004 (0.094)
2023-11-30 09:52:50,092 Train: 0 [  11/39 ( 29%)]  Loss: 1.21 (1.23)  Time: 0.262s,  122.21/s  (0.510s,   62.76/s)  LR: 1.000e-05  Data: 0.005 (0.087)
2023-11-30 09:52:50,354 Train: 0 [  12/39 ( 32%)]  Loss: 1.31 (1.23)  Time: 0.262s,  122.25/s  (0.491s,   65.20/s)  LR: 1.000e-05  Data: 0.005 (0.080)
2023-11-30 09:52:50,615 Train: 0 [  13/39 ( 34%)]  Loss: 1.91 (1.28)  Time: 0.261s,  122.61/s  (0.474s,   67.46/s)  LR: 1.000e-05  Data: 0.004 (0.075)
2023-11-30 09:52:50,877 Train: 0 [  14/39 ( 37%)]  Loss: 1.12 (1.27)  Time: 0.262s,  122.23/s  (0.460s,   69.53/s)  LR: 1.000e-05  Data: 0.004 (0.070)
2023-11-30 09:52:51,138 Train: 0 [  15/39 ( 39%)]  Loss: 1.05 (1.26)  Time: 0.262s,  122.36/s  (0.448s,   71.46/s)  LR: 1.000e-05  Data: 0.005 (0.066)
2023-11-30 09:52:51,400 Train: 0 [  16/39 ( 42%)]  Loss: 2.06 (1.31)  Time: 0.262s,  122.25/s  (0.437s,   73.25/s)  LR: 1.000e-05  Data: 0.004 (0.063)
2023-11-30 09:52:51,662 Train: 0 [  17/39 ( 45%)]  Loss: 0.833 (1.28)  Time: 0.262s,  122.18/s  (0.427s,   74.92/s)  LR: 1.000e-05  Data: 0.005 (0.059)
2023-11-30 09:52:51,924 Train: 0 [  18/39 ( 47%)]  Loss: 0.947 (1.26)  Time: 0.262s,  122.25/s  (0.418s,   76.48/s)  LR: 1.000e-05  Data: 0.004 (0.056)
2023-11-30 09:52:52,185 Train: 0 [  19/39 ( 50%)]  Loss: 1.56 (1.28)  Time: 0.262s,  122.24/s  (0.411s,   77.94/s)  LR: 1.000e-05  Data: 0.005 (0.054)
2023-11-30 09:52:52,447 Train: 0 [  20/39 ( 53%)]  Loss: 0.965 (1.26)  Time: 0.262s,  122.21/s  (0.404s,   79.30/s)  LR: 1.000e-05  Data: 0.005 (0.052)
2023-11-30 09:52:52,709 Train: 0 [  21/39 ( 55%)]  Loss: 1.08 (1.25)  Time: 0.262s,  122.12/s  (0.397s,   80.59/s)  LR: 1.000e-05  Data: 0.005 (0.049)
2023-11-30 09:52:52,971 Train: 0 [  22/39 ( 58%)]  Loss: 1.86 (1.28)  Time: 0.262s,  122.32/s  (0.391s,   81.80/s)  LR: 1.000e-05  Data: 0.004 (0.047)
2023-11-30 09:52:53,233 Train: 0 [  23/39 ( 61%)]  Loss: 0.941 (1.27)  Time: 0.262s,  122.22/s  (0.386s,   82.94/s)  LR: 1.000e-05  Data: 0.004 (0.046)
2023-11-30 09:52:53,494 Train: 0 [  24/39 ( 63%)]  Loss: 1.31 (1.27)  Time: 0.262s,  122.36/s  (0.381s,   84.03/s)  LR: 1.000e-05  Data: 0.005 (0.044)
2023-11-30 09:52:53,756 Train: 0 [  25/39 ( 66%)]  Loss: 1.06 (1.26)  Time: 0.262s,  122.35/s  (0.376s,   85.05/s)  LR: 1.000e-05  Data: 0.004 (0.042)
2023-11-30 09:52:54,018 Train: 0 [  26/39 ( 68%)]  Loss: 0.999 (1.25)  Time: 0.262s,  122.21/s  (0.372s,   86.02/s)  LR: 1.000e-05  Data: 0.005 (0.041)
2023-11-30 09:52:54,279 Train: 0 [  27/39 ( 71%)]  Loss: 0.885 (1.24)  Time: 0.262s,  122.34/s  (0.368s,   86.94/s)  LR: 1.000e-05  Data: 0.004 (0.040)
2023-11-30 09:52:54,540 Train: 0 [  28/39 ( 74%)]  Loss: 1.86 (1.26)  Time: 0.261s,  122.51/s  (0.364s,   87.82/s)  LR: 1.000e-05  Data: 0.004 (0.039)
2023-11-30 09:52:54,802 Train: 0 [  29/39 ( 76%)]  Loss: 0.989 (1.25)  Time: 0.262s,  122.33/s  (0.361s,   88.65/s)  LR: 1.000e-05  Data: 0.005 (0.037)
2023-11-30 09:52:55,063 Train: 0 [  30/39 ( 79%)]  Loss: 1.07 (1.24)  Time: 0.261s,  122.54/s  (0.358s,   89.45/s)  LR: 1.000e-05  Data: 0.005 (0.036)
2023-11-30 09:52:55,324 Train: 0 [  31/39 ( 82%)]  Loss: 1.68 (1.26)  Time: 0.261s,  122.79/s  (0.355s,   90.22/s)  LR: 1.000e-05  Data: 0.004 (0.035)
2023-11-30 09:52:55,584 Train: 0 [  32/39 ( 84%)]  Loss: 1.06 (1.25)  Time: 0.260s,  122.86/s  (0.352s,   90.95/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:52:55,844 Train: 0 [  33/39 ( 87%)]  Loss: 1.35 (1.25)  Time: 0.260s,  123.12/s  (0.349s,   91.65/s)  LR: 1.000e-05  Data: 0.004 (0.034)
2023-11-30 09:52:56,104 Train: 0 [  34/39 ( 89%)]  Loss: 0.996 (1.25)  Time: 0.260s,  123.03/s  (0.347s,   92.33/s)  LR: 1.000e-05  Data: 0.004 (0.033)
2023-11-30 09:52:56,364 Train: 0 [  35/39 ( 92%)]  Loss: 1.18 (1.25)  Time: 0.260s,  123.12/s  (0.344s,   92.97/s)  LR: 1.000e-05  Data: 0.004 (0.032)
2023-11-30 09:52:56,624 Train: 0 [  36/39 ( 95%)]  Loss: 1.33 (1.25)  Time: 0.260s,  123.21/s  (0.342s,   93.59/s)  LR: 1.000e-05  Data: 0.004 (0.031)
2023-11-30 09:52:56,883 Train: 0 [  37/39 ( 97%)]  Loss: 1.02 (1.24)  Time: 0.260s,  123.25/s  (0.340s,   94.19/s)  LR: 1.000e-05  Data: 0.004 (0.030)
2023-11-30 09:52:57,139 Train: 0 [  38/39 (100%)]  Loss: 2.46 (1.27)  Time: 0.256s,  125.18/s  (0.338s,   94.79/s)  LR: 1.000e-05  Data: 0.000 (0.030)
2023-11-30 09:52:58,198 Test: [   0/39]  Time: 1.056 (1.056)  Loss:   0.209 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:58,263 Test: [   1/39]  Time: 0.064 (0.560)  Loss:   0.206 ( 0.207)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:58,327 Test: [   2/39]  Time: 0.064 (0.395)  Loss:   0.209 ( 0.208)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:58,712 Test: [   3/39]  Time: 0.386 (0.392)  Loss:   0.210 ( 0.208)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:58,840 Test: [   4/39]  Time: 0.128 (0.339)  Loss:   0.210 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:58,903 Test: [   5/39]  Time: 0.064 (0.294)  Loss:   0.214 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:58,967 Test: [   6/39]  Time: 0.064 (0.261)  Loss:   0.209 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:59,299 Test: [   7/39]  Time: 0.331 (0.270)  Loss:   0.207 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:59,488 Test: [   8/39]  Time: 0.189 (0.261)  Loss:   0.208 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:59,552 Test: [   9/39]  Time: 0.064 (0.241)  Loss:   0.210 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:59,615 Test: [  10/39]  Time: 0.064 (0.225)  Loss:   0.210 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:52:59,936 Test: [  11/39]  Time: 0.321 (0.233)  Loss:   0.208 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,132 Test: [  12/39]  Time: 0.195 (0.230)  Loss:   0.209 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,195 Test: [  13/39]  Time: 0.064 (0.218)  Loss:   0.209 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,259 Test: [  14/39]  Time: 0.064 (0.208)  Loss:   0.210 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,602 Test: [  15/39]  Time: 0.343 (0.216)  Loss:   0.210 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,792 Test: [  16/39]  Time: 0.190 (0.215)  Loss:   0.208 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,855 Test: [  17/39]  Time: 0.064 (0.206)  Loss:   0.206 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:00,919 Test: [  18/39]  Time: 0.064 (0.199)  Loss:   0.208 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:01,226 Test: [  19/39]  Time: 0.307 (0.204)  Loss:   0.207 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:01,423 Test: [  20/39]  Time: 0.196 (0.204)  Loss:   0.207 ( 0.209)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:01,486 Test: [  21/39]  Time: 0.064 (0.197)  Loss:   0.205 ( 0.208)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:01,550 Test: [  22/39]  Time: 0.064 (0.192)  Loss:   1.285 ( 0.255)  Acc@1:  25.000 ( 96.739)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   1.000 (  1.000)f1:   0.000 (  0.000)
2023-11-30 09:53:01,820 Test: [  23/39]  Time: 0.270 (0.195)  Loss:   1.662 ( 0.314)  Acc@1:   0.000 ( 92.708)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.958)f1:   0.000 (  0.000)
2023-11-30 09:53:02,043 Test: [  24/39]  Time: 0.223 (0.196)  Loss:   1.660 ( 0.368)  Acc@1:   0.000 ( 89.000)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.920)f1:   0.000 (  0.000)
2023-11-30 09:53:02,107 Test: [  25/39]  Time: 0.064 (0.191)  Loss:   1.670 ( 0.418)  Acc@1:   0.000 ( 85.577)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.885)f1:   0.000 (  0.000)
2023-11-30 09:53:02,171 Test: [  26/39]  Time: 0.064 (0.186)  Loss:   1.663 ( 0.464)  Acc@1:   0.000 ( 82.407)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.852)f1:   0.000 (  0.000)
2023-11-30 09:53:02,440 Test: [  27/39]  Time: 0.269 (0.189)  Loss:   1.669 ( 0.507)  Acc@1:   0.000 ( 79.464)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.821)f1:   0.000 (  0.000)
2023-11-30 09:53:02,600 Test: [  28/39]  Time: 0.160 (0.188)  Loss:   1.675 ( 0.547)  Acc@1:   0.000 ( 76.724)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.793)f1:   0.000 (  0.000)
2023-11-30 09:53:02,667 Test: [  29/39]  Time: 0.067 (0.184)  Loss:   1.675 ( 0.585)  Acc@1:   0.000 ( 74.167)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.767)f1:   0.000 (  0.000)
2023-11-30 09:53:02,731 Test: [  30/39]  Time: 0.064 (0.180)  Loss:   1.669 ( 0.620)  Acc@1:   0.000 ( 71.774)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.742)f1:   0.000 (  0.000)
2023-11-30 09:53:03,068 Test: [  31/39]  Time: 0.337 (0.185)  Loss:   1.677 ( 0.653)  Acc@1:   0.000 ( 69.531)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.719)f1:   0.000 (  0.000)
2023-11-30 09:53:03,216 Test: [  32/39]  Time: 0.149 (0.184)  Loss:   1.660 ( 0.683)  Acc@1:   0.000 ( 67.424)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.697)f1:   0.000 (  0.000)
2023-11-30 09:53:03,280 Test: [  33/39]  Time: 0.064 (0.181)  Loss:   1.669 ( 0.712)  Acc@1:   0.000 ( 65.441)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.676)f1:   0.000 (  0.000)
2023-11-30 09:53:03,343 Test: [  34/39]  Time: 0.064 (0.177)  Loss:   1.662 ( 0.739)  Acc@1:   0.000 ( 63.571)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.657)f1:   0.000 (  0.000)
2023-11-30 09:53:03,432 Test: [  35/39]  Time: 0.088 (0.175)  Loss:   1.674 ( 0.765)  Acc@1:   0.000 ( 61.806)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.639)f1:   0.000 (  0.000)
2023-11-30 09:53:03,724 Test: [  36/39]  Time: 0.292 (0.178)  Loss:   1.628 ( 0.789)  Acc@1:   0.000 ( 60.135)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.000)recall:   0.000 (  0.000)soec:   0.000 (  0.622)f1:   0.000 (  0.000)
2023-11-30 09:53:04,397 Test: [  37/39]  Time: 0.673 (0.191)  Loss:   1.418 ( 0.805)  Acc@1:   9.375 ( 58.799)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.026)recall:   0.094 (  0.002)soec:   0.000 (  0.605)f1:   0.171 (  0.005)
2023-11-30 09:53:04,458 Test: [  38/39]  Time: 0.061 (0.188)  Loss:   1.474 ( 0.822)  Acc@1:   3.125 ( 57.372)  Acc@5: 100.000 (100.000)precision:   1.000 (  0.051)recall:   0.031 (  0.003)soec:   0.000 (  0.590)f1:   0.061 (  0.006)
2023-11-30 09:53:04,662 Test: [  39/39]  Time: 0.204 (0.188)  Loss:   1.248 ( 0.823)  Acc@1:   0.000 ( 57.280)  Acc@5: 100.000 (100.000)precision:   0.000 (  0.051)recall:   0.000 (  0.003)soec:   0.000 (  0.589)f1:   0.000 (  0.006)
2023-11-30 09:53:04,868 Current checkpoints:
 ('./output/train/20231130-095243-efficientnet_b0-259/checkpoint-0.pth.tar', 57.28)

2023-11-30 09:53:04,868 *** Best metric: 57.28 (epoch 0)
